{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:40:19.048306Z","iopub.execute_input":"2025-04-10T19:40:19.048579Z","iopub.status.idle":"2025-04-10T19:40:19.385931Z","shell.execute_reply.started":"2025-04-10T19:40:19.048561Z","shell.execute_reply":"2025-04-10T19:40:19.385160Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:40:19.386653Z","iopub.execute_input":"2025-04-10T19:40:19.387008Z","iopub.status.idle":"2025-04-10T19:40:24.340242Z","shell.execute_reply.started":"2025-04-10T19:40:19.386988Z","shell.execute_reply":"2025-04-10T19:40:24.339704Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Check Kaggle's directory structure\nprint(\"Available input directories:\")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(f\"Directory: {dirname}\")\n    if len(filenames) > 0:\n        print(f\"  Sample files: {filenames[:3]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:40:24.341834Z","iopub.execute_input":"2025-04-10T19:40:24.342130Z"}},"outputs":[{"name":"stdout","text":"Available input directories:\nDirectory: /kaggle/input\nDirectory: /kaggle/input/asl-signs\n  Sample files: ['sign_to_prediction_index_map.json', 'train.csv']\nDirectory: /kaggle/input/asl-signs/train_landmark_files\nDirectory: /kaggle/input/asl-signs/train_landmark_files/36257\n  Sample files: ['3762317508.parquet', '1613088982.parquet', '4161325779.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/25571\n  Sample files: ['867304231.parquet', '3857133971.parquet', '3549313597.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/55372\n  Sample files: ['3987235427.parquet', '3213314543.parquet', '347178606.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/26734\n  Sample files: ['39347591.parquet', '744758408.parquet', '1160711543.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/16069\n  Sample files: ['2285328250.parquet', '2596699720.parquet', '1733649131.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/53618\n  Sample files: ['4097657043.parquet', '3422236995.parquet', '523469205.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/27610\n  Sample files: ['1219736891.parquet', '2419073566.parquet', '2841791382.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/32319\n  Sample files: ['1049896113.parquet', '514715973.parquet', '391074653.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/49445\n  Sample files: ['2113974010.parquet', '1799073940.parquet', '161704811.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/28656\n  Sample files: ['3009868197.parquet', '4250408917.parquet', '3184474979.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/4718\n  Sample files: ['641163465.parquet', '985024003.parquet', '1987287384.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/2044\n  Sample files: ['3982913890.parquet', '2697174421.parquet', '586385714.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/37055\n  Sample files: ['3729181235.parquet', '1669822006.parquet', '4222976236.parquet']\nDirectory: /kaggle/input/asl-signs/train_landmark_files/22343\n  Sample files: ['3825956520.parquet', '3858609451.parquet', '3531754454.parquet']\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Find the dataset directory\ndef find_dataset_directory():\n    \"\"\"Locate the dataset directory in Kaggle input\"\"\"\n    # Common directory pattern for sign language datasets\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        if 'train.csv' in filenames:\n            return dirname\n    raise FileNotFoundError(\"Could not find train.csv in input directories\")\n\n# Get the dataset path\ntry:\n    DATASET_DIR = find_dataset_directory()\n    print(f\"Found dataset at: {DATASET_DIR}\")\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\n    # Fallback option\n    DATASET_DIR = \"/kaggle/input/asl-signs\"\n    print(f\"Using fallback path: {DATASET_DIR}\")\n\n# Define paths\nTRAIN_CSV = os.path.join(DATASET_DIR, 'train.csv')\nOUTPUT_DIR = '/kaggle/working'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SignLanguageDataset(Dataset):\n    def __init__(self, landmark_paths, labels=None, max_frames=100, landmark_types=None, base_path=''):\n        \"\"\"\n        Dataset for sign language landmarks\n        \n        Args:\n            landmark_paths: List of paths to parquet files\n            labels: List of sign labels (None for test)\n            max_frames: Maximum number of frames to consider\n            landmark_types: List of landmark types to include (default: all)\n            base_path: Base directory for landmark files\n        \"\"\"\n        self.landmark_paths = landmark_paths\n        self.labels = labels\n        self.max_frames = max_frames\n        self.landmark_types = landmark_types or ['face', 'left_hand', 'pose', 'right_hand']\n        self.base_path = base_path\n        \n        # Define landmark counts based on MediaPipe\n        self.landmark_counts = {\n            'face': 468,\n            'left_hand': 21,\n            'pose': 33,\n            'right_hand': 21\n        }\n        \n        # Calculate total feature dimension\n        self.feature_dim = sum(self.landmark_counts[t] * 3 for t in self.landmark_types)\n        \n    def __len__(self):\n        return len(self.landmark_paths)\n    \n    def __getitem__(self, idx):\n        # Construct full path - handle both relative and absolute paths\n        path = self.landmark_paths[idx]\n        if not os.path.isabs(path):\n            path = os.path.join(self.base_path, path)\n        \n        # Load landmark data from parquet file\n        try:\n            df = pd.read_parquet(path)\n        except Exception as e:\n            print(f\"Error loading file {path}: {e}\")\n            # Return empty tensors in case of error\n            empty_features = np.zeros((self.max_frames, self.feature_dim))\n            empty_mask = np.zeros(self.max_frames)\n            if self.labels is not None:\n                return torch.FloatTensor(empty_features), torch.FloatTensor(empty_mask), self.labels[idx]\n            else:\n                return torch.FloatTensor(empty_features), torch.FloatTensor(empty_mask)\n            \n        # Get all frames in the sequence\n        frames = df['frame'].unique()\n        frames = sorted(frames)[:self.max_frames]  # Limit to max_frames\n        num_frames = len(frames)\n        \n        # Initialize feature tensor\n        features = np.zeros((self.max_frames, self.feature_dim))\n        \n        # Fill in features for available frames\n        for i, frame_idx in enumerate(frames):\n            if i >= self.max_frames:\n                break\n                \n            frame_data = df[df['frame'] == frame_idx]\n            \n            feature_offset = 0\n            for landmark_type in self.landmark_types:\n                type_data = frame_data[frame_data['type'] == landmark_type]\n                num_landmarks = self.landmark_counts[landmark_type]\n                \n                # Check if we have data for this type\n                if len(type_data) > 0:\n                    # Get coordinates for each landmark\n                    for coord in ['x', 'y', 'z']:\n                        coord_data = np.zeros(num_landmarks)\n                        for _, row in type_data.iterrows():\n                            if 0 <= row['landmark_index'] < num_landmarks:\n                                coord_data[int(row['landmark_index'])] = row[coord]\n                        \n                        # Add to features\n                        features[i, feature_offset:feature_offset + num_landmarks] = coord_data\n                        feature_offset += num_landmarks\n                else:\n                    # Skip this landmark type if no data\n                    feature_offset += num_landmarks * 3\n        \n        # Create a mask for valid frames\n        mask = np.zeros(self.max_frames)\n        mask[:num_frames] = 1\n        \n        if self.labels is not None:\n            return torch.FloatTensor(features), torch.FloatTensor(mask), self.labels[idx]\n        else:\n            return torch.FloatTensor(features), torch.FloatTensor(mask)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DynamicPatchingModule(nn.Module):\n    def __init__(self, input_dim, hidden_dim, patch_sizes=[4, 8, 16]):\n        \"\"\"\n        Dynamic patching module that creates variable-sized patches\n        \n        Args:\n            input_dim: Input feature dimension\n            hidden_dim: Hidden dimension for patch embeddings\n            patch_sizes: List of patch sizes to use\n        \"\"\"\n        super().__init__()\n        self.patch_sizes = patch_sizes\n        \n        # Patch embedding layers for different patch sizes\n        self.patch_embeddings = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv1d(input_dim, hidden_dim, kernel_size=p, stride=p//2),\n                nn.LayerNorm(hidden_dim)\n            ) for p in patch_sizes\n        ])\n        \n        # Fusion layer\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim * len(patch_sizes), hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU()\n        )\n        \n    def forward(self, x, mask=None):\n        \"\"\"\n        Forward pass\n        \n        Args:\n            x: Input tensor [B, T, D]\n            mask: Attention mask [B, T]\n            \n        Returns:\n            Patched embeddings [B, T', D]\n        \"\"\"\n        B, T, D = x.shape\n        x = x.transpose(1, 2)  # [B, D, T]\n        \n        # Apply different patch sizes\n        patch_outputs = []\n        for embedding in self.patch_embeddings:\n            patch_out = embedding(x)  # [B, hidden_dim, T']\n            patch_outputs.append(patch_out.transpose(1, 2))  # [B, T', hidden_dim]\n        \n        # Create new mask for patched sequence if needed\n        new_masks = []\n        if mask is not None:\n            for p in self.patch_sizes:\n                # Downsample mask according to patch size\n                pool = nn.AvgPool1d(kernel_size=p, stride=p//2, padding=p//2)\n                new_mask = pool(mask.unsqueeze(1).float()).squeeze(1)\n                new_mask = (new_mask > 0).float()\n                new_masks.append(new_mask)\n        \n        # Get shortest sequence length\n        min_seq_len = min([po.shape[1] for po in patch_outputs])\n        patch_outputs = [po[:, :min_seq_len, :] for po in patch_outputs]\n        \n        # Concatenate along feature dimension\n        concat_patches = torch.cat(patch_outputs, dim=2)  # [B, T', hidden_dim*num_patches]\n        \n        # Fuse patches\n        fused_patches = self.fusion(concat_patches)  # [B, T', hidden_dim]\n        \n        if mask is not None:\n            # Use mask from smallest patch size\n            new_mask = new_masks[0][:, :min_seq_len]\n            return fused_patches, new_mask\n        \n        return fused_patches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ByteLatentTransformer(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        hidden_dim=256,\n        num_layers=4,\n        num_heads=8,\n        patch_sizes=[4, 8, 16],\n        dropout=0.1,\n        num_classes=250  # Adjust based on your dataset\n    ):\n        \"\"\"\n        Byte Latent Transformer with dynamic patching\n        \n        Args:\n            input_dim: Input feature dimension\n            hidden_dim: Hidden dimension for transformer\n            num_layers: Number of transformer layers\n            num_heads: Number of attention heads\n            patch_sizes: List of patch sizes for dynamic patching\n            dropout: Dropout rate\n            num_classes: Number of output classes\n        \"\"\"\n        super().__init__()\n        \n        # Dynamic patching module\n        self.patching = DynamicPatchingModule(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            patch_sizes=patch_sizes\n        )\n        \n        # Positional embedding\n        self.pos_embedding = nn.Parameter(torch.zeros(1, 1000, hidden_dim))\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim*4,\n            dropout=dropout,\n            batch_first=True,\n            norm_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Latent query tokens for classification\n        self.num_latents = 8\n        self.latent_tokens = nn.Parameter(torch.randn(1, self.num_latents, hidden_dim))\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim * self.num_latents, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, num_classes)\n        )\n        \n    def forward(self, x, mask=None):\n        \"\"\"\n        Forward pass\n        \n        Args:\n            x: Input tensor [B, T, D]\n            mask: Attention mask [B, T]\n            \n        Returns:\n            Classification logits [B, num_classes]\n        \"\"\"\n        B = x.shape[0]\n        \n        # Apply dynamic patching\n        if mask is not None:\n            x, mask = self.patching(x, mask)\n        else:\n            x = self.patching(x)\n        \n        # Add positional embedding\n        x = x + self.pos_embedding[:, :x.shape[1], :]\n        \n        # Add latent tokens\n        latent_tokens = self.latent_tokens.expand(B, -1, -1)\n        x = torch.cat([latent_tokens, x], dim=1)\n        \n        # Update mask to include latent tokens (always attended to)\n        if mask is not None:\n            latent_mask = torch.ones(B, self.num_latents, device=mask.device)\n            mask = torch.cat([latent_mask, mask], dim=1)\n            \n            # Create attention mask for transformer\n            attn_mask = torch.zeros(x.shape[1], x.shape[1], device=x.device)\n            for i in range(B):\n                seq_len = x.shape[1]\n                valid_len = int(mask[i].sum().item())\n                attn_mask[valid_len:, :] = float('-inf')\n                attn_mask[:, valid_len:] = float('-inf')\n        else:\n            attn_mask = None\n        \n        # Apply transformer\n        if mask is not None:\n            x = self.transformer(x, src_key_padding_mask=(1 - mask).bool())\n        else:\n            x = self.transformer(x)\n        \n        # Extract latent tokens\n        latent_output = x[:, :self.num_latents].reshape(B, -1)  # [B, num_latents*hidden_dim]\n        \n        # Classify\n        logits = self.classifier(latent_output)\n        \n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, n_epochs=10, lr=1e-4, device=None):\n    \"\"\"Train the model\"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n    \n    best_val_acc = 0.0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n    \n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{n_epochs} [Train]')\n        for features, mask, labels in pbar:\n            features, mask, labels = features.to(device), mask.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(features, mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # Gradient clipping to prevent exploding gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            pbar.set_postfix({'loss': train_loss/(pbar.n+1), 'acc': 100.*correct/total})\n        \n        epoch_train_loss = train_loss/len(train_loader)\n        epoch_train_acc = 100.*correct/total\n        history['train_loss'].append(epoch_train_loss)\n        history['train_acc'].append(epoch_train_acc)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for features, mask, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{n_epochs} [Val]'):\n                features, mask, labels = features.to(device), mask.to(device), labels.to(device)\n                \n                outputs = model(features, mask)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        epoch_val_loss = val_loss/len(val_loader)\n        epoch_val_acc = 100.*correct/total\n        history['val_loss'].append(epoch_val_loss)\n        history['val_acc'].append(epoch_val_acc)\n        \n        print(f'Epoch {epoch+1}: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n        \n        # Save best model\n        if epoch_val_acc > best_val_acc:\n            best_val_acc = epoch_val_acc\n            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_blt_model.pth'))\n            print(f\"Saved best model with validation accuracy: {best_val_acc:.2f}%\")\n        \n        scheduler.step()\n    \n    # Plot training history\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_acc'], label='Train Acc')\n    plt.plot(history['val_acc'], label='Val Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.title('Training and Validation Accuracy')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'))\n    plt.show()\n    \n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef analyze_data_distribution(train_df):\n    \"\"\"Analyze the distribution of signs in the dataset\"\"\"\n    plt.figure(figsize=(12, 6))\n    sign_counts = train_df['sign'].value_counts()\n    \n    print(f\"Total number of unique signs: {len(sign_counts)}\")\n    print(f\"Most common signs: {sign_counts.head(10).index.tolist()}\")\n    print(f\"Least common signs: {sign_counts.tail(10).index.tolist()}\")\n    \n    plt.subplot(1, 2, 1)\n    sns.histplot(sign_counts, bins=30)\n    plt.xlabel('Count per sign')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Sign Counts')\n    \n    plt.subplot(1, 2, 2)\n    sns.barplot(x=sign_counts.index[:20], y=sign_counts.values[:20])\n    plt.xticks(rotation=90)\n    plt.xlabel('Sign')\n    plt.ylabel('Count')\n    plt.title('Top 20 Most Common Signs')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'sign_distribution.png'))\n    plt.show()\n    \n    return sign_counts\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_sequence_lengths(train_df, sample_size=1000):\n    \"\"\"Analyze the distribution of sequence lengths\"\"\"\n    # Sample to avoid loading all files\n    sampled_df = train_df.sample(min(sample_size, len(train_df)), random_state=42)\n    \n    sequence_lengths = []\n    for path in tqdm(sampled_df['path'], desc=\"Analyzing sequence lengths\"):\n        try:\n            full_path = os.path.join(DATASET_DIR, path)\n            df = pd.read_parquet(full_path)\n            num_frames = len(df['frame'].unique())\n            sequence_lengths.append(num_frames)\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n    \n    plt.figure(figsize=(10, 5))\n    sns.histplot(sequence_lengths, bins=50)\n    plt.xlabel('Sequence Length (Frames)')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Sequence Lengths')\n    plt.axvline(x=np.percentile(sequence_lengths, 95), color='r', linestyle='--', \n                label=f'95th percentile: {np.percentile(sequence_lengths, 95):.0f} frames')\n    plt.axvline(x=np.median(sequence_lengths), color='g', linestyle='--', \n                label=f'Median: {np.median(sequence_lengths):.0f} frames')\n    plt.legend()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'sequence_lengths.png'))\n    plt.show()\n    \n    print(f\"Mean sequence length: {np.mean(sequence_lengths):.2f} frames\")\n    print(f\"Median sequence length: {np.median(sequence_lengths):.2f} frames\")\n    print(f\"95th percentile length: {np.percentile(sequence_lengths, 95):.2f} frames\")\n    \n    return sequence_lengths\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_with_augmentation(model, train_loader, val_loader, n_epochs=10, lr=1e-4):\n    \"\"\"Train with data augmentation strategies\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # Data augmentation functions\n    def time_warp(x, mask, factor=0.2):\n        \"\"\"Stretch or compress sequence in time dimension\"\"\"\n        b, t, d = x.shape\n        if torch.rand(1).item() > 0.5:\n            # Apply time warping\n            orig_len = mask.sum(dim=1).int()\n            max_warp = (orig_len * factor).int()\n            warp_len = torch.randint(-max_warp, max_warp+1, (b,))\n            warp_len = torch.clamp(orig_len + warp_len, min=10, max=t-1)\n            \n            new_x = torch.zeros_like(x)\n            new_mask = torch.zeros_like(mask)\n            \n            for i in range(b):\n                # Resize via linear interpolation\n                if orig_len[i] > 0:  # Avoid empty sequences\n                    indices = torch.linspace(0, orig_len[i]-1, warp_len[i])\n                    indices = indices.to(torch.long)\n                    valid_x = x[i, :orig_len[i]]\n                    new_x[i, :warp_len[i]] = valid_x[indices]\n                    new_mask[i, :warp_len[i]] = 1\n            \n            return new_x, new_mask\n        return x, mask\n    \n    def random_mask(x, mask, mask_ratio=0.1):\n        \"\"\"Randomly mask some frames\"\"\"\n        if torch.rand(1).item() > 0.5:\n            b, t, d = x.shape\n            for i in range(b):\n                valid_length = int(mask[i].sum().item())\n                if valid_length > 3:  # Don't mask if sequence is too short\n                    mask_length = int(valid_length * mask_ratio)\n                    start_idx = torch.randint(0, valid_length - mask_length, (1,)).item()\n                    x[i, start_idx:start_idx+mask_length] = 0\n        return x, mask\n    \n    def channel_mask(x, mask, feature_mask_ratio=0.05):\n        \"\"\"Randomly mask some feature channels\"\"\"\n        if torch.rand(1).item() > 0.5:\n            b, t, d = x.shape\n            mask_size = int(d * feature_mask_ratio)\n            for i in range(b):\n                mask_indices = torch.randperm(d)[:mask_size]\n                x[i, :, mask_indices] = 0\n        return x, mask\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n    \n    best_val_acc = 0.0\n    \n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{n_epochs} [Train]')\n        for features, mask, labels in pbar:\n            features, mask, labels = features.to(device), mask.to(device), labels.to(device)\n            \n            # Apply augmentations\n            features, mask = time_warp(features, mask)\n            features, mask = random_mask(features, mask)\n            features, mask = channel_mask(features, mask)\n            \n            optimizer.zero_grad()\n            outputs = model(features, mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            pbar.set_postfix({'loss': train_loss/len(train_loader), 'acc': 100.*correct/total})\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for features, mask, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{n_epochs} [Val]'):\n                features, mask, labels = features.to(device), mask.to(device), labels.to(device)\n                \n                outputs = model(features, mask)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_acc = 100.*correct/total\n        print(f'Validation Acc: {val_acc:.2f}%, Loss: {val_loss/len(val_loader):.4f}')\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_aug_model.pth'))\n        \n        scheduler.step()\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting BLT Sign Language Classification\")\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Load training data\ntry:\n    train_df = pd.read_csv(TRAIN_CSV)\n    print(f\"Loaded {len(train_df)} training samples\")\nexcept Exception as e:\n    print(f\"Error loading train.csv: {e}\")\n    return\n\n# Analyze data\nprint(\"\\nAnalyzing data distribution...\")\nsign_counts = analyze_data_distribution(train_df)\n\nprint(\"\\nAnalyzing sequence lengths...\")\nsequence_lengths = analyze_sequence_lengths(train_df, sample_size=500)\n\n# Calculate recommended max_frames\nrecommended_max_frames = int(np.percentile(sequence_lengths, 95))\nprint(f\"Recommended max_frames: {recommended_max_frames}\")\n\n# Create label encoder\nlabel_encoder = LabelEncoder()\ntrain_df['label_encoded'] = label_encoder.fit_transform(train_df['sign'])\nnum_classes = len(label_encoder.classes_)\nprint(f\"Number of classes: {num_classes}\")\n\n# Save label encoder mapping\nlabel_mapping = pd.DataFrame({\n    'sign': label_encoder.classes_,\n    'label_encoded': range(len(label_encoder.classes_))\n})\nlabel_mapping.to_csv(os.path.join(OUTPUT_DIR, 'label_mapping.csv'), index=False)\n\n# Split data\ntrain_indices, val_indices = train_test_split(\n    range(len(train_df)), test_size=0.2, random_state=42, \n    stratify=train_df['sign']\n)\n\n# Create datasets\ntrain_dataset = SignLanguageDataset(\n    landmark_paths=train_df['path'].iloc[train_indices].tolist(),\n    labels=train_df['label_encoded'].iloc[train_indices].tolist(),\n    max_frames=recommended_max_frames,\n    base_path=DATASET_DIR\n)\n\nval_dataset = SignLanguageDataset(\n    landmark_paths=train_df['path'].iloc[val_indices].tolist(),\n    labels=train_df['label_encoded'].iloc[val_indices].tolist(),\n    max_frames=recommended_max_frames,\n    base_path=DATASET_DIR\n)\n\nprint(f\"Feature dimension: {train_dataset.feature_dim}\")\n\n# Create dataloaders - adjust batch size based on your GPU memory\n# Start with smaller batch size for Kaggle\nbatch_size = 16  # Adjust based on available memory\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, num_workers=2)\n\n# Initialize model\nmodel = ByteLatentTransformer(\n    input_dim=train_dataset.feature_dim,\n    hidden_dim=192,  # Reduced for memory constraints\n    num_layers=3,    # Reduced for memory constraints\n    num_heads=6,\n    patch_sizes=[4, 8, 16],\n    num_classes=num_classes\n)\n\n# Check model size\nparam_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Model trainable parameters: {param_count / 1e6:.2f}M\")\n\n# Train model\nprint(\"\\nStarting training...\")\nmodel, history = train_model(\n    model, \n    train_loader, \n    val_loader, \n    n_epochs=15,\n    lr=5e-5  # Lower learning rate for stability\n)\n\n# Save final model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'label_encoder': label_encoder.classes_,\n    'feature_dim': train_dataset.feature_dim,\n    'hidden_dim': 192,\n    'num_classes': num_classes,\n    'max_frames': recommended_max_frames\n}, os.path.join(OUTPUT_DIR, 'final_model.pth'))\n\nprint(\"Training completed. Model saved to working directory.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}