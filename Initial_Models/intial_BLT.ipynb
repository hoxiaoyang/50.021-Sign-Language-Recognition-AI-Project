{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SignLanguageDataset(Dataset):\n    def __init__(self, landmark_paths, labels=None, max_frames=100, landmark_types=None, base_path=''):\n        \"\"\"\n        Dataset for sign language landmarks\n        \n        Args:\n            landmark_paths: List of paths to parquet files\n            labels: List of sign labels (None for test)\n            max_frames: Maximum number of frames to consider\n            landmark_types: List of landmark types to include (default: all)\n            base_path: Base directory for landmark files\n        \"\"\"\n        self.landmark_paths = landmark_paths\n        self.labels = labels\n        self.max_frames = max_frames\n        self.landmark_types = landmark_types or ['face', 'left_hand', 'pose', 'right_hand']\n        self.base_path = base_path\n        \n        # Define landmark counts based on MediaPipe\n        self.landmark_counts = {\n            'face': 468,\n            'left_hand': 21,\n            'pose': 33,\n            'right_hand': 21\n        }\n        \n        # Calculate total feature dimension\n        self.feature_dim = sum(self.landmark_counts[t] * 3 for t in self.landmark_types)\n        \n    def __len__(self):\n        return len(self.landmark_paths)\n    \n    def __getitem__(self, idx):\n        # Construct full path - handle both relative and absolute paths\n        path = self.landmark_paths[idx]\n        if not os.path.isabs(path):\n            path = os.path.join(self.base_path, path)\n        \n        # Load landmark data from parquet file\n        try:\n            df = pd.read_parquet(path)\n        except Exception as e:\n            print(f\"Error loading file {path}: {e}\")\n            # Return empty tensors in case of error\n            empty_features = np.zeros((self.max_frames, self.feature_dim))\n            empty_mask = np.zeros(self.max_frames)\n            if self.labels is not None:\n                return torch.FloatTensor(empty_features), torch.FloatTensor(empty_mask), self.labels[idx]\n            else:\n                return torch.FloatTensor(empty_features), torch.FloatTensor(empty_mask)\n            \n        # Get all frames in the sequence\n        frames = df['frame'].unique()\n        frames = sorted(frames)[:self.max_frames]  # Limit to max_frames\n        num_frames = len(frames)\n        \n        # Initialize feature tensor\n        features = np.zeros((self.max_frames, self.feature_dim))\n        \n        # Fill in features for available frames\n        for i, frame_idx in enumerate(frames):\n            if i >= self.max_frames:\n                break\n                \n            frame_data = df[df['frame'] == frame_idx]\n            \n            feature_offset = 0\n            for landmark_type in self.landmark_types:\n                type_data = frame_data[frame_data['type'] == landmark_type]\n                num_landmarks = self.landmark_counts[landmark_type]\n                \n                # Check if we have data for this type\n                if len(type_data) > 0:\n                    # Get coordinates for each landmark\n                    for coord in ['x', 'y', 'z']:\n                        coord_data = np.zeros(num_landmarks)\n                        for _, row in type_data.iterrows():\n                            if 0 <= row['landmark_index'] < num_landmarks:\n                                coord_data[int(row['landmark_index'])] = row[coord]\n                        \n                        # Add to features\n                        features[i, feature_offset:feature_offset + num_landmarks] = coord_data\n                        feature_offset += num_landmarks\n                else:\n                    # Skip this landmark type if no data\n                    feature_offset += num_landmarks * 3\n        \n        # Create a mask for valid frames\n        mask = np.zeros(self.max_frames)\n        mask[:num_frames] = 1\n        \n        if self.labels is not None:\n            return torch.FloatTensor(features), torch.FloatTensor(mask), self.labels[idx]\n        else:\n            return torch.FloatTensor(features), torch.FloatTensor(mask)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DynamicPatching(nn.Module):\n    def __init__(self, input_dim, hidden_dim, kernel_sizes=[3, 5, 10], n_layers=1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_sizes = kernel_sizes\n        \n        # Create separate convolutional embedding for each kernel size\n        self.patch_embeddings = nn.ModuleList()\n        for kernel_size in kernel_sizes:\n            layers = []\n            for i in range(n_layers):\n                if i == 0:\n                    layers.append(nn.Conv1d(\n                        in_channels=input_dim,\n                        out_channels=hidden_dim,\n                        kernel_size=kernel_size,\n                        padding='same'\n                    ))\n                else:\n                    layers.append(nn.Conv1d(\n                        in_channels=hidden_dim,\n                        out_channels=hidden_dim,\n                        kernel_size=kernel_size,\n                        padding='same'\n                    ))\n                layers.append(nn.ReLU())\n            self.patch_embeddings.append(nn.Sequential(*layers))\n        \n        # Layer norm with correct normalized_shape\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        \n        # Combine with weighted sum\n        self.patch_weights = nn.Parameter(torch.ones(len(kernel_sizes)) / len(kernel_sizes))\n        self.softmax = nn.Softmax(dim=0)\n    \n    def forward(self, x, mask=None):\n        # x: [B, T, input_dim]\n        batch_size, seq_len, _ = x.shape\n        \n        # Transpose for conv1d: [B, input_dim, T]\n        x = x.transpose(1, 2)\n        \n        patch_outputs = []\n        for embedding in self.patch_embeddings:\n            patch_out = embedding(x)  # [B, hidden_dim, T]\n            patch_outputs.append(patch_out.transpose(1, 2))  # [B, T, hidden_dim]\n        \n        # Apply weighted sum with learnable weights\n        weights = self.softmax(self.patch_weights)\n        combined = torch.zeros_like(patch_outputs[0])\n        for i, patch_output in enumerate(patch_outputs):\n            combined += weights[i] * patch_output\n        \n        # Apply layer norm with correct dimensions\n        # Since combined is [B, T, hidden_dim], the layer_norm expects normalized_shape=[hidden_dim]\n        normalized = self.layer_norm(combined)\n        \n        if mask is not None:\n            return normalized, mask\n        return normalized","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ByteLatentTransformer(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, n_heads=4, n_layers=3, \n                 kernel_sizes=[3, 5, 10], dropout=0.1):\n        super().__init__()\n        \n        self.patching = DynamicPatching(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            kernel_sizes=kernel_sizes\n        )\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=n_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            batch_first=True,\n            norm_first=True\n        )\n        \n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer,\n            num_layers=n_layers\n        )\n        \n        self.fc_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x, mask=None):\n        # Apply dynamic patching\n        if mask is not None:\n            x, mask = self.patching(x, mask)\n        else:\n            x = self.patching(x)\n        \n        # Apply transformer encoder\n        if mask is not None:\n            x = self.transformer_encoder(x, src_key_padding_mask=mask)\n        else:\n            x = self.transformer_encoder(x)\n        \n        # Global pooling (mean of non-masked positions)\n        if mask is not None:\n            # Ensure mask is boolean type\n            if not mask.dtype == torch.bool:\n                mask = mask.bool()\n                \n            # Create an inverted mask for averaging: True for real tokens (not padding), False for padding\n            attention_mask = (~mask).float().unsqueeze(-1)  # This is now 1 for tokens to keep, 0 for padding\n            \n            # Sum all non-masked token embeddings and divide by count\n            pooled = (x * attention_mask).sum(dim=1) / attention_mask.sum(dim=1).clamp(min=1e-9)\n        else:\n            # Simple mean pooling if no mask\n            pooled = x.mean(dim=1)\n        \n        # Apply classification head\n        output = self.fc_head(pooled)\n        \n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, n_epochs=10, lr=1e-4, device='cuda'):\n    model = model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n    \n    best_val_acc = 0.0\n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n    \n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (features, mask, labels) in enumerate(train_loader):\n            features = features.to(device)\n            if mask is not None:\n                # Ensure mask is the correct type before sending to device\n                if not mask.dtype == torch.bool:\n                    mask = mask.bool()\n                mask = mask.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(features, mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            if batch_idx % 20 == 0:\n                print(f'Epoch: {epoch+1}/{n_epochs} | Batch: {batch_idx}/{len(train_loader)} | '\n                      f'Loss: {loss.item():.4f} | Acc: {100.*correct/total:.2f}%')\n        \n        train_loss = train_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for features, mask, labels in val_loader:\n                features = features.to(device)\n                if mask is not None:\n                    # Ensure mask is the correct type before sending to device\n                    if not mask.dtype == torch.bool:\n                        mask = mask.bool()\n                    mask = mask.to(device)\n                labels = labels.to(device)\n                \n                outputs = model(features, mask)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_loss = val_loss / len(val_loader)\n        val_acc = 100. * correct / total\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        \n        print(f'\\nEpoch: {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | '\n              f'Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(f'Model saved with validation accuracy: {val_acc:.2f}%')\n        \n        scheduler.step()\n    \n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef analyze_data_distribution(train_df):\n    \"\"\"Analyze the distribution of signs in the dataset\"\"\"\n    plt.figure(figsize=(12, 6))\n    sign_counts = train_df['sign'].value_counts()\n    \n    print(f\"Total number of unique signs: {len(sign_counts)}\")\n    print(f\"Most common signs: {sign_counts.head(10).index.tolist()}\")\n    print(f\"Least common signs: {sign_counts.tail(10).index.tolist()}\")\n    \n    plt.subplot(1, 2, 1)\n    sns.histplot(sign_counts, bins=30)\n    plt.xlabel('Count per sign')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Sign Counts')\n    \n    plt.subplot(1, 2, 2)\n    sns.barplot(x=sign_counts.index[:20], y=sign_counts.values[:20])\n    plt.xticks(rotation=90)\n    plt.xlabel('Sign')\n    plt.ylabel('Count')\n    plt.title('Top 20 Most Common Signs')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'sign_distribution.png'))\n    plt.show()\n    \n    return sign_counts\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_sequence_lengths(train_df, sample_size=1000):\n    \"\"\"Analyze the distribution of sequence lengths\"\"\"\n    # Sample to avoid loading all files\n    sampled_df = train_df.sample(min(sample_size, len(train_df)), random_state=42)\n    \n    sequence_lengths = []\n    for path in tqdm(sampled_df['path'], desc=\"Analyzing sequence lengths\"):\n        try:\n            full_path = os.path.join(DATASET_DIR, path)\n            df = pd.read_parquet(full_path)\n            num_frames = len(df['frame'].unique())\n            sequence_lengths.append(num_frames)\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n    \n    plt.figure(figsize=(10, 5))\n    sns.histplot(sequence_lengths, bins=50)\n    plt.xlabel('Sequence Length (Frames)')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Sequence Lengths')\n    plt.axvline(x=np.percentile(sequence_lengths, 95), color='r', linestyle='--', \n                label=f'95th percentile: {np.percentile(sequence_lengths, 95):.0f} frames')\n    plt.axvline(x=np.median(sequence_lengths), color='g', linestyle='--', \n                label=f'Median: {np.median(sequence_lengths):.0f} frames')\n    plt.legend()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'sequence_lengths.png'))\n    plt.show()\n    \n    print(f\"Mean sequence length: {np.mean(sequence_lengths):.2f} frames\")\n    print(f\"Median sequence length: {np.median(sequence_lengths):.2f} frames\")\n    print(f\"95th percentile length: {np.percentile(sequence_lengths, 95):.2f} frames\")\n    \n    return sequence_lengths\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    print(\"Starting BLT Sign Language Classification\")\n    print(f\"Working directory: {os.getcwd()}\")\n    \n    # Load training data\n    try:\n        train_df = pd.read_csv(TRAIN_CSV)\n        print(f\"Loaded {len(train_df)} training samples\")\n    except Exception as e:\n        print(f\"Error loading train.csv: {e}\")\n        # Fix the syntax error - remove return statement\n        print(\"Could not load training data. Please check the file path.\")\n        # Continue with other operations if possible\n    \n    # Check if training data is loaded before proceeding\n    if 'train_df' in locals() and len(train_df) > 0:\n        # Analyze data\n        print(\"\\nAnalyzing data distribution...\")\n        sign_counts = analyze_data_distribution(train_df)\n        \n        print(\"\\nAnalyzing sequence lengths...\")\n        sequence_lengths = analyze_sequence_lengths(train_df, sample_size=500)\n        \n        # Calculate recommended max_frames\n        recommended_max_frames = int(np.percentile(sequence_lengths, 95))\n        print(f\"Recommended max_frames: {recommended_max_frames}\")\n        \n        # Create label encoder\n        label_encoder = LabelEncoder()\n        train_df['label_encoded'] = label_encoder.fit_transform(train_df['sign'])\n        num_classes = len(label_encoder.classes_)\n        print(f\"Number of classes: {num_classes}\")\n        \n        # Save label encoder mapping\n        label_mapping = pd.DataFrame({\n            'sign': label_encoder.classes_,\n            'label_encoded': range(len(label_encoder.classes_))\n        })\n        label_mapping.to_csv(os.path.join(OUTPUT_DIR, 'label_mapping.csv'), index=False)\n        \n        # Split data\n        train_indices, val_indices = train_test_split(\n            range(len(train_df)), test_size=0.2, random_state=42, \n            stratify=train_df['sign']\n        )\n        \n        # Create datasets\n        train_dataset = SignLanguageDataset(\n            landmark_paths=train_df['path'].iloc[train_indices].tolist(),\n            labels=train_df['label_encoded'].iloc[train_indices].tolist(),\n            max_frames=recommended_max_frames,\n            base_path=DATASET_DIR\n        )\n        \n        val_dataset = SignLanguageDataset(\n            landmark_paths=train_df['path'].iloc[val_indices].tolist(),\n            labels=train_df['label_encoded'].iloc[val_indices].tolist(),\n            max_frames=recommended_max_frames,\n            base_path=DATASET_DIR\n        )\n        \n        print(f\"Feature dimension: {train_dataset.feature_dim}\")\n        \n        # Create dataloaders - adjust batch size based on your GPU memory\n        # Start with smaller batch size for Kaggle\n        batch_size = 16  # Adjust based on available memory\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, num_workers=2)\n        \n        # Initialize model\n        model = ByteLatentTransformer(\n            input_dim=train_dataset.feature_dim,\n            hidden_dim=192,  # Reduced for memory constraints\n            output_dim=num_classes,\n            n_layers=3,  # Use n_layers instead of num_layers\n            n_heads=4,\n            kernel_sizes=[3, 5, 10],\n            dropout=0.1\n        )\n        \n        # Check model size\n        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Model trainable parameters: {param_count / 1e6:.2f}M\")\n        \n        # Train model\n        print(\"\\nStarting training...\")\n        model, history = train_model(\n            model, \n            train_loader, \n            val_loader, \n            n_epochs=15,\n            lr=5e-5  # Lower learning rate for stability\n        )\n        \n        # Save final model\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'label_encoder': label_encoder.classes_,\n            'feature_dim': train_dataset.feature_dim,\n            'hidden_dim': 192,\n            'num_classes': num_classes,\n            'max_frames': recommended_max_frames\n        }, os.path.join(OUTPUT_DIR, 'final_model.pth'))\n        \n        print(\"Training completed. Model saved to working directory.\")\n    else:\n        print(\"Could not proceed with training due to missing data.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the main function when executed\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}