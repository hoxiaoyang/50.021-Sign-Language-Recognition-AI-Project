{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 0. Dataset Description\n*Taken from dataset [Kaggle page](https://www.kaggle.com/competitions/asl-signs/data)*\n\nDeaf children are often born to hearing parents who do not know sign language. Your challenge in this competition is to help identify signs made in processed videos, which will support the development of mobile apps to help teach parents sign language so they can communicate with their Deaf children.\n\n### 0.1. Files\n`train_landmark_files/[participant_id]/[sequence_id].parquet` \n\nThe landmark data. The landmarks were extracted from raw videos with the MediaPipe holistic model. Not all of the frames necessarily had visible hands or hands that could be detected by the model.\n\n- frame - The frame number in the raw video.\n- row_id - A unique identifier for the row.\n- type - The type of landmark. One of ['face', 'left_hand', 'pose', 'right_hand'].\n- landmark_index - The landmark index number. Details of the hand landmark locations can be found here.\n- [x/y/z] - The normalized spatial coordinates of the landmark. These are the only columns that will be provided to your submitted model for inference. The MediaPipe model is not fully trained to predict depth so you may wish to ignore the z values.\n\n`train.csv`\n- path - The path to the landmark file.\n- participant_id - A unique identifier for the data contributor.\n- sequence_id - A unique identifier for the landmark sequence.\n- sign - The label for the landmark sequence.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup","metadata":{}},{"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:49.519848Z","iopub.execute_input":"2025-04-12T13:30:49.520130Z","iopub.status.idle":"2025-04-12T13:30:49.841383Z","shell.execute_reply.started":"2025-04-12T13:30:49.520100Z","shell.execute_reply":"2025-04-12T13:30:49.840417Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"BASE_DIR = \"../input/asl-signs/\"\n\n# Read in train dataset\n\ndf = pd.read_csv(f\"{BASE_DIR}/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:49.842372Z","iopub.execute_input":"2025-04-12T13:30:49.842836Z","iopub.status.idle":"2025-04-12T13:30:50.035602Z","shell.execute_reply.started":"2025-04-12T13:30:49.842803Z","shell.execute_reply":"2025-04-12T13:30:50.034589Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 2. Dataset Information","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:50.036603Z","iopub.execute_input":"2025-04-12T13:30:50.036900Z","iopub.status.idle":"2025-04-12T13:30:50.064078Z","shell.execute_reply.started":"2025-04-12T13:30:50.036861Z","shell.execute_reply":"2025-04-12T13:30:50.063237Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                            path  participant_id  sequence_id  \\\n0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n\n    sign  \n0   blow  \n1   wait  \n2  cloud  \n3   bird  \n4   owie  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:50.066073Z","iopub.execute_input":"2025-04-12T13:30:50.066432Z","iopub.status.idle":"2025-04-12T13:30:50.071176Z","shell.execute_reply.started":"2025-04-12T13:30:50.066411Z","shell.execute_reply":"2025-04-12T13:30:50.070359Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(94477, 4)"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"`train.csv` has 4 features: the path to each parquet file, the corresponding participant's id, the sequence id, and the sign. Contains 94477 entries.","metadata":{}},{"cell_type":"code","source":"df[\"sign\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:50.072577Z","iopub.execute_input":"2025-04-12T13:30:50.072839Z","iopub.status.idle":"2025-04-12T13:30:50.098178Z","shell.execute_reply.started":"2025-04-12T13:30:50.072806Z","shell.execute_reply":"2025-04-12T13:30:50.097310Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"sign\nlisten    415\nlook      414\nshhh      411\ndonkey    410\nmouse     408\n         ... \ndance     312\nperson    312\nbeside    310\nvacuum    307\nzipper    299\nName: count, Length: 250, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"top_k = 20 # How many signs to plot \nfig, ax = plt.subplots(figsize=(8, 8))\ndf[\"sign\"].value_counts().head(top_k).sort_values(ascending=True).plot(\n    kind=\"barh\", ax=ax, title=f\"Top {top_k} Signs in Training Dataset\"\n)\nax.set_xlabel(\"Number of Training Samples\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:50.099169Z","iopub.execute_input":"2025-04-12T13:30:50.099480Z","iopub.status.idle":"2025-04-12T13:30:50.565973Z","shell.execute_reply.started":"2025-04-12T13:30:50.099454Z","shell.execute_reply":"2025-04-12T13:30:50.565164Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 8))\ndf[\"sign\"].value_counts().tail(top_k).sort_values(ascending=True).plot(\n    kind=\"barh\", ax=ax, title=f\"Bottom {top_k} Signs in Training Dataset\"\n)\nax.set_xlabel(\"Number of Training Samples\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:50.566744Z","iopub.execute_input":"2025-04-12T13:30:50.566991Z","iopub.status.idle":"2025-04-12T13:30:50.863632Z","shell.execute_reply.started":"2025-04-12T13:30:50.566970Z","shell.execute_reply":"2025-04-12T13:30:50.862555Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"Recall that the Parquet files are stored in the `train_landmark_files` directory. Each entry in `train.csv` contains the path to a specific Parquet file.","metadata":{}},{"cell_type":"code","source":"# Get the path to a Parquet file that has the \"cow\" sign\ntarget_sign = \"cow\"\nexample_parquet_path = df.query(f\"sign == '{target_sign}'\")[\"path\"].values[0]\nexample_parquet_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:50.864591Z","iopub.execute_input":"2025-04-12T13:30:50.864829Z","iopub.status.idle":"2025-04-12T13:30:50.879946Z","shell.execute_reply.started":"2025-04-12T13:30:50.864799Z","shell.execute_reply":"2025-04-12T13:30:50.879096Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'train_landmark_files/36257/1021205595.parquet'"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"We can load the parquet file into a `Dataframe` to get the landmark information.","metadata":{}},{"cell_type":"code","source":"landmark_df = pd.read_parquet(f\"{BASE_DIR}/{example_parquet_path}\")\nlandmark_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:50.880965Z","iopub.execute_input":"2025-04-12T13:30:50.881326Z","iopub.status.idle":"2025-04-12T13:30:51.096823Z","shell.execute_reply.started":"2025-04-12T13:30:50.881292Z","shell.execute_reply":"2025-04-12T13:30:51.096099Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   frame    row_id  type  landmark_index         x         y         z\n0      1  1-face-0  face               0  0.476809  0.456741 -0.042842\n1      1  1-face-1  face               1  0.469517  0.416260 -0.068233\n2      1  1-face-2  face               2  0.471521  0.429064 -0.038716\n3      1  1-face-3  face               3  0.453238  0.387312 -0.047121\n4      1  1-face-4  face               4  0.467671  0.406001 -0.071225","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frame</th>\n      <th>row_id</th>\n      <th>type</th>\n      <th>landmark_index</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1-face-0</td>\n      <td>face</td>\n      <td>0</td>\n      <td>0.476809</td>\n      <td>0.456741</td>\n      <td>-0.042842</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1-face-1</td>\n      <td>face</td>\n      <td>1</td>\n      <td>0.469517</td>\n      <td>0.416260</td>\n      <td>-0.068233</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1-face-2</td>\n      <td>face</td>\n      <td>2</td>\n      <td>0.471521</td>\n      <td>0.429064</td>\n      <td>-0.038716</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1-face-3</td>\n      <td>face</td>\n      <td>3</td>\n      <td>0.453238</td>\n      <td>0.387312</td>\n      <td>-0.047121</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1-face-4</td>\n      <td>face</td>\n      <td>4</td>\n      <td>0.467671</td>\n      <td>0.406001</td>\n      <td>-0.071225</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"example_landmark = landmark_df\nunique_frames = example_landmark[\"frame\"].nunique()\nunique_types = example_landmark[\"type\"].nunique()\ntypes_in_video = example_landmark[\"type\"].unique()\nprint(\n    f\"The file has {unique_frames} unique frames and {unique_types} unique types: {types_in_video}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:51.097549Z","iopub.execute_input":"2025-04-12T13:30:51.097874Z","iopub.status.idle":"2025-04-12T13:30:51.110772Z","shell.execute_reply.started":"2025-04-12T13:30:51.097852Z","shell.execute_reply":"2025-04-12T13:30:51.109912Z"}},"outputs":[{"name":"stdout","text":"The file has 48 unique frames and 4 unique types: ['face' 'left_hand' 'pose' 'right_hand']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"landmark_df[\"frame\"].value_counts().sort_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:51.111785Z","iopub.execute_input":"2025-04-12T13:30:51.112120Z","iopub.status.idle":"2025-04-12T13:30:51.127874Z","shell.execute_reply.started":"2025-04-12T13:30:51.112090Z","shell.execute_reply":"2025-04-12T13:30:51.127184Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"frame\n1     543\n2     543\n3     543\n4     543\n5     543\n6     543\n7     543\n8     543\n9     543\n10    543\n11    543\n12    543\n13    543\n14    543\n15    543\n16    543\n17    543\n18    543\n19    543\n20    543\n21    543\n22    543\n23    543\n24    543\n25    543\n26    543\n27    543\n28    543\n29    543\n30    543\n31    543\n32    543\n33    543\n34    543\n35    543\n36    543\n37    543\n38    543\n39    543\n40    543\n41    543\n42    543\n43    543\n44    543\n45    543\n46    543\n47    543\n48    543\nName: count, dtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## 3. WIP - 1D CNN preprocessing","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport gc\nimport os\nimport torch.amp as amp\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nfrom sklearn.model_selection import train_test_split\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:51.128615Z","iopub.execute_input":"2025-04-12T13:30:51.128865Z","iopub.status.idle":"2025-04-12T13:30:55.529488Z","shell.execute_reply.started":"2025-04-12T13:30:51.128834Z","shell.execute_reply":"2025-04-12T13:30:55.528803Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\n\nprint(\"CUDA Available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"Using device:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"Using CPU\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:55.532526Z","iopub.execute_input":"2025-04-12T13:30:55.532890Z","iopub.status.idle":"2025-04-12T13:30:55.652027Z","shell.execute_reply.started":"2025-04-12T13:30:55.532869Z","shell.execute_reply":"2025-04-12T13:30:55.651303Z"}},"outputs":[{"name":"stdout","text":"CUDA Available: True\nUsing device: Tesla T4\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:55.653463Z","iopub.execute_input":"2025-04-12T13:30:55.653723Z","iopub.status.idle":"2025-04-12T13:30:55.656914Z","shell.execute_reply.started":"2025-04-12T13:30:55.653701Z","shell.execute_reply":"2025-04-12T13:30:55.656115Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def seed_everything(seed=42):\n    # Set PYTHONHASHSEED for reproducibility\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    # Set random seed for Python's random module\n    random.seed(seed)\n    \n    # Set random seed for numpy\n    np.random.seed(seed)\n    \n    # Set random seed for PyTorch (both CPU and GPU)\n    torch.manual_seed(seed)\n    \n    # For GPU support\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)  # for all devices (GPUs)\n    \n    # Ensure deterministic behavior for reproducibility\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:55.657765Z","iopub.execute_input":"2025-04-12T13:30:55.658025Z","iopub.status.idle":"2025-04-12T13:30:55.677409Z","shell.execute_reply.started":"2025-04-12T13:30:55.658005Z","shell.execute_reply":"2025-04-12T13:30:55.676633Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nROWS_PER_FRAME = 543\nMAX_LEN = 384\nCROP_LEN = MAX_LEN\nNUM_CLASSES  = 250\nPAD = -100.\nNOSE=[\n    1,2,98,327\n]\nLNOSE = [98]\nRNOSE = [327]\nLIP = [ 0, \n    61, 185, 40, 39, 37, 267, 269, 270, 409,\n    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n]\nLLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\nRLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n\nPOSE = [500, 502, 504, 501, 503, 505, 512, 513]\nLPOSE = [513,505,503,501]\nRPOSE = [512,504,502,500]\n\nREYE = [\n    33, 7, 163, 144, 145, 153, 154, 155, 133,\n    246, 161, 160, 159, 158, 157, 173,\n]\nLEYE = [\n    263, 249, 390, 373, 374, 380, 381, 382, 362,\n    466, 388, 387, 386, 385, 384, 398,\n]\n\nLHAND = np.arange(468, 489).tolist()\nRHAND = np.arange(522, 543).tolist()\n\nPOINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n\nNUM_NODES = len(POINT_LANDMARKS)\nCHANNELS = 6*NUM_NODES\n\nprint(NUM_NODES)\nprint(CHANNELS)\n\ndef interp1d_(x, target_len, method='random'):\n    \"\"\"Interpolates the input tensor to a target length.\"\"\"\n    length = x.shape[1]\n    target_len = max(1, target_len)\n    \n    if method == 'random':\n        # Randomly choose one of the interpolation methods\n        rand_val = torch.rand(())\n        if rand_val < 0.33:\n            method = 'bilinear'\n        else:\n            method = 'bicubic' if torch.rand(()) < 0.5 else 'nearest'\n    \n    # Resize the tensor using the chosen method\n    x = F.interpolate(x, size=(target_len, x.shape[2]), mode=method, align_corners=False)\n    return x\n\ndef torch_nan_mean(x, dim=0, keepdim=False):\n    \"\"\"Calculates mean, ignoring NaNs.\"\"\"\n    mask = ~torch.isnan(x)\n    x = torch.where(mask, x, torch.zeros_like(x))\n    count = mask.sum(dim=dim, keepdim=keepdim).clamp(min=1)  # Avoid division by zero\n    return x.sum(dim=dim, keepdim=keepdim) / count\n\ndef torch_nan_std(x, center=None, dim=0, keepdim=False):\n    \"\"\"Calculates standard deviation, ignoring NaNs.\"\"\"\n    if center is None:\n        center = torch_nan_mean(x, dim=dim, keepdim=True)\n    d = x - center\n    variance = torch_nan_mean(d * d, dim=dim, keepdim=keepdim)\n    return torch.sqrt(variance)\n\nclass Preprocess(nn.Module):\n    def __init__(self, max_len=None, point_landmarks=None):\n        super(Preprocess, self).__init__()\n        self.max_len = max_len\n        self.point_landmarks = point_landmarks\n\n    def forward(self, inputs):\n        # Ensure input is 4D (batch, time, points, channels)\n        if inputs.dim() == 3:\n            x = inputs.unsqueeze(0)\n        else:\n            x = inputs\n\n        # Calculate mean over landmarks (axis 2)\n        mean = torch_nan_mean(x[:, :, [17], :], dim=(1, 2), keepdim=True)\n        mean = torch.where(torch.isnan(mean), torch.tensor(0.5, dtype=x.dtype, device=x.device), mean)\n\n        # Gather the specified landmarks and calculate the standard deviation\n        x = x[:, :, self.point_landmarks, :]  # N, T, P, C\n        std = torch_nan_std(x, mean, dim=(1, 2), keepdim=True)\n\n        # Normalize the data\n        x = (x - mean) / std\n\n        # Trim the sequence to max_len if specified\n        if self.max_len is not None:\n            x = x[:, :self.max_len]\n        length = x.shape[1]\n\n        # Keep only the first two coordinates\n        x = x[..., :2]\n\n        # Compute first and second differences\n        if x.shape[1] > 1:\n            dx = torch.cat([x[:, 1:] - x[:, :-1], torch.zeros_like(x[:, :1])], dim=1)\n        else:\n            dx = torch.zeros_like(x)\n\n        if x.shape[1] > 2:\n            dx2 = torch.cat([x[:, 2:] - x[:, :-2], torch.zeros_like(x[:, :2])], dim=1)\n        else:\n            dx2 = torch.zeros_like(x)\n\n        # Concatenate the features\n        x = torch.cat([\n            x.reshape(-1, length, 2 * len(self.point_landmarks)),\n            dx.reshape(-1, length, 2 * len(self.point_landmarks)),\n            dx2.reshape(-1, length, 2 * len(self.point_landmarks)),\n        ], dim=-1)\n\n        # Replace NaNs with zeros\n        x = torch.where(torch.isnan(x), torch.tensor(0., dtype=x.dtype, device=x.device), x)\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:55.677991Z","iopub.execute_input":"2025-04-12T13:30:55.678178Z","iopub.status.idle":"2025-04-12T13:30:55.696172Z","shell.execute_reply.started":"2025-04-12T13:30:55.678161Z","shell.execute_reply":"2025-04-12T13:30:55.695537Z"}},"outputs":[{"name":"stdout","text":"118\n708\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Helper function to replace NaNs with zeroes\ndef torch_nanmean(x, dim=0, keepdim=False):\n    mask = ~torch.isnan(x)\n    x = torch.where(mask, x, torch.zeros_like(x))\n    count = mask.sum(dim=dim, keepdim=keepdim).clamp(min=1)  # Avoid division by zero\n    return x.sum(dim=dim, keepdim=keepdim) / count\n\n# Decode TFRecord equivalent\ndef decode_torchrec(record_bytes):\n    coordinates = record_bytes['coordinates']\n    sign = record_bytes['sign']\n    out = {\n        'coordinates': coordinates.view(-1, ROWS_PER_FRAME, 3),\n        'sign': sign\n    }\n    return out\n\n# Filter NaNs\ndef filter_nans_torch(x, ref_point=POINT_LANDMARKS):\n    # Convert to tensor if input is a NumPy array\n    if isinstance(x, np.ndarray):\n        x = torch.tensor(x, dtype=torch.float32)\n        \n    # Check for NaNs across rows (axis=1) and keep rows that are not all NaN\n    mask = ~torch.all(torch.isnan(x), dim=1)\n    return x[mask]\n\n# Create a label-to-index mapping (only once)\nsign_to_index = {sign: idx for idx, sign in enumerate(df['sign'].unique())}\n\ndef preprocess(x, sign, augment=False, max_len=MAX_LEN):\n    coord = torch.as_tensor(x, dtype=torch.float32)\n\n    # Replace NaNs with PAD value\n    coord = filter_nans_torch(coord)\n\n    # Ensure proper shape: (T, 543, 3)\n    total_points = coord.size(0)\n    expected_size = ROWS_PER_FRAME * 3\n\n    if total_points % expected_size != 0:\n        padding_needed = expected_size - (total_points % expected_size)\n        if padding_needed < expected_size:\n            pad = torch.full((padding_needed, coord.size(1)), PAD, dtype=torch.float32)\n            coord = torch.cat((coord, pad), dim=0)\n        else:\n            coord = coord[:total_points - (total_points % expected_size)]\n\n    coord = coord.view(-1, ROWS_PER_FRAME, 3)\n\n    # print(\"Before flip\", coord.shape)\n\n    # # Apply flip here\n    # coord = coord[:, POINT_LANDMARKS, :]\n\n    # print(\"After flip\", coord.shape)\n\n\n    # Apply augmentation\n    if augment:\n        coord = augment_fn(coord, max_len=max_len)\n\n    # Apply padding of zeros\n    if coord.shape[0] < max_len:\n        pad_len = max_len - coord.shape[0]\n        pad = torch.zeros((pad_len, coord.shape[1], coord.shape[2]))\n        coord = torch.cat([coord, pad], dim=0)\n    else:\n        coord = coord[:max_len]\n\n    # print(\"After padding\", coord.shape)\n\n\n    # Run through preprocessing module\n    preprocessor = Preprocess(max_len=max_len, point_landmarks=POINT_LANDMARKS)\n    processed = preprocessor(coord.unsqueeze(0)).squeeze(0)  # Output: (max_len, CHANNELS)\n\n    # Convert sign to one-hot label\n    sign_index = sign_to_index[sign]\n    one_hot_label = F.one_hot(torch.tensor(sign_index), NUM_CLASSES).float()\n    \n    return processed.float(), one_hot_label\n\n\n# def preprocess(x, sign, augment=False, max_len=MAX_LEN):\n#     coord = torch.as_tensor(x, dtype=torch.float32)\n#     coord = filter_nans_torch(x)\n\n#     # Check if the size is divisible by the expected number of columns (543 * 3)\n#     total_points = coord.size(0)\n#     expected_size = ROWS_PER_FRAME * 3\n\n#     # Padding or truncation if necessary\n#     if total_points % expected_size != 0:\n#         padding_needed = expected_size - (total_points % expected_size)\n#         if padding_needed < expected_size:\n#             pad = torch.zeros((padding_needed, coord.size(1)), dtype=torch.float32)\n    #         coord = torch.cat((coord, pad), dim=0)\n    #     else:\n    #         coord = coord[:total_points - (total_points % expected_size)]\n\n    # # Reshape the coordinates to (-1, ROWS_PER_FRAME, 3)\n    # coord = coord.view(-1, ROWS_PER_FRAME, 3)\n\n    # # Augmentation if required\n    # if augment:\n    #     coord = augment_fn(coord, max_len=max_len)\n\n    # # Convert sign to an integer label using the mapping\n    # sign_index = sign_to_index[sign]\n\n    # return coord.float(), F.one_hot(torch.tensor(sign_index), NUM_CLASSES)\n\n# def preprocess(x, augment=False, max_len=MAX_LEN):\n#     coord = x['coordinates']\n    \n#     # Filter out NaNs (assuming the function is implemented in PyTorch)\n    # # coord = filter_nans(coord)\n    \n    # # # Apply augmentation if specified\n    # # if augment:\n    # #     coord = augment_fn(coord, max_len=max_len)\n    \n    # # # Ensure the shape (this can be done with padding or truncation if necessary)\n    # # coord = coord.view(-1, ROWS_PER_FRAME, 3)  # Reshape the tensor\n    \n    # # # Apply the Preprocess transformation (assuming it's implemented for PyTorch)\n    # # coord = Preprocess(max_len=max_len)(coord)[0]  # Get the first element if it's a tuple\n    \n    # # # Convert to float32\n    # # coord = coord.float()\n    \n    # # # Convert the sign to a one-hot encoded tensor\n    # # sign = torch.tensor(x['sign'], dtype=torch.long)\n    # # target = torch.nn.functional.one_hot(sign, num_classes=NUM_CLASSES).float()\n    \n    # # return coord, target\n\n# Flip left-right\ndef flip_lr(x):\n    x[..., 0] = 1 - x[..., 0]\n    new_x = x.clone()\n\n    # Swap landmarks based on constants\n    for l, r in zip([LHAND, LLIP, LPOSE, LEYE, LNOSE], [RHAND, RLIP, RPOSE, REYE, RNOSE]):\n        new_x[l], new_x[r] = x[r], x[l]\n\n    return new_x\n\n# Temporal resampling\ndef resample(x, rate=(0.8, 1.2)):\n    rate = torch.FloatTensor(1).uniform_(*rate).item()\n    length = x.shape[0]\n    new_size = int(rate * length)\n    return F.interpolate(x.unsqueeze(0), size=(new_size, x.shape[1]), mode='bilinear', align_corners=False).squeeze(0)\n\n# Spatial random affine transformation, applies randomized scaling, shearing, rotation, and shifting to 2D coordinates\ndef spatial_random_affine(xyz, scale=(0.8, 1.2), shear=(-0.15, 0.15), shift=(-0.1, 0.1), degree=(-30, 30)):\n    center = torch.tensor([0.5, 0.5], dtype=torch.float32)\n\n    if scale:\n        scale_factor = torch.FloatTensor(1).uniform_(*scale).item()\n        xyz *= scale_factor\n\n    if shear:\n        shear_x = shear_y = torch.FloatTensor(1).uniform_(*shear).item()\n        if torch.rand(1).item() < 0.5:\n            shear_x = 0.0\n        else:\n            shear_y = 0.0\n        shear_mat = torch.tensor([[1., shear_x], [shear_y, 1.]], dtype=torch.float32)\n        xy = torch.matmul(xyz[..., :2], shear_mat)\n        xyz[..., :2] = xy\n\n    if degree:\n        deg = torch.FloatTensor(1).uniform_(*degree).item()\n        rad = torch.tensor(deg / 180 * np.pi)\n        c, s = torch.cos(rad), torch.sin(rad)\n        rotate_mat = torch.tensor([[c, s], [-s, c]], dtype=torch.float32)\n        xy = xyz[..., :2] - center\n        xy = torch.matmul(xy, rotate_mat) + center\n        xyz[..., :2] = xy\n\n    if shift:\n        shift_val = torch.FloatTensor(1).uniform_(*shift).item()\n        xyz += shift_val\n\n    return xyz\n\n# Temporal crop, cuts out random parts of sequence\ndef temporal_crop(x, length=MAX_LEN):\n    l = x.shape[0]\n    offset = torch.randint(0, max(1, l - length + 1), (1,)).item()\n    return x[offset:offset + length]\n\n# Temporal mask, masks a continuous chunk of frames over *time*.\ndef temporal_mask(x, size=(0.2, 0.4), mask_value=0.0):\n    l = x.shape[0]\n    mask_size = int(torch.FloatTensor(1).uniform_(*size).item() * l)\n    mask_offset = torch.randint(0, max(1, l - mask_size + 1), (1,)).item()\n    x[mask_offset:mask_offset + mask_size] = mask_value\n    return x\n\n# Spatial mask, masks a rectangular spatial *region* within the (x, y) coordinates of the input.\n# Like hiding a hand or body part\ndef spatial_mask(x, size=(0.2, 0.4), mask_value=0.0):\n    mask_size = torch.FloatTensor(1).uniform_(*size).item()\n    mask_offset_y = torch.rand(1).item()\n    mask_offset_x = torch.rand(1).item()\n    y_mask = (x[..., 1] > mask_offset_y) & (x[..., 1] < mask_offset_y + mask_size)\n    x_mask = (x[..., 0] > mask_offset_x) & (x[..., 0] < mask_offset_x + mask_size)\n    mask = y_mask & x_mask\n    x[mask] = mask_value\n    return x\n\ndef augment_fn(x, always=False, max_len=None):\n    # if random.random() < 0.5 or always:\n    #     x = flip_lr(x)\n    #     print(\"flip applied\")\n    # if random.random() < 0.8 or always:\n    #     x = resample(x, (0.5, 1.5))\n    #     print(\"resample applied\", x.shape)\n    # if max_len is not None:\n    #     x = temporal_crop(x, max_len)\n    #     # print(\"temporal crop applied\", x.shape)\n    # if random.random() < 0.75 or always:\n    #     x = spatial_random_affine(x)\n    #     # print(\"spatial random affine applied\", x.shape)\n    # if random.random() < 0.5 or always:\n    #     x = temporal_mask(x)\n    #     # print(\"temporal mask applied\", x.shape)\n    # if random.random() < 0.5 or always:\n    #     x = spatial_mask(x)\n    #     # print(\"spatial mask applied\", x.shape)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:55.697003Z","iopub.execute_input":"2025-04-12T13:30:55.697266Z","iopub.status.idle":"2025-04-12T13:30:55.724441Z","shell.execute_reply.started":"2025-04-12T13:30:55.697240Z","shell.execute_reply":"2025-04-12T13:30:55.723512Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pyarrow.parquet as pq\n\nclass SignLanguageDataset(Dataset):\n    def __init__(self, df, max_len, augment=None, transform=None):\n        self.data = df\n        self.max_len = max_len\n        self.augment = augment\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Get the sample from the DataFrame\n        sample = self.data.iloc[idx]\n\n        # Load the Parquet file specified in the 'path' column\n        path = f\"{BASE_DIR}/{sample['path']}\"\n        landmark_df = pd.read_parquet(path)\n\n        # Extract coordinates (x, y, z) from the DataFrame\n        coordinates = landmark_df[['x', 'y', 'z']].values\n\n        sign = sample['sign']\n\n        # Preprocess the data\n        if self.transform:\n            # coordinates = self.transform(coordinates, sign)\n            coordinates, sign = self.transform(coordinates, sign, augment=self.augment)\n        else:\n            coordinates = landmark_df[['x', 'y', 'z']].fillna(0).values\n            coordinates = np.asarray(coordinates, dtype=np.float32)\n    \n            # Pad/truncate to max_len\n            if coordinates.shape[0] > self.max_len:\n                coordinates = coordinates[:self.max_len]\n            elif coordinates.shape[0] < self.max_len:\n                pad_len = self.max_len - coordinates.shape[0]\n                padding = np.zeros((pad_len, 3), dtype=np.float32)\n                coordinates = np.vstack((coordinates, padding))\n\n        # Convert the sign to a numeric label if necessary\n        # Assuming you have a label-to-index mapping\n        sign = sample['sign']\n        if isinstance(sign, str):\n            sign = sign_to_index[sign]\n\n        # coordinates = landmark_df[['x', 'y', 'z']].fillna(0).values\n        # coordinates = np.asarray(coordinates, dtype=np.float32)\n        \n        # # Pad/truncate to max_len\n        # if coordinates.shape[0] > self.max_len:\n        #     coordinates = coordinates[:self.max_len]\n        # elif coordinates.shape[0] < self.max_len:\n        #     pad_len = self.max_len - coordinates.shape[0]\n        #     padding = np.zeros((pad_len, 3), dtype=np.float32)\n        #     coordinates = np.vstack((coordinates, padding))\n        inputs = torch.tensor(coordinates, dtype=torch.float32)\n\n        # Transposed to fit dimensions\n        # inputs = inputs.transpose(0, 1)\n        target = sign\n        return inputs, target\n\n# Example usage!!\ndataset = SignLanguageDataset(df=df, max_len=64, transform=preprocess)\ndata_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n\nfor inputs, targets in data_loader:\n    print(inputs.shape, targets.shape)\n    print(inputs)\n    print(\"AAAA\")\n    print(targets[0])\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:46:07.965755Z","iopub.execute_input":"2025-04-12T13:46:07.966094Z","iopub.status.idle":"2025-04-12T13:46:09.941853Z","shell.execute_reply.started":"2025-04-12T13:46:07.966069Z","shell.execute_reply":"2025-04-12T13:46:09.940986Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 384, 708]) torch.Size([64])\ntensor([[[ 1.1565e-01,  1.1631e-01,  1.1117e-01,  ...,  8.6581e-03,\n          -2.2037e-03,  5.6347e-03],\n         [ 1.0233e-01,  1.0310e-01,  1.1458e-01,  ..., -1.0176e-02,\n           1.1487e-04,  5.0089e-03],\n         [ 1.1260e-01,  1.1762e-01,  1.0472e-01,  ..., -2.1231e-03,\n          -4.3823e-02,  5.3429e-02],\n         ...,\n         [ 5.5786e-02,  5.5004e-02,  5.5786e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 5.5786e-02,  5.5004e-02,  5.5786e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 5.5786e-02,  5.5004e-02,  5.5786e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 1.2624e-01,  1.3236e-01,  1.1517e-01,  ...,  1.7840e-02,\n          -1.4304e-01,  1.0164e-01],\n         [ 1.1969e-01,  1.3369e-01,  1.0551e-01,  ...,  4.7092e-02,\n          -3.5898e-03,  3.3042e-03],\n         [ 1.2545e-01,  1.3749e-01,  1.1578e-01,  ..., -2.2414e-02,\n           1.2300e-01, -1.0016e-01],\n         ...,\n         [-1.8800e-03, -2.0087e-03, -1.8800e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-1.8800e-03, -2.0087e-03, -1.8800e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-1.8800e-03, -2.0087e-03, -1.8800e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 1.1118e-01,  1.0387e-01,  1.0375e-01,  ...,  1.1367e-02,\n          -2.6874e-03,  6.7927e-03],\n         [ 9.0846e-02,  8.7142e-02,  1.0921e-01,  ..., -1.4350e-02,\n          -7.7809e-04,  6.7377e-03],\n         [ 1.0611e-01,  1.0596e-01,  9.4806e-02,  ...,  4.0386e-02,\n           1.9397e-02,  4.0035e-02],\n         ...,\n         [ 5.8591e-02,  5.9151e-02,  5.8591e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 5.8591e-02,  5.9151e-02,  5.8591e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 5.8591e-02,  5.9151e-02,  5.8591e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        ...,\n\n        [[ 1.0861e-01,  1.1372e-01,  1.0274e-01,  ...,  9.2227e-03,\n          -1.3986e-03,  4.9843e-03],\n         [ 9.0319e-02,  1.0181e-01,  1.0754e-01,  ..., -1.1061e-02,\n           7.6611e-04,  5.3798e-03],\n         [ 1.0461e-01,  1.1590e-01,  9.3230e-02,  ...,  1.0566e-02,\n           1.8903e-02, -1.5174e-02],\n         ...,\n         [ 5.9106e-02,  5.9015e-02,  5.9106e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 5.9106e-02,  5.9015e-02,  5.9106e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 5.9106e-02,  5.9015e-02,  5.9106e-02,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 9.6487e-02,  8.0683e-02,  8.1484e-02,  ...,  1.3195e-02,\n          -1.0084e-02,  1.6318e-02],\n         [ 6.1535e-02,  4.6147e-02,  9.3784e-02,  ...,  3.4676e-02,\n          -4.3993e-03,  1.7580e-02],\n         [ 6.9824e-02,  4.2923e-02,  6.6315e-02,  ..., -2.4463e-02,\n          -1.9344e-02, -1.2376e-02],\n         ...,\n         [-8.3762e-03, -8.2434e-03, -8.3762e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-8.3762e-03, -8.2434e-03, -8.3762e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-8.3762e-03, -8.2434e-03, -8.3762e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 1.3510e-01,  1.1517e-01,  1.2122e-01,  ...,  8.8379e-03,\n          -1.5405e-01,  1.5614e-01],\n         [ 1.2518e-01,  1.1647e-01,  1.0620e-01,  ..., -3.1072e-02,\n          -2.3598e-02, -1.1462e-02],\n         [ 1.3004e-01,  1.2202e-01,  1.1720e-01,  ...,  1.6314e-03,\n           1.2490e-01, -1.6678e-01],\n         ...,\n         [-1.8440e-03, -1.6644e-03, -1.8440e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-1.8440e-03, -1.6644e-03, -1.8440e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-1.8440e-03, -1.6644e-03, -1.8440e-03,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]]])\nAAAA\ntensor(235)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def get_pytorch_dataset(df, batch_size=64, max_len=384, augment=None, transform=None, gcs=False):\n    \"\"\"\n    Creates a PyTorch DataLoader from a DataFrame.\n    \n    Args:\n        df (pd.DataFrame): DataFrame containing file paths and labels.\n        batch_size (int): Batch size for loading data.\n        transform (callable, optional): Optional transformation to apply on the data.\n        gcs (bool): Whether to read files from GCS (True) or local file system (False).\n    \n    Returns:\n        DataLoader: A DataLoader that can be used for training/testing.\n    \"\"\"\n    dataset = SignLanguageDataset(df=df, max_len=max_len, augment=augment, transform=transform)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    return dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:58.289907Z","iopub.execute_input":"2025-04-12T13:30:58.290258Z","iopub.status.idle":"2025-04-12T13:30:58.294611Z","shell.execute_reply.started":"2025-04-12T13:30:58.290225Z","shell.execute_reply":"2025-04-12T13:30:58.293899Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Transformer model","metadata":{}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, dim=256, num_heads=4, dropout=0):\n        super().__init__()\n        self.dim = dim\n        self.scale = dim ** -0.5\n        self.num_heads = num_heads\n        self.qkv = nn.Linear(dim, 3 * dim, bias=False)\n        self.drop1 = nn.Dropout(dropout)\n        self.proj = nn.Linear(dim, dim, bias=False)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.shape\n        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.dim // self.num_heads)\n        q, k, v = qkv.unbind(dim=2)\n\n        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n        if mask is not None:\n            attn = attn.masked_fill(mask[:, None, None, :].bool(), float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        attn = self.drop1(attn)\n\n        x = torch.matmul(attn, v).transpose(1, 2).reshape(batch_size, seq_len, self.dim)\n        x = self.proj(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=nn.SiLU()):\n        super().__init__()\n        self.attn_norm = nn.BatchNorm1d(dim)\n        self.attn = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)\n        self.attn_drop = nn.Dropout(drop_rate)\n        self.ffn_norm = nn.BatchNorm1d(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * expand, bias=False),\n            activation,\n            nn.Linear(dim * expand, dim, bias=False),\n            nn.Dropout(drop_rate)\n        )\n\n    def forward(self, x):\n        attn_out = x + self.attn_drop(self.attn(self.attn_norm(x.transpose(1, 2)).transpose(1, 2)))\n        x = attn_out + self.ffn(self.ffn_norm(attn_out.transpose(1, 2)).transpose(1, 2))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:58.295357Z","iopub.execute_input":"2025-04-12T13:30:58.295632Z","iopub.status.idle":"2025-04-12T13:30:58.316933Z","shell.execute_reply.started":"2025-04-12T13:30:58.295604Z","shell.execute_reply":"2025-04-12T13:30:58.316261Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class GetModel(nn.Module):\n    def __init__(self, max_len=64, dropout_step=0, dim=192, channels=CHANNELS, num_classes=NUM_CLASSES):\n        super(GetModel, self).__init__()\n        self.max_len = max_len\n        self.channels = channels\n        self.dim = dim\n        self.num_classes = num_classes\n        self.ksize = 17\n\n        # Stem\n        self.stem_conv = nn.Linear(channels, dim, bias=False)\n        self.stem_bn = nn.BatchNorm1d(max_len, momentum=0.95)\n\n        # Blocks\n        self.blocks = nn.Sequential(\n            Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n            Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n            Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n            TransformerBlock(dim, expand=2),\n            Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n            Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n            Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n            TransformerBlock(dim, expand=2)\n        )\n\n        # Additional blocks for the 4x model\n        if dim == 384:\n            self.extra_blocks = nn.Sequential(\n                Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n                Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n                Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n                TransformerBlock(dim, expand=2),\n                Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n                Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n                Conv1DBlock(dim, self.ksize, drop_rate=0.2),\n                TransformerBlock(dim, expand=2)\n            )\n        else:\n            self.extra_blocks = None\n\n        # Top layers\n        self.top_conv = nn.Linear(dim, dim * 2)\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.dropout = nn.Dropout(0.8)\n        self.classifier = nn.Linear(dim * 2, num_classes)\n\n    def forward(self, x):\n        x = self.stem_conv(x)\n        x = self.stem_bn(x)\n        x = self.blocks(x)\n\n        if self.extra_blocks:\n            x = self.extra_blocks(x)\n\n        x = self.top_conv(x)\n        x = self.global_avg_pool(x).squeeze(-1)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n# Instantiate model\n# model = GetModel()\n\n# Example forward pass\n# temp_train should be a list of inputs, where temp_train[0] is input data and temp_train[1] is the labels\n# y = model(torch.tensor(temp_train[0]))\n# loss = nn.CrossEntropyLoss()(y, torch.tensor(temp_train[1]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:58.317741Z","iopub.execute_input":"2025-04-12T13:30:58.318058Z","iopub.status.idle":"2025-04-12T13:30:58.334666Z","shell.execute_reply.started":"2025-04-12T13:30:58.318031Z","shell.execute_reply":"2025-04-12T13:30:58.333884Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Efficient Channel Attention (ECA) Layer\nclass ECA(nn.Module):\n    def __init__(self, kernel_size=5):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, \n                             padding=(kernel_size - 1) // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        # x is [B, C, L]\n        # Take channel attention\n        y = self.avg_pool(x)  # [B, C, 1]\n        \n        # Transpose to use conv1d across channels\n        y = y.transpose(1, 2)  # [B, 1, C]\n        y = self.conv(y)\n        y = self.sigmoid(y)\n        y = y.transpose(1, 2)  # [B, C, 1]\n        \n        # Multiply with input\n        return x * y\n\n# LateDropout Layer\nclass LateDropout(nn.Module):\n    def __init__(self, rate, start_step=0):\n        super().__init__()\n        self.rate = rate\n        self.start_step = start_step\n        self.train_counter = 0\n        self.dropout = nn.Dropout(rate)\n\n    def forward(self, x, training=False):\n        if self.train_counter < self.start_step:\n            output = x\n        else:\n            output = self.dropout(x) if training else x\n        if training:\n            self.train_counter += 1\n        return output\n\nclass CausalDWConv1D(nn.Module):\n    def __init__(self, channels, kernel_size=17, dilation_rate=1, use_bias=False):\n        super().__init__()\n        self.causal_pad = nn.ConstantPad1d((dilation_rate * (kernel_size - 1), 0), 0)\n        self.dw_conv = nn.Conv1d(\n            in_channels=channels,\n            out_channels=channels,\n            kernel_size=kernel_size,\n            stride=1,\n            dilation=dilation_rate,\n            padding=0,\n            groups=channels,  # This makes it truly depthwise\n            bias=use_bias\n        )\n        \n    def forward(self, x):\n        # x should be [B, C, L]\n        x = self.causal_pad(x)\n        x = self.dw_conv(x)\n        return x\n\nclass Conv1DBlock(nn.Module):\n    def __init__(self, channel_size, kernel_size, dilation_rate=1, drop_rate=0.0, expand_ratio=2, activation=F.silu):\n        super().__init__()\n        self.channel_size = channel_size\n        self.expanded_size = channel_size * expand_ratio\n        \n        # Layers\n        self.norm1 = nn.LayerNorm(channel_size)\n        self.expand_conv = nn.Linear(channel_size, self.expanded_size)\n        self.norm2 = nn.LayerNorm(self.expanded_size)\n        self.dw_conv = CausalDWConv1D(self.expanded_size, kernel_size, dilation_rate)\n        self.eca = ECA(kernel_size=5)\n        self.project_conv = nn.Linear(self.expanded_size, channel_size)\n        self.dropout = nn.Dropout(drop_rate)\n        self.activation = activation\n        \n    def forward(self, x):\n        # x is [B, L, C]\n        skip = x\n        \n        # Pre-normalization\n        x = self.norm1(x)\n        \n        # Expansion\n        x = self.expand_conv(x)  # [B, L, C*expand]\n        x = self.activation(x)\n        \n        # Depthwise conv requires channels first\n        x = x.transpose(1, 2)  # [B, C*expand, L]\n        x = self.dw_conv(x)\n        \n        # Apply ECA\n        x = self.eca(x)\n        \n        # Back to [B, L, C*expand] for projection\n        x = x.transpose(1, 2)\n        \n        # Normalization before projection\n        x = self.norm2(x)\n        \n        # Project back to original dimensionality\n        x = self.project_conv(x)  # [B, L, C]\n        x = self.dropout(x)\n        \n        # Residual connection if shapes match\n        if x.shape == skip.shape:\n            x = x + skip\n            \n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:58.335487Z","iopub.execute_input":"2025-04-12T13:30:58.335695Z","iopub.status.idle":"2025-04-12T13:30:58.353595Z","shell.execute_reply.started":"2025-04-12T13:30:58.335678Z","shell.execute_reply":"2025-04-12T13:30:58.352942Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.amp import GradScaler, autocast\nfrom sklearn.metrics import accuracy_score\n\n# Training function\ndef train_fold(CFG, fold, train_files, valid_files=None, strategy=None, summary=True):\n    seed_everything(CFG.seed)\n    gc.collect()\n\n    # Set mixed precision\n    if CFG.fp16:\n        scaler = GradScaler()  # Updated to use GradScaler directly \n    else:\n        scaler = None\n        \n    df = CFG.df\n    if fold != 'all':\n        train_df = df[df['path'].isin(train_files)].reset_index(drop=True)\n        valid_df = df[df['path'].isin(valid_files)].reset_index(drop=True)\n        train_ds = get_pytorch_dataset(train_df, CFG.batch_size, CFG.max_len, augment=True, transform=preprocess)\n        valid_ds = get_pytorch_dataset(valid_df, CFG.batch_size, CFG.max_len, augment=False, transform=preprocess)\n    else:\n        train_df = df[df['path'].isin(train_files)].reset_index(drop=True)\n        train_ds = get_pytorch_dataset(train_df, CFG.batch_size, CFG.max_len, transform=preprocess)\n        valid_ds = None\n        valid_files = []\n\n    num_train = len(train_df)  # Use the actual number of samples, not dataset length\n    num_valid = len(valid_df) if valid_ds is not None else 0\n    \n    # Calculate actual steps per epoch correctly\n    steps_per_epoch = (num_train + CFG.batch_size - 1) // CFG.batch_size  # Ceiling division\n    print(f\"Number of training samples: {num_train}, batches: {steps_per_epoch}\")\n    print(f\"Number of validation samples: {num_valid}\")\n    \n    dropout_step = CFG.dropout_start_epoch * steps_per_epoch\n\n    model = GetModel(dim=CFG.dim, max_len=CFG.max_len, dropout_step=CFG.dropout_start_epoch)\n    model = model.to(CFG.device)\n\n    # Optimizer and scheduler\n    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n    \n    # Calculate total steps correctly for OneCycleLR\n    total_steps = steps_per_epoch * CFG.epoch\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer, \n        max_lr=CFG.lr, \n        total_steps=total_steps,\n        pct_start=CFG.warmup\n    )\n\n    # Loss function\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    if summary:\n        print(f\"Training fold {fold}...\")\n        print(model)\n        print(f\"Total training steps: {total_steps}\")\n\n    # Checkpoint for saving best model\n    best_val_loss = float('inf')\n    best_val_acc = 0.0\n    \n    # Training loop\n    for epoch in range(CFG.resume, CFG.epoch):\n        start_time = time.time()\n        print(\"On epoch number:\", epoch + 1, \"/\", CFG.epoch)\n        model.train()\n        running_loss = 0.0\n        all_preds = []\n        all_targets = []\n        \n        for step, (inputs, targets) in enumerate(train_ds):\n            print(f\"Training batch {step} at epoch {epoch+1} input shape is: {inputs.shape}\")\n            inputs, targets = inputs.to(CFG.device), targets.to(CFG.device)\n            optimizer.zero_grad()\n            \n            with autocast(device_type='cuda', enabled=CFG.fp16):  # Updated to use autocast directly\n                outputs = model(inputs)\n                print(\"Train predictions:\", outputs)\n                print(\"Train targets:\", targets)\n                loss = criterion(outputs, targets)\n            \n            if scaler:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n\n            running_loss += loss.item()\n            \n            # Calculate accuracy \n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n            \n            # Scheduler step\n            scheduler.step()\n\n        # Calculate training metrics\n        time_end = time.time()\n        print(f\"Epoch {epoch+1} took {time_end - start_time} seconds to train\")\n        epoch_loss = running_loss / len(train_ds)\n        epoch_acc = accuracy_score(all_targets, all_preds)\n        print(f\"Epoch {epoch+1}/{CFG.epoch}, Training loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n\n        # Validation step\n        if valid_ds is not None:\n            model.eval()\n            val_loss = 0.0\n            val_preds = []\n            val_targets = []\n            \n            with torch.no_grad():\n                for inputs, targets in valid_ds:\n                    print(f\"Validation at epoch {epoch+1} input shape is: {inputs.shape}\")\n                    inputs, targets = inputs.to(CFG.device), targets.to(CFG.device)\n                    outputs = model(inputs)\n                    print(\"Validation predictions:\", outputs)\n                    print(\"Validation targets:\", targets)\n                    loss = criterion(outputs, targets)\n                    val_loss += loss.item()\n                    \n                    # Calculate validation accuracy\n                    _, preds = torch.max(outputs, 1)\n                    val_preds.extend(preds.cpu().numpy())\n                    val_targets.extend(targets.cpu().numpy())\n                    \n            val_loss /= len(valid_ds)\n            val_acc = accuracy_score(val_targets, val_preds)\n            print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n            \n            # Save best model based on validation loss\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(model.state_dict(), f\"{CFG.output_dir}/{CFG.comment}-fold{fold}-best-loss.pth\")\n                \n            # Save best model based on validation accuracy\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                torch.save(model.state_dict(), f\"{CFG.output_dir}/{CFG.comment}-fold{fold}-best-acc.pth\")\n        \n        # Save model checkpoint periodically\n        if (epoch + 1) % 10 == 0:\n            torch.save(model.state_dict(), f\"{CFG.output_dir}/{CFG.comment}-fold{fold}-epoch{epoch+1}.pth\")\n    \n    # Load best model for evaluation\n    model.load_state_dict(torch.load(f\"{CFG.output_dir}/{CFG.comment}-fold{fold}-best-loss.pth\"))\n    \n    # Final evaluation\n    if valid_ds is not None:\n        model.eval()\n        val_loss = 0.0\n        val_preds = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for inputs, targets in valid_ds:\n                inputs, targets = inputs.to(CFG.device), targets.to(CFG.device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss += loss.item()\n                \n                # Calculate final accuracy\n                _, preds = torch.max(outputs, 1)\n                val_preds.extend(preds.cpu().numpy())\n                val_targets.extend(targets.cpu().numpy())\n                \n        val_loss /= len(valid_ds)\n        val_acc = accuracy_score(val_targets, val_preds)\n        print(f\"Final Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n\n    if valid_ds is not None:\n        return model, val_loss, val_acc\n    else:\n        return model, None, None\n\n# Training loop for multiple folds\ndef train_folds(CFG, folds, strategy=None, summary=True):\n    results = []\n    for fold in folds:\n        if fold != 'all':\n            all_files = CFG.df[\"path\"].tolist()  # Fixed to use CFG.df\n            train_files, valid_files = train_test_split(all_files, test_size=0.2, random_state=CFG.seed, shuffle=True)\n        else:\n            train_files = CFG.df[\"path\"]  # Fixed to use CFG.df\n            valid_files = None\n\n        model, val_loss, val_acc = train_fold(CFG, fold, train_files, valid_files, strategy=strategy, summary=summary)\n        results.append((fold, val_loss, val_acc))\n    \n    # Print summary of results\n    print(\"\\nTraining Results Summary:\")\n    for fold, loss, acc in results:\n        if acc is not None:\n            print(f\"Fold {fold}: Loss = {loss:.4f}, Accuracy = {acc:.4f}\")\n        else:\n            print(f\"Fold {fold}: Loss = {loss:.4f}\")\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:30:58.354417Z","iopub.execute_input":"2025-04-12T13:30:58.354643Z","iopub.status.idle":"2025-04-12T13:30:58.374428Z","shell.execute_reply.started":"2025-04-12T13:30:58.354619Z","shell.execute_reply":"2025-04-12T13:30:58.373712Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class CFG:\n    n_splits = 5\n    df = pd.read_csv(f\"{BASE_DIR}/train.csv\")\n    save_output = True\n    output_dir = '.'\n    \n    seed = 42\n    verbose = 2 #0) silent 1) progress bar 2) one line per epoch\n    \n    max_len = 384\n    replicas = 8\n    lr = 5e-4 * replicas\n    weight_decay = 0.1\n    lr_min = 1e-6\n    epoch = 10 #400\n    # warmup needs to be between 0 and 1\n    warmup = 0.1\n    batch_size = 64 * replicas\n    snapshot_epochs = []\n    swa_epochs = [] #list(range(epoch//2,epoch+1))\n    \n    fp16 = True\n    fgm = False\n    awp = True\n    awp_lambda = 0.2\n    awp_start_epoch = 15\n    dropout_start_epoch = 15\n    resume = 0\n    decay_type = 'cosine'\n    dim = 192\n    comment = f'islr-fp16-192-8-seed{seed}'\n    device = device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:46:39.429614Z","iopub.execute_input":"2025-04-12T13:46:39.429938Z","iopub.status.idle":"2025-04-12T13:46:39.528805Z","shell.execute_reply.started":"2025-04-12T13:46:39.429908Z","shell.execute_reply":"2025-04-12T13:46:39.527775Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_folds(CFG, [0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:46:40.676620Z","iopub.execute_input":"2025-04-12T13:46:40.676912Z","iopub.status.idle":"2025-04-12T13:50:58.220354Z","shell.execute_reply.started":"2025-04-12T13:46:40.676886Z","shell.execute_reply":"2025-04-12T13:50:58.219544Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 2048, batches: 4\nNumber of validation samples: 512\nTraining fold 0...\nGetModel(\n  (stem_conv): Linear(in_features=708, out_features=192, bias=False)\n  (stem_bn): BatchNorm1d(384, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n  (blocks): Sequential(\n    (0): Conv1DBlock(\n      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n      (expand_conv): Linear(in_features=192, out_features=384, bias=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (dw_conv): CausalDWConv1D(\n        (causal_pad): ConstantPad1d(padding=(16, 0), value=0)\n        (dw_conv): Conv1d(384, 384, kernel_size=(17,), stride=(1,), groups=384, bias=False)\n      )\n      (eca): ECA(\n        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (sigmoid): Sigmoid()\n      )\n      (project_conv): Linear(in_features=384, out_features=192, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (1): Conv1DBlock(\n      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n      (expand_conv): Linear(in_features=192, out_features=384, bias=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (dw_conv): CausalDWConv1D(\n        (causal_pad): ConstantPad1d(padding=(16, 0), value=0)\n        (dw_conv): Conv1d(384, 384, kernel_size=(17,), stride=(1,), groups=384, bias=False)\n      )\n      (eca): ECA(\n        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (sigmoid): Sigmoid()\n      )\n      (project_conv): Linear(in_features=384, out_features=192, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (2): Conv1DBlock(\n      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n      (expand_conv): Linear(in_features=192, out_features=384, bias=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (dw_conv): CausalDWConv1D(\n        (causal_pad): ConstantPad1d(padding=(16, 0), value=0)\n        (dw_conv): Conv1d(384, 384, kernel_size=(17,), stride=(1,), groups=384, bias=False)\n      )\n      (eca): ECA(\n        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (sigmoid): Sigmoid()\n      )\n      (project_conv): Linear(in_features=384, out_features=192, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (3): TransformerBlock(\n      (attn_norm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (attn): MultiHeadSelfAttention(\n        (qkv): Linear(in_features=192, out_features=576, bias=False)\n        (drop1): Dropout(p=0.2, inplace=False)\n        (proj): Linear(in_features=192, out_features=192, bias=False)\n      )\n      (attn_drop): Dropout(p=0.2, inplace=False)\n      (ffn_norm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (ffn): Sequential(\n        (0): Linear(in_features=192, out_features=384, bias=False)\n        (1): SiLU()\n        (2): Linear(in_features=384, out_features=192, bias=False)\n        (3): Dropout(p=0.2, inplace=False)\n      )\n    )\n    (4): Conv1DBlock(\n      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n      (expand_conv): Linear(in_features=192, out_features=384, bias=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (dw_conv): CausalDWConv1D(\n        (causal_pad): ConstantPad1d(padding=(16, 0), value=0)\n        (dw_conv): Conv1d(384, 384, kernel_size=(17,), stride=(1,), groups=384, bias=False)\n      )\n      (eca): ECA(\n        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (sigmoid): Sigmoid()\n      )\n      (project_conv): Linear(in_features=384, out_features=192, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (5): Conv1DBlock(\n      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n      (expand_conv): Linear(in_features=192, out_features=384, bias=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (dw_conv): CausalDWConv1D(\n        (causal_pad): ConstantPad1d(padding=(16, 0), value=0)\n        (dw_conv): Conv1d(384, 384, kernel_size=(17,), stride=(1,), groups=384, bias=False)\n      )\n      (eca): ECA(\n        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (sigmoid): Sigmoid()\n      )\n      (project_conv): Linear(in_features=384, out_features=192, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (6): Conv1DBlock(\n      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n      (expand_conv): Linear(in_features=192, out_features=384, bias=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (dw_conv): CausalDWConv1D(\n        (causal_pad): ConstantPad1d(padding=(16, 0), value=0)\n        (dw_conv): Conv1d(384, 384, kernel_size=(17,), stride=(1,), groups=384, bias=False)\n      )\n      (eca): ECA(\n        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n        (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (sigmoid): Sigmoid()\n      )\n      (project_conv): Linear(in_features=384, out_features=192, bias=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n    (7): TransformerBlock(\n      (attn_norm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (attn): MultiHeadSelfAttention(\n        (qkv): Linear(in_features=192, out_features=576, bias=False)\n        (drop1): Dropout(p=0.2, inplace=False)\n        (proj): Linear(in_features=192, out_features=192, bias=False)\n      )\n      (attn_drop): Dropout(p=0.2, inplace=False)\n      (ffn_norm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (ffn): Sequential(\n        (0): Linear(in_features=192, out_features=384, bias=False)\n        (1): SiLU()\n        (2): Linear(in_features=384, out_features=192, bias=False)\n        (3): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (top_conv): Linear(in_features=192, out_features=384, bias=True)\n  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n  (dropout): Dropout(p=0.8, inplace=False)\n  (classifier): Linear(in_features=384, out_features=250, bias=True)\n)\nTotal training steps: 20\nOn epoch number: 1 / 5\nTraining batch 0 at epoch 1 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.0029, -0.1024, -0.1061,  ...,  0.1017,  0.0176,  0.0153],\n        [-0.0604, -0.0381, -0.0161,  ..., -0.0094, -0.0611, -0.0441],\n        [ 0.0021, -0.0423, -0.1160,  ...,  0.0013, -0.0208, -0.0854],\n        ...,\n        [ 0.0034,  0.0805,  0.0972,  ...,  0.0189,  0.0125,  0.0545],\n        [-0.0853, -0.1350, -0.1190,  ...,  0.1070,  0.0100, -0.0990],\n        [-0.0267, -0.0440, -0.0595,  ..., -0.0470,  0.0831, -0.0012]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([232, 198, 109, 136,  51,  72, 120, 174,  33, 145, 240,   8, 199,  63,\n         28,  27, 125,  99, 169, 106, 101,  14, 140, 171,  27,  85, 108,  32,\n        118,  81, 223,  15,  84,  62, 101, 154, 144,  58,  11, 194,  82, 167,\n          5,  77, 134, 233, 166,  56,  70, 112, 161,  88, 158, 218, 159, 115,\n         57,  55, 239, 186,  31,  53,  15, 126, 217, 162, 215,  45, 144,  85,\n        206, 161,  44,  21,  33, 147,  13, 181, 132, 197, 173, 239, 222,  85,\n        150, 220, 240,   1, 165,  76,  20,  17,  81, 125, 105,  96,  65,   0,\n         49, 246, 185,  33,  26,  36, 179, 235,  46,   6,  27,  19,  51,  22,\n        207,  31,  98,  63,  36, 241,  22, 231,  38, 216,  92, 194,  29, 161,\n        213, 130, 175,  42,  94,   9,  35, 192,   1, 190,  71,  91,  97, 226,\n        135, 119, 199, 197, 176, 120, 225,  45,  63, 223, 201, 198, 162, 245,\n         87, 145, 166, 138, 237, 234, 212, 141,  27,  74,  64, 175, 131, 154,\n        217,  40, 173,  10,  17, 159, 225, 245, 106,  11, 116, 176,  87, 194,\n        188,  89,   8, 198, 225,  94, 142,  99,  18, 156,  94, 199,  63, 136,\n         43,  83, 218, 140,  25, 173,  61, 241, 147,  30, 239,  73,  52,  12,\n        138,   7,  21,  57, 244, 172, 124, 119,  86,  29, 194, 171,  64, 118,\n         48, 171, 166, 157,  44,  56,   0, 206,  21,  24,  71, 226, 196, 203,\n          2, 175,  25, 171,  83, 215, 191, 114, 203,   9, 196, 208,  91, 201,\n        136,   7, 215, 208,  66, 101,  14,  98,  27, 174, 249, 242, 204, 133,\n         43,  82, 131,  41,  48,  59, 148, 108, 186, 140,  65,  63, 176, 112,\n        200, 152, 178, 206, 214, 113, 129, 117, 173, 192, 195, 181, 131,  25,\n         67, 227, 152,   1, 181, 206, 181,  67, 195, 135, 218, 193, 138,  15,\n        103, 221,   8, 148, 100,   7, 117,   9,  33, 110, 140, 126, 220, 117,\n         48,   5,  53,  39, 146,  17,  52, 167, 127, 170, 118, 105,  72,  17,\n        164,  45, 194, 156, 112, 169, 227,  53, 156, 202,  34,  47, 119,  25,\n        158, 213,  79,  95,  44, 134, 246,  88,   9, 158, 238,  24, 115,  37,\n        139, 188, 163, 135, 160,  45, 150, 117, 148, 103,  18,  45, 195,  41,\n        160, 115,   8, 162,  93, 177, 190, 199, 206, 207,   4,  28,  10, 158,\n        247, 191, 192, 176,  20, 147, 122,  87,  17, 130, 210,  35, 247, 210,\n        180, 104, 172,  29, 162, 104,  96, 246, 160,  87, 153,  53, 193, 232,\n         29, 134, 129,  30,  18,  11, 199,  10,  39,  71,  40,   3, 113, 187,\n        245, 193, 144, 109, 121, 158,  50, 161,  50, 201,  54, 194, 210,  62,\n        180,  28, 189, 144,  69,  46, 157, 236, 236,  93, 230,   0,  11,  38,\n        194, 156,  84, 146, 159, 215,  78, 208, 112,  67,  31, 180, 167, 106,\n          4,  68,  51,  62,  70,  85, 163, 146,  17, 205,  85, 100, 116, 240,\n        109, 157,  31,  99, 192, 177, 133, 224, 177, 215,  70,  96,  85,  58,\n        242,  55,  33,  37,  47, 151, 203, 213], device='cuda:0')\nTraining batch 1 at epoch 1 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.0765, -0.2137, -0.1809,  ...,  0.1851, -0.0267,  0.1783],\n        [ 0.2908, -0.1881,  0.3418,  ..., -0.0021,  0.0293, -0.0558],\n        [ 0.1401, -0.0398,  0.1960,  ...,  0.3809, -0.1764, -0.1300],\n        ...,\n        [ 0.1648,  0.1227,  0.0927,  ...,  0.2273, -0.0793, -0.1289],\n        [-0.0362,  0.1372, -0.3953,  ...,  0.1661, -0.1686,  0.0471],\n        [-0.1015, -0.3005,  0.2651,  ..., -0.2507,  0.2257, -0.2886]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([121,  83, 189, 214, 202,  67, 182,   6, 227,  23, 229, 230,  81, 189,\n        182, 237, 188, 197, 173, 181, 136,  89, 144, 121, 199, 121,  71,  31,\n         34,  45,  53, 123,  89, 203, 153,  98, 244,  80,  26, 162,  93,  72,\n          5, 143, 224,  25,  13,   1,  85, 103,   2, 101,   5, 239,  99, 165,\n        136, 110, 119,  78,   5, 131,  79, 186, 249,  21,  73, 194,  22,  69,\n         22,  85,   9,   0,  36,  53,  74,  42, 147,  47,  61, 221, 225, 247,\n          5, 209,  76, 222,   1, 232,  48,  56, 115,   5,   7, 191, 131,  37,\n         90,  74, 130, 142, 190,  56, 131, 183,  61, 193,  19,  87, 223,  37,\n         63,  23, 225, 101, 153, 187, 133, 186, 244,  28,  71,  21,  79,  78,\n        124, 116,  75,  75,  16,  29,  32,  12, 240,  51, 238,  34, 168,  64,\n        236, 108, 173, 118, 175, 112, 141, 203, 184,  27, 145, 130,  82,  95,\n         40, 239,   0, 100,   8,  65, 147, 180, 183,  92,  95, 215, 231, 204,\n         65, 211,  81,   2, 200,  41,  44, 102, 129, 206, 141, 104, 111, 177,\n        107,  50,  82, 202, 242,  81,  55, 146, 160, 218,  33, 135,  12, 188,\n         37, 180,  45, 196, 160, 175,  42,   9,  57, 163,  99, 149, 151,  12,\n         54, 218,  61, 214, 111,  91,  54,  70,  59, 112,  53, 163, 139, 213,\n         98,  42,  81, 140, 198, 166, 127, 125,  40, 161,  56,  25, 210,  65,\n         85,  43, 111, 206, 233, 126,  52, 201, 115, 137,   8,  48,  45, 109,\n         64, 103, 168,  52,   4,   3, 121, 133,  30, 209, 198,  86, 228, 143,\n         69, 146,  10,  22,  20,  85, 114, 177, 161,  98, 115, 144,  32,  39,\n         84,  50,  15, 115,  85, 192,  90, 180,  15,  95,  90, 117, 209, 193,\n         37, 135,  66,  98, 231,  47,  60,  39,  72, 236,  92, 223,   9, 218,\n        125,  47,  13, 229, 104,  90, 165,  11, 131,  83, 174, 246,  14, 113,\n         18, 191, 166, 126,  41, 186, 199, 213,  43,  62, 215, 194,  66, 116,\n        129, 140,  19, 231,   5, 131,  31, 117, 183, 185,  21, 228, 174, 198,\n        109, 114, 147, 190, 247,  31,  64,  47, 128, 156,  43,  56, 170, 149,\n         22, 118,  28, 117, 109, 152,  43,  93, 165,  21, 146,  89,  70, 105,\n         16, 165, 233, 133, 149,  99,  64, 211, 167, 218, 236,  88,  55,  84,\n         87,  57,  11,  53, 108, 148, 239, 159, 105,  71, 134, 208, 196,  10,\n         51, 147, 158,  38, 132, 143,  56,  93, 173, 237, 104, 133, 173,  21,\n         87, 145,  99, 177,   1, 246, 183,  50,  27, 110, 165, 139, 198, 120,\n        130,  17,  32,   6, 181,  72,  93, 212,  35,   8,  20,  18,  36, 142,\n        129, 229,  76, 197, 191,  12, 149, 173, 136,  77,  16, 232, 228,  74,\n         40,  75, 224, 171, 118, 214, 143, 131, 105, 162,  50, 166, 158, 234,\n        237, 159,   5, 204, 153, 124, 181, 242, 104,   2, 193, 127, 115, 239,\n         97, 169,  76, 141,  65,  43, 208, 156, 110, 119, 164, 180,  88, 132,\n        177,   4,  46, 207, 194, 238, 115, 159], device='cuda:0')\nTraining batch 2 at epoch 1 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.0382, -0.0481,  0.0950,  ..., -0.0360,  0.0444,  0.0443],\n        [ 0.2347, -0.0129,  0.1486,  ...,  0.0525,  0.3301, -0.0992],\n        [-0.0764,  0.1108, -0.0762,  ...,  0.1433, -0.1279,  0.1550],\n        ...,\n        [-0.0961, -0.0604,  0.0933,  ...,  0.0054, -0.0186,  0.0316],\n        [-0.1647, -0.2019,  0.4028,  ..., -0.1528,  0.1776,  0.3384],\n        [-0.1217,  0.0471,  0.0219,  ..., -0.3081, -0.2065, -0.0362]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([153, 206, 130,  24, 171,  19, 150, 156, 182,  93,  22, 186, 121,  93,\n        103,   1,  74,  75,   5, 124,  21,  63,  43,  70, 124, 208, 195,  77,\n         36, 105, 193, 127, 129,  72, 218, 129,   9, 172, 144, 145,  94, 212,\n        206,  96, 127, 127,  77,  57,  67, 176, 100,  70, 235,  91, 160, 235,\n         39, 159, 150, 185, 231,  15,  92, 122, 153, 184, 244, 126, 135, 104,\n         43,  98, 182,  62, 106,  93, 142,  28,  58,  65, 183,  74, 102, 235,\n        139, 162, 106, 126,  30,  78, 149,   7,  37,   1, 215,  70,  37, 245,\n         98, 119, 189, 101, 230,  38, 178,  45, 173, 155, 100, 177,  86,   7,\n         85, 162, 145, 196, 115, 243, 175,  70, 179,  98,   2,  44, 245, 191,\n         64, 132, 146,  41, 116, 206,  14,  39, 220, 116,  70,   2, 168,  38,\n        243,   4, 175,  57, 240,  58, 184,  66,   7,  73,  18, 103, 176, 169,\n        106,  98,  25,  15,  34, 202,  55, 132,  93, 221, 121, 223,  48, 143,\n        248, 248, 176, 192,  82, 152, 227,  98,  51,  60,  49, 237, 188, 148,\n         54, 245,  75,   8,  32, 110,  19,  12, 137,  89, 216,  10,  15,  43,\n        212, 198,  68,  94, 119,  19, 125,  58,  25,  45, 115,  19, 230, 102,\n        153, 232, 241, 153, 108,  70, 164,  52,  52, 177,  68, 195, 130, 221,\n        164, 143, 185,  53,   5, 171,  88, 148, 210, 117,  93, 167,  38, 101,\n        241, 224, 233,  78, 118,  96,  16,  35, 197,  44,  21, 107,  36, 146,\n         75, 219,  60,  97, 212, 235,  42,   6, 226,  69, 191, 105, 124, 156,\n        162, 155, 234, 121,  58, 184, 112, 248, 104,  76,  81, 138, 181, 154,\n         44,  16,  22,  41, 238,  21, 111, 140, 167,  43, 189,  58, 166,  42,\n        224, 201, 151, 113,  93, 217, 114,  42, 179, 179,  84, 242, 202,  24,\n        242, 207, 236,  81, 150, 142,   0, 241,  19,  52,  22,  66,  28,  47,\n        149, 164, 184, 115,  48, 112,  63, 179, 195, 216, 241,  99, 149, 206,\n        157, 151,  75,  23, 186,  72,  14, 108,  38, 216, 163,  59, 194, 188,\n        145,  38, 101,  57,   1,  37,  74, 246, 109, 230, 140, 165, 139,  51,\n         27, 225, 163, 235,  28,  33, 114,  41,  25,  89, 194,  96,  28, 224,\n        224, 229,  85,  33,  91, 202, 220, 123, 133, 242,  49, 103,  85,  40,\n        203,  87,  48, 234, 194,  11,  16,  45, 203,  39, 119,  34, 240,  49,\n        229, 190, 210, 135,  47,  25, 184,  92, 130, 169, 119, 157, 164, 148,\n        226,  99, 225, 236, 162, 204, 185, 171,  86, 199,  65,   4, 136, 125,\n        134, 232, 134, 165,  49, 172, 153, 175, 204,  38, 177,  82, 193, 186,\n        188, 130, 236,  69, 102,  22,  53, 182, 182, 186, 139,   8,   6,  90,\n         38,  48, 162,  55,  58,  31, 117, 159, 139, 205, 177, 194,  89,  92,\n         78, 139,  79, 161,  12, 118,  35, 133, 161,  47, 160, 125, 211, 152,\n         60,  23, 181,  18, 236,  65, 166,  96, 220, 149,  90, 174,  79, 162,\n        241, 216, 238,  50,  11,  45, 208,  34], device='cuda:0')\nTraining batch 3 at epoch 1 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.2043,  1.7188, -1.7012,  ...,  1.5771,  1.0322,  1.8457],\n        [ 1.8496,  1.4834, -1.2100,  ..., -0.3452, -1.3389,  0.2603],\n        [ 1.0527,  1.4092, -0.4883,  ..., -0.2129, -0.5054,  0.4480],\n        ...,\n        [ 0.2374, -0.1248,  0.4651,  ...,  0.1930,  0.1520, -0.1292],\n        [ 0.2087, -0.0371,  0.3640,  ...,  0.0635,  0.2010, -0.3687],\n        [ 0.1931, -0.4619, -0.3828,  ..., -0.1135,  0.0679,  0.2603]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([106, 113,  62, 184, 245, 229, 171, 172,  39, 213,  94, 213,  94,  66,\n        145,  58, 172, 161, 113,  57,  49, 156,   1,  83,  97,   5, 183,  87,\n        153,  18, 192,  76,  25,   9,  45, 238, 178, 242, 162,  89, 111,  14,\n         47, 246, 107, 131, 181,  28, 146, 136,  41, 235,  30, 151,  13,  53,\n        176, 214, 112, 108,  39, 232,  89,  49, 142, 152,  16,   3, 120, 188,\n         49,   6, 153,  64,  73, 233, 156, 231, 244, 243, 216, 122,  13, 234,\n        133, 238, 150, 125, 125,   3, 174, 126, 110, 218,  96, 109, 227, 155,\n        199, 160, 214,  10,   2,  22, 211,  80,  77,  39,  40,  89,  41,  12,\n         32, 113, 167,  55, 139, 163, 214,  12,  85,   7,  77,  52, 130, 138,\n        176, 230,  75,  74, 149, 198, 182, 176,  14, 182, 169, 172, 185,  95,\n        124,  60, 208,  80, 109,  87,   2, 124, 234, 131, 191,  26, 172,   4,\n        211, 180,  71, 154, 117,  32, 135,  16, 210, 213, 185, 157,  83, 108,\n         21,  39,  65, 223,  91, 232, 232, 231,   7,  69, 135,  71,  13, 113,\n        136, 111, 124, 170,  24,  14,  54,  13,  93, 246,  44, 166,  61, 165,\n         45,  94,  50, 193,   2, 210,  71, 205, 164, 245,  18,  13, 200, 108,\n        137,  39, 182,  67, 211,  88,  26, 218, 145, 131, 154, 126,  60, 237,\n         20,  84, 219,  47,  31,  93, 191, 180,  41, 186,  12, 199, 109,  58,\n        211, 104,   0, 218, 169, 103, 189, 232,  37, 213,  52,  43,  70,  22,\n         25, 212, 148, 212,  92, 145, 130,  93,  48, 212, 196,  90, 194,  32,\n        105, 234,  55,  78, 157, 158,  94, 170, 126,  54,  15, 223,  57, 153,\n          1,  94, 210, 210,  32, 181, 180, 215, 212, 200,  56, 151, 212, 221,\n         88, 210,  11, 146, 145, 160, 179, 126,  98,  51,  88, 112, 216,  63,\n         46,  67, 223, 188, 176,  78,  54,  90, 210,  91,   2, 128, 187, 191,\n        149, 197, 218,  19,  82, 107, 130,  44,  17,  52,  75,  53,  88, 128,\n        104, 199, 136,  38, 223, 219, 169, 202,  75,  76, 197, 128,  15, 120,\n        235, 158,  12, 237,  67, 235, 137,  44, 248,  36,  24, 114, 222, 197,\n         91,   5, 207, 169,  17, 204,  64,  78,  61,  41,  61, 131,  53,  37,\n        226, 123, 214, 166, 112,  21, 237, 167,  66, 119, 157, 124, 182, 172,\n         76, 141,  13, 206, 211,  33, 174, 245, 101, 143,  88,  66, 101, 171,\n         53, 221, 219,  40,  82, 153, 165, 242, 100, 203, 107, 132, 125, 135,\n        226, 172,  76, 108, 162, 104,  11,  88,  45, 233, 184, 116,  38, 108,\n         99,   4, 154, 133,  18,  82,  25,  32, 194, 239,  49,  81, 233, 135,\n         95, 183, 126, 204, 220, 128,  45, 228,  12, 136, 138,  82, 159, 117,\n        176, 134,  18, 181,  92, 238,  78, 209, 176,  61, 127,  41, 144,  65,\n        142, 131,  93,   8,  64,  87, 126, 168,  87,  40, 136, 125,  50, 230,\n         53,  50,  92,  19,  25, 126, 241,  39, 171, 232, 227, 156, 167,  77,\n        123, 230,  49, 246, 188,  34,  22, 123], device='cuda:0')\nEpoch 1 took 40.12152051925659 seconds to train\nEpoch 1/5, Training loss: 5.6099, Accuracy: 0.0024\nValidation at epoch 1 input shape is: torch.Size([512, 384, 708])\nValidation predictions: tensor([[ 0.0238, -0.0818,  0.0214,  ...,  0.3900,  0.0038, -0.0889],\n        [-0.0200,  0.1322, -0.0793,  ...,  0.1315,  0.0389, -0.0336],\n        [-0.0318,  0.1691, -0.1849,  ...,  0.0064,  0.0823,  0.0105],\n        ...,\n        [-0.0344,  0.0546, -0.0526,  ...,  0.1318,  0.0248, -0.0659],\n        [ 0.1145,  0.2370, -0.3100,  ..., -0.0482,  0.1048,  0.0624],\n        [ 0.0185, -0.0369, -0.1377,  ..., -0.3095,  0.2111,  0.0605]],\n       device='cuda:0')\nValidation targets: tensor([ 59,  82, 211, 199, 150,  35,  83, 178, 212,  90, 129,   3, 219,  66,\n         32, 116,  75,  96,  20,  73, 229, 219, 101, 201,  80,  48,  87, 206,\n        186, 108,  68,  29, 168, 144, 104,  31,  64,  80, 204,   5,   0, 183,\n        145,  61,  44, 126, 157, 231, 196,  44, 109, 154, 193, 154, 170, 101,\n        108,  44,  64,  59,  71,  74,  29,  25, 162, 223, 236,  12, 190,  13,\n         15,  30,  27,  33,  75,  80,  38, 137,  54, 224,  10,   5, 192, 141,\n         49,  75, 154,  45,   3,  47, 196, 200,  64,  31,  51,   6, 180, 172,\n          2,  58, 212,  55,  82,  78,   1, 227,  34,   8,  98,  33,  31, 155,\n        202, 235, 206,  35, 118,   8, 114, 193, 194, 191, 233,  85,  73, 198,\n        225, 169,  98, 232, 219, 177,  77, 104,  45, 118,  40, 211,  22,  94,\n        195, 178, 218, 171,  92, 176, 115,  91, 140, 136, 133, 141, 100, 190,\n        155, 139, 137,  76,  15, 229,  43, 236, 221, 140,  58, 197,  31,  17,\n         88,  48,  43,  89, 143,  51, 191, 217, 214, 202, 171, 232,  24, 145,\n         55, 245,  98,  34,  34, 154,  19,  31, 132, 238, 129, 185, 247, 104,\n        132, 136, 137, 134, 208,  83, 144, 154, 200, 135,  38, 117, 172, 113,\n         96,  96,  72, 202,  99,   2, 106,  35, 113,  60,  85, 164,  64, 160,\n        108,  90, 157,  72,  21, 206,  49,   4, 130, 102, 145,  91,  21,  63,\n         93, 129, 196, 234,  11,  88, 136,  10,  75,  11, 203,  55,  46,  84,\n        151, 103,  98,  23,  33, 225,   3,   3, 136,  88,  70, 113, 196,  28,\n        116,  30,  17,  97,  81, 211,  92, 151, 213, 156, 119,  89, 161, 182,\n         23, 137,  36,  29, 178,  78, 211, 107, 165, 205, 160,  57, 191, 116,\n         69, 207, 241, 162, 124, 116, 200,  30,  62, 153, 100, 128,   1, 133,\n        169, 159, 125,  55,  19, 170,  40, 233, 191,  66,  48, 120, 163,  75,\n        101, 249,   5,  83, 237, 210, 206, 222,   6, 122,  67, 115, 108, 108,\n        148, 215, 172,  62, 213,  66, 143,  76, 151, 172,   7,  57, 114,  45,\n         49, 150, 206, 155,  77, 238, 123,  41,  80,  48, 248, 171, 157,  29,\n         38,  63, 123, 107, 154, 220, 129,  68,  72, 120, 160,  85, 203, 124,\n        114, 178, 132,  22, 209,  40,  84,   2,  36, 175, 118,  56, 125,  74,\n        172, 114, 210, 150,  16,  58, 129, 200, 216,  58,  56, 227,  66,  54,\n        117, 131, 144,  80,   5, 140,  12, 201,  41,  92,  45, 185, 119, 136,\n        112,  57, 201, 101, 113, 164,  47, 152, 243,  31, 115, 165, 136,  99,\n        164,  86, 217, 187,  15, 147,   6, 195, 166, 208, 167,  98, 240, 114,\n         63,  69, 214, 136, 194, 220,  30, 210,  65,  56,  38, 216,  39, 114,\n        215, 208,  95,  48,  34, 184,  29, 211,  23,  17, 219,  75, 139, 176,\n        129, 175, 121,  69, 193,  94, 146,  52,  87, 210, 118, 147, 190, 216,\n        107, 217, 161,   7,  25,  14,   9, 147, 162,  75, 245, 135,  91,   4,\n         83,  32,  48, 213,  75, 188, 204, 113], device='cuda:0')\nValidation Loss: 5.5682, Accuracy: 0.0059\nOn epoch number: 2 / 5\nTraining batch 0 at epoch 2 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.0936, -0.4216, -0.0850,  ...,  0.0359, -0.0630, -0.0487],\n        [-1.2305, -0.7520, -0.0466,  ...,  0.3127, -0.3269,  1.1143],\n        [ 0.4309,  0.5796,  0.3147,  ...,  0.0659,  0.2310,  0.5205],\n        ...,\n        [-0.4160,  0.2903,  0.5532,  ..., -1.1934,  0.3843,  0.1469],\n        [ 0.1566,  0.2360, -0.2012,  ..., -0.5527, -0.6543,  0.8472],\n        [-0.5352,  0.2756,  0.1725,  ...,  0.3669, -0.0464,  0.8765]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([158,  57, 108,  91, 182, 213, 233, 107, 225, 236,  29,  53, 188, 172,\n         39,   5, 236, 213, 141, 198, 135, 167, 128, 159, 181,  57,  67,  56,\n         70, 173, 135, 165,  75, 120,  96,  25,  95,  61,  26, 200,   2,  76,\n        174,  22,  32, 126,  96, 128,  45, 163, 125, 154, 163,  51, 221, 109,\n          0, 214,  65,  34, 195, 229, 109, 149, 162,  45, 184, 207, 223, 183,\n         45, 125,  66, 194,  22,   2, 203, 208, 213,   4, 245,  93,  88,  25,\n        161, 101,  65, 123, 214,  43, 127,  22, 191,  82,  70, 141,  60,  35,\n        183,  20,  81, 241,  32, 131,   8, 161,  66, 109, 147, 221, 126,  48,\n         61,  50, 247, 235, 236, 131, 164,  31, 132, 231,  69, 189,  85,  99,\n        112,  85, 116, 186, 171,  92, 228,  52,  27, 171, 198, 120,  85,  45,\n         78, 126, 228,  96,  45, 218, 145,  94, 245, 160, 230, 170,   6, 231,\n         10, 181,  31, 176, 121, 126,  92, 149, 148, 192,  17,  61,  83,  52,\n          1, 231, 121, 226, 166, 146, 233,  33,  54,   6, 181, 154,  60, 192,\n        183, 237, 141,  73,   8,  15, 245, 133, 131,  64, 194, 232, 145, 153,\n        185, 131,  42, 178,  98,   5, 150, 210, 216,  34, 157, 175, 226, 232,\n        191, 109, 123, 234, 246, 234,  78,  24,  50,   5,  45, 121,  94, 216,\n         26,  56,  41, 212, 184,  52,  36,   4, 118, 183,  34,   8, 224, 156,\n         50,  64,  61, 174, 174,  38, 160, 102, 240, 182,  80,  66,  85, 191,\n        104,  88, 148, 104,  98, 124,  67, 143, 198,  65, 138,  77,  16, 136,\n        236,  87,  50, 104,  57,  77,  69, 224, 168,  18,  94, 153, 156,  28,\n         70, 226,  75, 117, 113, 141, 143, 115,  37, 244,  75,  85,  92,  41,\n        226, 194,  98,  32,  77, 247,  42,  33,  25,  94, 241,  10,  70, 246,\n        225, 196,  44,  44, 238, 214, 171,  96,  27,   2,  45,  82,  43, 242,\n         66, 180, 214, 130, 211,  88,  18,  19, 238,  91,  77,  11, 149, 241,\n         31,  90, 161,  69, 197,   2, 135, 182, 239,  40,  93, 186,  74,  53,\n        139, 209,  37,  11, 179,   7, 199, 133,  61, 143,   0, 239,  72,  77,\n         38,  63, 223, 177,  99,  51,  84, 140,  19, 176,  45, 117, 132, 134,\n        120, 225,  92, 237, 108,  12,  68,  38,  15,  14, 117, 210,  91,  89,\n         34,  89,  37,  29, 164, 112, 237,   9, 231, 248, 135, 117, 158, 172,\n        180, 165, 227, 177,  35, 105, 125, 204, 235,  90, 126, 233,  89,  25,\n        152, 104, 227, 130, 103,   6,  58,  19, 142,  78, 136,  92, 148,  91,\n        108,  39,  84, 109, 157, 249,  55, 202, 125,  64, 163,  98, 200, 146,\n         94, 139, 165, 100, 143, 154,  94, 164, 181, 171, 232,   5, 218,  56,\n        117, 228,  76, 173, 237, 160, 218, 238, 146,  75, 104,  76,   4,  53,\n         58, 198,  81,  72, 167, 217, 131,  75, 169,  53, 193,  17, 157, 204,\n         90, 162,  31,  82, 237,  98, 246, 171,   8, 179,  67, 238, 199,  85,\n        174, 117, 165,  18,  88,  81,  12, 203], device='cuda:0')\nTraining batch 1 at epoch 2 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.9087,  0.1821,  0.0283,  ..., -0.2810, -2.1660, -1.5830],\n        [ 0.7705,  0.4229, -0.6143,  ...,  0.6807,  0.5645,  0.4890],\n        [ 1.0811, -1.0137, -0.7998,  ...,  0.7485,  0.1232,  0.5830],\n        ...,\n        [ 0.7979, -0.2473,  0.4561,  ...,  2.1914, -0.0316, -0.0871],\n        [-0.1105,  0.9360, -0.1355,  ..., -0.0026,  0.5088, -0.1321],\n        [-0.3384, -0.1772, -0.6753,  ..., -0.0520,  0.5542, -0.1482]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([218, 204,  14,  53, 144,  17, 158, 211, 233, 245, 205, 185,  13,  92,\n         65, 208, 153, 192, 164,  37, 184,  81,   1, 173, 115, 134, 149, 199,\n         29,  89,  49, 136, 109,  66, 104,  74, 115, 140, 216, 159,  45,  38,\n        201, 161, 140,  16, 177,  33,  96, 215,  56, 110, 134, 131, 105, 208,\n         80,  66, 100, 206,  33, 114, 159, 193,  65, 103, 146, 125, 122, 180,\n        242,   9, 149, 172, 218, 212, 202,  12, 186, 156, 240, 220, 163, 197,\n        220, 102, 138,  82, 156, 119, 113,  21,  82, 145,   7,  49, 239,  30,\n        147, 106, 169,  13, 184, 181, 130,  22, 216,  58,  59, 246, 126, 162,\n         64, 205, 117, 157, 125,  44, 241, 199,  69,  43, 166, 119,  44,  93,\n         41, 240,  47, 157, 126,  70,  12, 116, 196, 248,  93,  85, 223, 119,\n        100, 115, 159, 233, 146,  47,  21, 232,  80, 134, 142, 136, 229, 172,\n         11, 130, 153, 189,  88, 194,  63, 119, 249, 158,  50,   4,  11,  40,\n         14,  89,  16,  12, 172, 201, 212,  10, 104, 119, 147,  16, 224,  97,\n        115, 231, 224,  70,  76, 199, 142, 175, 232,  31, 113, 211,   6, 108,\n         73, 195, 235,   1, 188,  93,  25, 232,  71, 192,   4,  18, 169,  12,\n        168,  65, 153, 229, 199, 220, 137, 139,  99,  51,  43,  79, 188, 177,\n         85,  78, 224, 208, 161,  38,  22, 117, 237, 106, 135,  94, 144,  39,\n        186, 144, 110,  86,  24,  20,   7, 171, 228, 188,  29, 236,  57, 162,\n        201, 129,  53,  91,  40,  74, 127, 180, 166,   2, 242, 169,   1, 142,\n        113, 146, 124,  36,  89, 220, 124,  74, 198, 215, 193,   6, 203, 225,\n        108, 246, 131, 165, 156,   7, 176, 155, 190, 105,  38, 245, 223, 135,\n         12, 181,  82, 218, 150, 155, 220,  39, 160,  48, 115, 163, 124, 146,\n         12, 238, 187,  86,  19,  25,   1, 118,  57, 206,  98,  11, 153,  98,\n         17,  19,   9,  55, 183,  47,   7,  58, 206,  91, 149, 101, 212, 105,\n         71, 182,  78, 173, 102, 178,  36,  27, 201, 142,  48, 203,  50,  21,\n        155, 101,  84, 192,  66, 239,  98, 194, 201, 243, 137, 140,  39,   5,\n        151, 235,  21, 210,  34,  39, 131,  47,  76, 218,   0, 133,  55, 193,\n         16,  28, 123,  50,   2,  95,  65, 216,  54, 142, 118, 109, 173, 127,\n        163, 186, 133,  29,  21, 112, 227, 153,  84, 118, 179,  13, 219, 208,\n         40,  18,  55, 210,  40, 107, 131, 192, 127, 212,  98,  54, 153,  58,\n         21, 202, 153, 232,  24, 111,  13,  88,  11,  93, 165, 236, 216, 156,\n        218,   8,  32, 169, 136, 194, 222, 198, 140,  28, 215, 186, 223, 167,\n        134, 152,  75,  54,  86, 173, 203,  99, 115, 129,  77, 196, 199,  54,\n        230,  39,  58,  83, 154,  85, 238, 151,  19, 166, 210, 159, 194,   3,\n        146,  85, 162,  53,  59,  22, 180, 104,  56,  93, 221,  52,  83,  32,\n         75, 203, 101, 152,  73,  60,  18,  33,  51,   5, 179,  18, 108, 197,\n         70,  25,  21, 200,  19, 217, 177,  83], device='cuda:0')\nTraining batch 2 at epoch 2 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.8320,  0.4568,  0.0210,  ..., -0.0642,  0.3333,  0.2257],\n        [ 0.1260,  3.0352,  0.1302,  ...,  0.3657,  2.9180,  2.5020],\n        [-1.1055,  0.4111, -0.2192,  ..., -0.2126,  0.9526,  0.0850],\n        ...,\n        [ 0.1445, -0.3022,  0.5327,  ...,  0.7427,  1.3916,  0.7603],\n        [ 0.6133, -1.0664, -0.7036,  ...,  1.2656,  1.3613,  1.3477],\n        [ 0.3152, -0.1760, -0.3679,  ...,  0.2415,  0.1052,  0.4348]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([ 94, 173,  11,  38,  88, 121, 204, 176,  52, 139,  51, 233, 176, 107,\n        111, 156,  22,  88, 122,  70, 190,  87, 162,  87,  30,  27,  18, 194,\n         79, 199,  13, 169,  70, 236,  64, 132, 207,  58, 171,  49,   1,  10,\n        210,  71,  99, 164,   3, 142, 182, 128, 161, 227,  54, 165,  40,  51,\n         87, 125, 229, 211, 136, 209, 218,   8,  71, 106, 144, 146,  60,  30,\n        230,  71,   1,  78,  10, 191, 112,   2,  25,  42, 103, 185, 129, 215,\n         53, 130,  50, 184,  14, 156, 114, 139,  81, 208, 108, 138, 236, 241,\n         32,  78, 191,  23, 242, 131,  63, 148, 198,  68, 134,  13,  81, 241,\n         18,  95, 207, 114, 202,  92,  85,  43,  55, 213,   3,  58,  62, 192,\n          0,  65,  11, 213,  41, 238, 115,  40, 187, 235, 194,  43, 111,  63,\n        226,  28, 118, 157, 166,  28, 152, 213, 239, 202,  39, 108, 230, 177,\n        196, 227, 206, 219, 191, 195, 232, 222,  52, 120,  81,  52,  82, 158,\n        175,  38,  48, 243, 185,  88, 176, 188,  38, 194,  42, 133,  75, 168,\n         24, 145, 134, 246,  28, 244,  88, 175,  17, 145, 130, 111, 221,  71,\n          9,  87, 209, 168, 235,  41, 248, 206, 209,  44, 176,   8, 104,  41,\n        138, 176,  24, 175, 105, 160, 104, 223, 117,  36,  50, 129,   7, 103,\n         38, 212,  64, 246,  45, 199, 172, 189,  23, 177, 111, 198, 160, 125,\n         36, 246, 128, 197,  76, 163,  61,  47, 214, 183, 158, 162, 239, 206,\n         49, 150, 136, 159, 175,  50, 195, 204, 203,   9, 202, 229,  53,  82,\n        244,  64,  48, 107, 232, 167, 223, 178,  58, 144,   7, 166,  47,  42,\n        242,  56,  44, 176,  43, 129, 239, 190, 197, 161,  21,  20, 180, 145,\n         49, 121, 105, 156,  63, 153,  66, 166,  60, 167,  63, 162, 190, 159,\n        152,  45, 145,  43, 206,  90,  87, 245,  49, 191,  31, 108, 230, 225,\n         95, 135, 235, 164,  74, 137, 180,  51, 147, 167,  33,  52, 204,  94,\n        124, 211,  37, 113,  96, 110,  39,  47,  70,   4, 223, 227,  49,  46,\n         39, 236,  83,  73,  57,  97, 144,  75, 194, 101, 126, 170, 167,  91,\n        186,  14, 242, 183, 116,  45, 109, 173,  19, 191,  93, 109,  89, 182,\n        245,  57, 127,  86, 125, 130,   9, 219, 110, 232,  39,  72, 123, 170,\n         42, 125, 147, 105,  72, 211, 131,  52,  65,  26,  64, 157, 120, 205,\n        224, 112, 166, 115,  64,  18, 147, 158, 135,  98,  87, 222,   4, 119,\n         69, 197, 111, 217, 103, 135, 133, 162,  93, 165, 162, 117, 101,  67,\n        190,  15, 154,  30, 188, 156, 235, 172, 194,  12, 149,  25,  55,  39,\n         44,  42,  62, 136,  38,  22, 146, 218,  63,  84,  36, 162, 194,  40,\n         21, 215,  44,  53, 139, 206, 101, 234, 166, 116, 140,  13, 118, 145,\n        215,  68,  13, 189,  61,  35, 114, 194,  78, 199, 116, 207, 121, 139,\n        218,  93,  19, 243,  23, 175,  79, 242,  72,  90, 234, 147, 119,  69,\n         65,  65,  41, 151,  89,  58, 213, 141], device='cuda:0')\nTraining batch 3 at epoch 2 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.7383,  0.6553,  0.2930,  ..., -0.5024, -0.4028, -0.5742],\n        [ 0.0310,  0.0890, -1.5566,  ..., -0.6875,  0.6313, -1.4561],\n        [ 0.3003,  0.4841, -0.6016,  ..., -3.6191, -1.3164, -1.7305],\n        ...,\n        [ 1.1562, -1.6367,  1.2549,  ...,  1.0908,  0.7192, -0.8579],\n        [-0.1611,  0.4209, -0.5518,  ...,  1.7119,  0.6304,  1.2529],\n        [ 0.1405,  1.4365, -4.3320,  ..., -2.9258,  0.4131, -4.1641]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([194, 112, 113,  19, 100, 186,   2,  33, 211, 202,  25, 172,  95, 117,\n         84,  37,  28,   5,  41, 130, 138, 214,  96,  93, 240, 241, 167, 194,\n         28, 167, 175, 176, 188, 150,  47,  27, 162,  53, 237, 212, 237, 200,\n         43,  79, 118,   0, 214, 112, 128, 247,  16, 193,  67, 159, 234,   5,\n        173,  93, 102, 186, 224, 189,   8, 161,  79,  25, 145, 206, 148, 171,\n        206, 154,  83,  89, 172,  21, 116,  44, 150,  85, 215, 206, 182,  94,\n        124, 174, 101,  46, 210,  24, 115,  64, 193, 153,  17, 204, 126, 100,\n        223, 181, 221, 143, 110, 212, 158, 165,  70, 225, 239,  32, 151, 153,\n        101,   5, 196,  56, 195, 162, 245, 182, 164,  10,  76, 220, 226,  61,\n         49, 191,   5,  43, 230,  90, 231,  33,  90,  16,  47, 182, 208,  63,\n        158, 193, 119,  34,  57, 106, 109,  41,   5, 136, 126, 112, 108, 184,\n        244,  87,   4, 171, 149, 181, 132,  75,  96,  53, 195, 180,  53, 212,\n         34, 148, 160,  71,  17, 229,   9, 131,  15,  35, 156,  11,   7,  32,\n         62,  48,  40, 161,  45,  60, 213,  67,  46,  23,  10, 160,  25, 216,\n        124, 112, 215, 144, 242, 181,  55,  93,  87,  11,   3,  43, 214,  28,\n         88, 235, 240,  31,  37, 246,   1, 230, 242,  14, 131,  62, 136, 119,\n        169,  25,  70, 103, 177, 184, 185, 140,   6,  15, 108,  45, 122,  97,\n         49,  41,   7,  52, 234, 213, 140,  41, 118,  92,  48,  37,  62, 171,\n        145, 126, 180,  74, 150, 177, 177,  15,  15,  48,  55, 231,  93,   0,\n         74,  90,  35,  48, 119, 160, 113,  56,  67, 169,  14,  20,  41, 151,\n        186, 230,   5, 136, 172, 145, 193,  51, 176, 105, 188, 210,  98,  82,\n        198,   9, 170, 112,  99, 211, 133,  22,  49, 132, 208, 101, 176,   9,\n        233, 185, 225, 148, 133,  97, 153, 207,  37, 112,  98, 151, 206, 138,\n         46, 188, 123, 107,  74, 196, 248, 187, 144,  15,  87,  48,  17, 247,\n        185,   2, 171,  15, 114, 129,  13,  27,  37, 240, 104, 210,  82, 136,\n        103, 179,  92, 130, 238,  95,  25, 210, 245, 106,  37, 165, 129, 210,\n        219,  81, 149, 197, 179, 182, 181,  72,  21, 159, 132, 181,  26, 133,\n         27, 127,  76, 130,  99, 115,  12,  53, 113,  43,  91,  22, 189, 210,\n         12,  20, 244, 139,  27, 173,  36,  63,   8,  14,  45, 124, 124, 197,\n         22, 212, 177, 139, 166,  31, 133,  76, 130,  18,  78, 135, 193,   1,\n        241, 100,  30,   1, 215,  21, 191,  77, 176, 125,  87,  32,   5, 106,\n        143, 174,  32, 121,  17, 180, 218,  93, 157, 121,  33, 126, 131,  62,\n         78,  85,  93,   2,  99, 124, 135,  28,   0,  53, 115,  38, 221,  22,\n        130, 127,  15,  44,  85,   8,  59,  99, 106, 114, 143,  57, 181,  72,\n         12, 116,  39, 199,  31, 234, 184,  47, 126, 169,  89,  71, 188,  47,\n         85,  99,  54, 232, 174,  45, 152, 203, 110,   1,  81, 103, 149,  67,\n         21,  71, 148,  22, 162,  87, 137,  16], device='cuda:0')\nEpoch 2 took 38.37347650527954 seconds to train\nEpoch 2/5, Training loss: 5.9525, Accuracy: 0.0034\nValidation at epoch 2 input shape is: torch.Size([512, 384, 708])\nValidation predictions: tensor([[ 0.6614,  0.2850, -0.1915,  ...,  0.8792,  0.8491,  0.6582],\n        [ 0.2588, -0.4665,  0.1432,  ..., -0.2096,  0.1431,  0.0994],\n        [ 0.6338,  0.3482,  0.0076,  ...,  0.7437,  0.7063,  0.6200],\n        ...,\n        [ 0.2162, -0.5290,  0.1303,  ..., -0.3158,  0.2878, -0.0797],\n        [ 0.6311,  0.3441, -0.0057,  ...,  0.7529,  0.7295,  0.6351],\n        [ 0.7353,  0.3957, -0.1387,  ...,  0.8919,  0.8158,  0.6575]],\n       device='cuda:0')\nValidation targets: tensor([192, 116, 211, 107, 164, 145, 211, 211,  99,  33, 160,  31,  29, 143,\n         75, 113, 206, 227,   3, 180, 132, 219, 114,  48,   3,  25,  10, 209,\n        184, 190,  86,   7,  58,  90,  99, 148,  54,  32,  74, 225, 194,  85,\n        108,  77, 112, 129, 152,  33, 101, 231,  29, 114,  38,  57, 200, 210,\n        136, 233, 117,  78, 139, 150, 199,  55, 135, 227,  64, 220, 145,  83,\n        129,  73,  33, 215,  31,  64, 222, 116, 140,  80, 245,  56, 115, 160,\n         22,  64,  75,  75,  34,  88, 210,  67, 120,  22,  94,  91, 154, 144,\n         76,  95, 212,  89,   2, 162,  38,  96, 122, 229,  43,   9,  40,  66,\n         96,  58, 118,   8, 154,  34, 109, 200,  55,  75,  45,  30, 182,  35,\n        100, 191,  15,  83,  44,   6, 134,  80,  92, 129,  20,  48, 172, 219,\n         55,  14, 216, 116,  97, 153, 235, 114, 196, 164, 196,  40,  72,  68,\n        206, 224, 156, 200,  35,  16,  91, 123,  15, 157,   6, 151, 241,  57,\n        178, 114,  43,  27,  87, 218,  75, 154, 135,   5,  39,   0, 238, 171,\n        170, 232, 143, 201, 118,  52,  69, 123,  56, 200,   7, 145,  73,   5,\n        136,  75, 237,  23, 114,  13, 178,  41, 213, 151,  90, 232, 136, 108,\n          1,  81,  88, 217, 197, 191, 100, 213, 168,  92,  31,  12, 147, 136,\n        161,  11,  45, 208,  36,  31, 132, 115, 108, 115,  48, 190, 201, 128,\n         19, 129,  45, 154,  17, 185, 136,  63, 176, 117, 204,  66,   8, 137,\n         59, 101,  49, 130, 178, 151, 216, 136, 113, 215, 125,  82, 217,  44,\n        162, 196, 171, 157, 172, 214, 163, 221, 119, 144, 154, 104,  96, 108,\n        176, 106, 161,  69,  17, 155, 216,  54,  85, 212,  41,  12,  98, 113,\n         66, 133,  98, 150,  30,  51, 194,  48, 169,  63, 124,  48,   2, 238,\n         64, 169,  30, 101, 114, 196,  70, 150, 202,  49, 140,  65, 198, 225,\n        146, 210,  15,  32, 102,   6, 101,  19,  78,  85,  23,  38,  40,  94,\n         45,  98, 193,  88,  35,  57, 144, 162, 234,  72, 217, 206, 208, 191,\n        203,   2, 195, 155,  71, 126, 141, 133, 202,  38, 104,  34,  80, 172,\n        208, 139, 137,  74, 164,  17, 214, 249,  62,   1,   3, 187,  60, 104,\n         51, 219,  21, 165, 147, 121,  46,   3,  80, 201,   5,  68, 175, 177,\n         84, 113, 157, 206,  31, 108, 118, 124, 204,   4, 195, 248, 211, 206,\n        172,  93, 193, 155,  63,  66, 166,  11,  77,  91,  29,  76, 205,  23,\n        178,  21, 167,  80,  48, 243, 203, 210,  28, 136,  82, 103, 113, 213,\n        118,  58,  34, 120, 165, 129,  83, 236, 188,  47, 191,  59,  30, 186,\n        240, 190, 211, 141,  75,  10, 233,  89, 185, 160,  72, 220, 175, 236,\n        147,  31,  62,  92,  61,  36,  58, 229, 207,  69, 245, 137,  56, 107,\n         47, 125,  29, 202,  98,  25, 193,  83, 119,  84, 223, 131,  44, 129,\n         75, 170,  29,  87, 140,   5,  24, 154,   4, 171,  55, 183,  49, 132,\n        247, 219, 172, 116,  98, 159, 137, 107], device='cuda:0')\nValidation Loss: 5.6569, Accuracy: 0.0059\nOn epoch number: 3 / 5\nTraining batch 0 at epoch 3 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.0617, -0.1353,  0.5752,  ...,  1.2764,  0.6846,  0.0284],\n        [-0.5259, -0.3518,  1.0830,  ..., -0.6567,  0.2039, -1.2314],\n        [ 0.7578, -1.0459,  1.2725,  ...,  1.5518,  1.0439,  0.6362],\n        ...,\n        [ 1.7461,  1.6084, -0.0459,  ...,  0.4263,  0.3184,  0.1958],\n        [-0.6079, -0.4644, -1.5381,  ...,  1.6582,  1.1221, -0.8394],\n        [ 2.1875, -1.3066, -0.5400,  ...,  1.3965,  1.4482,  1.1953]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([235, 152, 191,  69, 160,  90, 121, 233,  86, 129, 133, 242, 202, 143,\n         41,  74,   1, 158,   6,  68,  33, 228, 194,  70,  70, 158, 108,  11,\n         32,  12, 153,  82,  52,  33,  54,  27, 104, 206, 167,  64,  40,  21,\n        171,  13, 213, 169, 211, 237,  62, 177,  12, 114,  93,  22, 118, 109,\n         56, 240,   6,   9, 183, 212, 178,  71, 111, 136, 206, 108,  77, 212,\n        213, 133,  41,  78,  58,  69,  52, 197, 194, 220, 188, 133,  67, 218,\n         67,  70,  22, 164, 184,  84, 226, 171,  38,  32,  96,  16, 112,  50,\n         75,  74, 127,  72,  28,  38, 233, 194, 116, 104,  23, 168, 196,  10,\n        206, 179,   3, 231, 181, 186, 130,  65, 243, 151, 242,  25, 179, 174,\n        199,   5, 182,  30,  90, 125, 139, 117,  16,  50, 140,  66, 240, 153,\n        173, 218, 104,  56, 218,  13,  39,  44, 112, 159, 241, 238, 140, 206,\n        110, 197, 123, 116,  93, 124, 148,  64,   7,  85,  64, 112,  47, 197,\n         45, 103,  48,  43, 108,  97, 208, 104, 177, 165,  71, 123, 239, 181,\n         28, 126, 192,  42, 153,  62, 210, 136, 125, 205,  78,  51,   7, 173,\n         34, 113,  61, 149,  62,  51, 163,  46,  66, 130, 236,  78, 139,   6,\n        248, 245, 141, 232, 104,  21, 124, 150, 181, 169,  91, 236, 143, 116,\n        165, 165,  51,  39, 173,   1,  74,  56,  16, 247, 218, 206,  10, 101,\n         41,  94, 124,  38,  65, 208,   7, 199,  65, 191, 232, 128, 145, 176,\n        233, 112,  49, 180, 170,  49, 190, 109, 186,   8,   9,  50,  94,  42,\n        136,  27, 176,  23, 187, 222, 174, 102, 131, 118,  94,  34,  25,  20,\n          4, 133, 135, 104,  53, 145,   8, 140,  66, 108,  83,  89,  77,  98,\n        118, 166, 234, 130,  88, 153, 233,  37,  83, 116,  50, 239, 234, 225,\n        135,  64, 181,  19, 186, 148, 158, 210,   5, 156, 224, 212,  43,  83,\n         97,  27, 164,  32, 124,  87, 139, 214, 162, 124, 115, 136, 138, 210,\n        152, 115, 169, 227,   2,  32, 137, 209,  73, 162, 143, 215,  50,  61,\n         67,  88, 159, 232, 211, 133,  36,  32,  55,  93,  97, 203,  49,  11,\n         99,   8,   1,  93,  69,  93, 235, 161, 127,  31, 219,  88,  15,  11,\n        155,  49,   8, 107, 232, 158,   1,  30,  25, 115, 129,  88,  33, 195,\n        147, 167,  35, 108, 117,  14,  22,  15,  17,  87,   9, 166,  95, 173,\n        223, 210, 205, 114, 212, 230, 125, 136, 159, 172,  82, 119, 190,   5,\n        148, 203,  39,  74,   2,  93, 215, 182, 141, 145, 162, 220,  65, 115,\n        174,  15, 154, 215,  86, 197, 243, 199, 206,  70,  25, 207, 130,  13,\n        233, 183,  41,  44,  38, 194, 121, 132,   3,  37, 213, 168,  45, 211,\n         75, 128, 225, 147,  85, 148,  40, 206, 201,  76,  98, 153,  50, 237,\n         77,  81, 149,   8, 188, 176, 148, 191, 161, 146,  58, 246,  86, 155,\n         45,  47,  87, 162,  10,  72,  57, 234, 193,  41,  77, 150,  14, 107,\n         21,   1, 125, 214, 142, 229, 235,   9], device='cuda:0')\nTraining batch 1 at epoch 3 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.5371, -0.4458, -0.0757,  ...,  0.3784, -0.3560,  1.7148],\n        [-0.3574,  0.2717,  0.0960,  ...,  0.3640,  0.0831,  0.2102],\n        [ 1.5322, -0.1747, -0.0660,  ...,  0.9038,  0.0106,  0.0691],\n        ...,\n        [-0.1085,  0.2112, -0.3904,  ...,  0.2603, -0.4292,  0.1562],\n        [ 0.3579,  0.7026, -0.6885,  ...,  0.0423, -0.3127, -0.4099],\n        [-0.0196,  0.5454, -0.9307,  ...,  1.8828,  1.1348, -0.5352]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([100,  11, 157, 237, 110,  98, 132, 145, 245, 236,  20, 177,   5, 148,\n        207,  47,  37, 119, 125, 119, 193,  35,  26, 160,  48,  57,  84, 165,\n        235, 172, 246,  31, 216, 124, 235,  28, 207, 217,   5,  89, 162, 176,\n        163,  24, 214, 199,  93,  98,  29,  32, 160, 218,  10, 212,  33,   4,\n        174, 211,  18, 165, 240,  92, 104, 202, 234, 139, 224,   2,  17,  21,\n        177,  71,  47,  98,  25, 171, 230, 157,  57,  91,   7, 182, 179,  93,\n        249,  82, 245, 194, 183, 183, 157,  81,  39,  96, 145, 144, 163, 159,\n         45, 176,  58,   9, 181, 193,  99,  90, 238,  99, 143, 193,  27, 129,\n         26, 111, 143, 218, 225,   5, 162, 180, 248, 101, 228, 223,  50,  84,\n        237,  89, 210, 107, 189, 175,  88, 230, 147, 134, 103,  65, 239, 194,\n        101, 161, 149,  91, 138, 244,  81,  44, 177, 139, 247,  83,  18, 242,\n         87,  43, 113, 125,  28,  52,  90,  41, 184, 126, 117, 233, 108, 105,\n        234,  85,  21, 188,  56,  15, 189, 127,  45, 135, 139,  10,  36, 242,\n        217, 198, 196,   7,  47, 182,  57,  31,   6,  39, 247, 185, 159, 115,\n        175,  87, 169, 238,  87, 157, 223, 118, 138, 172, 227,  98,   5, 236,\n        195, 173, 126,  45,  53, 134, 152, 167, 136, 149, 202,  53, 172, 214,\n        119, 165,  63,  94, 169, 192,  56,  76, 195,  59, 195, 168,  31,  76,\n         99,  54,  55, 184,  71,  49, 134,  12, 201,  19, 132, 194, 180, 158,\n         19,  22, 192, 206, 167,   0, 175,  19, 231,   6, 151, 160, 136, 171,\n         22,  17,  28,  98,  24,   1,  98, 154, 203,  25,  47,  88, 114, 202,\n         67, 244, 108, 208, 127, 187, 216,  96, 197, 206, 203, 161,   6, 167,\n         58, 193,  56,  99, 204, 186, 236, 218,  68,  21, 229, 233,  87, 164,\n        139, 125,  21,  43,  57,  48,  27,  53, 150,  21,  65,  45, 215,  11,\n         37,  85,  41,  16, 106, 204, 115, 239, 128,  70, 195, 177,  63, 249,\n         93,  48,  25,  92, 235,  37,  28,  54, 230, 192, 158, 154,  23, 105,\n         63, 178, 215, 225, 121, 150,  63,  24, 102, 166, 144, 245,   4, 179,\n        103, 119, 161, 223, 130, 106, 103,  53,  77, 200, 210, 204, 146,  90,\n        239,  88, 209,  58,  89, 174,  44, 122, 103, 177, 114,  70,  11,  94,\n        236,  43, 212,  91, 210, 156, 117,  25,   3, 213,  12, 198, 145, 164,\n        103,  18, 171, 180, 194,  60, 131, 113,  62, 153, 126, 131,  85, 119,\n         38, 170, 105,  32, 185,  34, 100, 223,  82, 241,  35,  60,  66,  78,\n        109,  70, 198, 193, 183,  58,  95,  75, 119,   9,  78,  74,  76, 158,\n        104, 209, 198,  49,  89,   4, 163,  85,  87,  93, 238, 154, 133, 120,\n         53, 172, 226, 154,  39,   7, 177, 156, 175, 143, 246,   8,  78, 148,\n        187,  72, 151, 242,  46, 237,  22,  64,  92, 118, 181,  43,  87,  54,\n         15,  56, 188, 215, 191,   1,  18,   2,   8,  11, 185,  93, 229, 176,\n        232,   4, 101, 153,  19,  43, 194,  21], device='cuda:0')\nTraining batch 2 at epoch 3 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.8267, -1.1494,  1.3604,  ...,  0.2649, -0.2625, -0.5962],\n        [-0.3259,  0.0117,  0.2947,  ...,  0.1205, -0.4229, -0.3589],\n        [-0.2942, -0.0439,  0.4131,  ...,  0.0615, -0.6104, -0.5103],\n        ...,\n        [-0.4607,  0.0608, -0.4338,  ...,  0.0615, -0.9082, -0.3237],\n        [ 0.1968, -0.4744,  0.6768,  ...,  0.0585, -0.4878, -0.4592],\n        [-0.2949, -0.4980, -0.2419,  ..., -0.2389,  0.0975, -0.3413]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([ 93, 112, 224,  66,  25,  49, 146,  41,  81, 121,  90,  71, 162,  17,\n        136,  89,  71, 146, 118, 179, 200, 244,  91,  17, 221,  61,  82, 221,\n        199,   8, 109,  69, 245, 181, 211, 218,  18, 128,  53, 105,  33,  14,\n          0,  92,  40, 171, 235, 182,  67, 149,  40,  60,  64, 225, 138,  55,\n         25,   1, 141,  45,  96, 153, 240,  65, 218, 145,  55,  45,  43, 145,\n         96,  14, 170, 127, 186,  34, 130, 214, 151,  36,  66, 123,  75, 240,\n         82,  60, 163, 159, 126,  83, 208,   2,   2, 140, 151,   0, 248, 146,\n         89, 116, 167, 193,  39, 173, 214,  75, 135,  46,  75,  33, 246, 227,\n        246,  13,  62, 224,  43, 163, 142, 113,  71,   0, 131, 165, 129,  12,\n        122, 111, 162, 237, 138, 146, 161, 180,  85, 173, 192, 181,  17,  31,\n        160, 208, 229,  67,  13,  87, 220, 167,  47, 111,  41, 163, 223, 176,\n        161,  14, 150,  54, 242, 130, 185, 198, 152, 107, 211,  81,  74,   2,\n        132, 244,  92, 193,  39,  13, 246,  92, 159, 180,  52,  39, 121, 232,\n        117,  80, 162,  14, 238,  96,  23, 125,   4,  50, 198,  12,  40, 201,\n         22,  45, 109,  70, 119, 131, 126,  79,  81,  30, 160, 201,  48, 203,\n         14,  64, 194, 101,  77, 226, 134,  97, 134, 121, 223,  63, 106, 223,\n         44, 230, 158, 189, 166,  49, 160, 104, 130,  78, 153, 100, 215, 118,\n         52, 237,  91,  68,  42, 124, 136,  85,  88, 116,  44,  45,  18, 204,\n         89, 223, 171,  93,  53, 176,  57, 172,  99,  29,  52,  90,  42,  37,\n        177,  84,  37,  92, 156,  98, 210,  47, 213,  19, 175,  51,  70, 104,\n        166,  38, 220,  38, 157, 131, 178,  48, 150, 226, 155, 213,  21,  59,\n        126, 220, 200, 166, 211, 190, 115,  15, 153, 149,  12, 228, 201, 177,\n        214, 156, 238, 157, 236,  90, 176, 131, 108,  57,  19, 180, 115, 136,\n         78, 192, 222, 122,  43,  58,  18, 159, 117,  25,  18,  53,   0,  50,\n        145, 180, 218,  93, 160, 174, 241,  53,  65,  52,  27,  29, 197,  81,\n         54, 172, 189,  55,  73,  63, 149,  48, 239,  26,  70, 203, 119, 230,\n        148,  38,  54,  71, 182, 231,  72, 199,  66, 188, 181, 131,  41,   8,\n        194,   1,  82, 143, 186,  40, 100, 184, 232, 213, 169, 200, 131, 242,\n        215, 147,   4,  13,  99, 246,  55,  37, 238, 211,  10, 184, 202,  57,\n         81, 176,  58, 109, 216, 186,  84,  66, 184, 117,  85, 146, 219, 185,\n        100, 142, 169,  19, 218, 135, 133, 134, 194, 144, 231, 106,  38,  95,\n        126,  37, 215,  21,  91,  26,  45,  40, 188, 156, 209, 216,  29,  69,\n        232, 136, 105,  76, 186,  76, 165,  18,  62,  25, 232, 121,   3,  36,\n        232, 203, 144, 212, 144, 108, 231,  64,  87,  45, 173, 198,  31, 142,\n        177, 112, 123, 131, 159,   5, 130, 145,   1,  15, 117, 176, 210,  10,\n         28,  78, 242, 193, 185,  12, 138, 222,  48,  16,  53, 181, 172, 139,\n         42,  47,  17, 134,  61,  11,  30, 184], device='cuda:0')\nTraining batch 3 at epoch 3 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-1.9717,  0.2673,  0.7241,  ..., -0.2893, -0.9971, -0.1672],\n        [-0.4976, -0.2734,  0.0686,  ...,  0.0871,  0.2073, -0.2151],\n        [-0.7017,  0.1044,  0.4482,  ..., -0.2527,  0.4832, -0.2324],\n        ...,\n        [-0.3774, -0.7959, -0.2786,  ..., -0.4087, -1.0479,  1.1660],\n        [-0.4976,  0.3513,  0.0876,  ...,  0.1054, -0.4771,  0.0320],\n        [-0.3140, -0.7534,  0.2311,  ...,  0.8511, -0.5288, -0.3979]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([ 73, 221,  64, 245, 140,  61,  12,   7, 141,  63, 135, 210, 162, 168,\n        245, 130, 120, 194, 152,  51,  79,  28, 136, 246,  71, 191,  92, 129,\n         45,  75, 160, 188, 119, 182, 236,  84,  75, 133,  63,  46, 161, 156,\n        109, 196,   7,  43, 166,  31,   0, 120, 126,  22, 224,  58,  94, 146,\n        212, 230,  13, 105,  67,  22,  19,  44, 111, 224, 236, 214,  51, 190,\n         14,  85,   5, 186, 164, 188, 158, 202, 112,  92,  22, 206, 108, 182,\n         38, 103, 238, 103, 246,  81,  55,  16,  58, 205, 199, 135, 137,  85,\n         96, 196, 101, 133,  53,  52, 184, 142,  33, 176, 131,  72, 194,   5,\n         61, 108,  74, 175, 113, 107, 115,  39, 185,  22, 173,  63, 199,  51,\n         86,  79, 112, 166, 131,  12,  13,  44, 105,  57, 101, 126,  79, 147,\n        156,  85, 101,   1,  39, 131,  69,  42, 120,  27, 189, 226, 153, 112,\n        171,  11,  15, 104,  38, 109, 132,  85,   8, 208,  60, 112, 239, 192,\n         93,  74, 221, 101,  45, 247, 131,   9, 106, 183,   9, 101, 102, 204,\n        221,  48, 115,  47, 231, 191,   2,  93, 162,  82,  30,  95,  99,  18,\n         28,  53, 179,  31, 114,  67, 190,  15, 140,  73, 171, 110, 126, 171,\n        232, 194, 132, 129,  11, 191, 241,   4,  56, 105,  16, 197, 153,  25,\n        164, 217, 129,  40,  72,   2, 113, 146, 212, 191,  95,  17, 208,  32,\n          5, 135, 157, 146,  75, 244,   7,  61, 124, 139,  38, 218, 117, 149,\n        117, 116,  45,  88,  36, 202, 239, 225, 242, 227, 112,  24,  37, 110,\n        182,   5,  36,   0, 144,  77,  25, 226, 167, 130, 240, 225,  20, 207,\n        216,  47, 195, 156, 241, 141,   2,  24,  20,  76, 125,  65, 137, 237,\n        151, 144,  75,  53,  39,  87, 189, 175,  95,  83,  31, 170, 174,   5,\n         99, 114, 191,  98,  98,  76, 186,  49, 248, 180, 123, 197, 188,  88,\n        140, 162,  60, 165, 210,  53, 147, 220,  34,  36, 180, 100, 245, 169,\n        140, 216, 144, 228,  21, 171, 115,  99,  40, 153, 231, 157, 106, 203,\n         48,  22,  41, 145, 162, 219, 156, 113, 161, 120, 183,  20, 198, 113,\n        149, 207,  33,  94,  27, 235,  43,  50, 133,  94,  70,  34, 219, 142,\n        165, 135,  44, 135, 166, 230, 125, 137,  72, 199, 173, 175, 154, 130,\n         28, 128,  44,  65,  19,  82, 194, 109, 198, 196, 234, 109, 124, 172,\n         32, 216,  78,  91, 166,  34, 127, 212,  33, 204, 172, 196, 106, 110,\n        181, 213, 118,  52, 188,  12, 102,  59,  21,  35, 234, 126, 182, 206,\n         80, 229, 124,  96, 191, 206,  15, 210,  29, 152, 227, 164, 142,  85,\n        149, 241, 235,  18, 227,  70, 135,  24,  64,  79, 115, 221,  88, 167,\n        199,  32, 156,  39, 224,  98, 204, 181, 176,  61, 121,  52, 117,  55,\n         94,  41, 111,  22, 169, 127, 126,  94,  35,  85, 147,  80,  12,  42,\n        241,  89,  51, 229, 245, 162, 241,  89,  37, 125, 243,   9,  82, 110,\n         76,  17, 199, 213,  49,  65, 208,  16], device='cuda:0')\nEpoch 3 took 39.176228046417236 seconds to train\nEpoch 3/5, Training loss: 5.7337, Accuracy: 0.0020\nValidation at epoch 3 input shape is: torch.Size([512, 384, 708])\nValidation predictions: tensor([[ 0.0804,  0.0087, -0.0748,  ...,  0.0710,  0.0319, -0.0525],\n        [-0.0302,  0.1484,  0.0373,  ..., -0.0706,  0.1416,  0.1577],\n        [ 0.1101, -0.0823, -0.0518,  ..., -0.1211,  0.0674, -0.0423],\n        ...,\n        [ 0.0221,  0.0183, -0.1255,  ..., -0.0168,  0.0841, -0.0778],\n        [ 0.0204, -0.0139, -0.1345,  ...,  0.0627,  0.0438, -0.0393],\n        [ 0.0036, -0.0993, -0.0100,  ..., -0.1389,  0.0487,  0.0336]],\n       device='cuda:0')\nValidation targets: tensor([143, 139, 150, 240, 159, 165, 150,   5,  30, 108,  69, 241, 175,  98,\n        172, 168,  98, 211,  15,  81,  31,   0,  72,  27,  94, 203, 132, 211,\n         38, 145, 118, 190, 216,  47, 200,  29,  48,  94, 135, 150, 121, 107,\n        196, 119,   1,  11, 153, 220,  65, 113, 164, 120, 171, 217, 144,  80,\n        208,  75,  73, 200, 203,  92, 207, 225,  66, 194,  80, 164, 220, 187,\n        238, 116,  45, 114,  92,  38,  29,  91, 136, 162, 161,  85, 184,  76,\n          6, 233, 170,  63,  29, 125, 154,  31, 206, 231,  97, 136,   6,  89,\n        201,  51, 198,  55,  40, 107, 213,  47,  66, 234,  85,  25, 176, 219,\n         75,  38,  83, 218, 124, 136, 210,  13,  48,   2, 161, 212, 172, 192,\n        148, 157, 236,   7, 243,  90,  96,  87,  75,  63, 193, 176,  64, 208,\n         44, 115,  24, 104, 219, 225, 237,  76, 135,  29,  49, 144, 191, 154,\n        201, 200, 157,  31,  48,   8,  12, 247,  73,  40,  75,  12, 211, 115,\n        116,  25,  34, 248, 202,  78, 101, 206,  77,  45, 229, 140,  54,   7,\n         49, 137,  88, 104, 130,  38,  39, 109, 212,  48, 219,   5, 132, 154,\n        177, 165, 245, 114,   2,  33,  64,  54,   9,  33, 190,  64, 155, 137,\n        196,  14,  29,  55,  45,  84,  10, 141,  28, 108, 216, 160, 204, 224,\n         15, 102, 100, 193,  30,  88, 219, 104, 195,  92,  49, 171, 202, 232,\n         23,  74,  35,  57, 129, 129, 143,  44,  75,  34,  34, 116,  82, 129,\n        215,  68,  64, 114, 222, 140, 191, 185,  69, 114, 126,  90, 114, 178,\n         63, 101,  22,  89,  32,  32,   8, 172, 151,   5,  95,  59,  40, 172,\n        205,  98,  88, 186, 144,  55, 169,  35,  84,  16, 155, 114, 188, 210,\n        190, 117, 131, 172,  21, 249,   3, 233, 157, 206, 129,  78, 221,  33,\n        136, 108,  11, 108, 129,  41, 209,  30, 170, 122, 120, 112, 154, 178,\n        217,   4,  72,  36, 196,  21, 133, 116,  91, 191,  80, 194,  23, 162,\n        154,  75, 178, 107,  66,   6, 154, 132,  43, 145, 197, 204,  58,  83,\n        118,  77,  82,  86,  91, 180,  22, 169, 113, 146,   1,  52, 196,  85,\n         56, 118, 162,  72,  31, 178, 210, 171,  34, 214, 211, 140,  35,  70,\n        223, 128, 113, 202, 137,  62, 133,  68, 175, 123,  83,  99, 238, 134,\n        227, 201,   5,  48, 182, 213, 137, 155,  98, 129,  57, 123,  36, 210,\n         66, 145, 164,  87, 151,  58, 167, 106,  10,  83, 199,   4, 147,  17,\n        208,  56, 147, 216, 227, 136, 101, 136,  80,  75,   2, 235, 124,  57,\n         41,  80, 115,  96, 185, 113, 193, 108, 206,  56,  59,   3, 118,  96,\n          3,  31,  58,   3,  43,  30, 236,  51, 211,  31, 119, 103, 206,  61,\n        152, 163, 160,  17,  20,  98, 100,  46, 101, 136, 166,  62,  45,  99,\n         44,  60,  93, 117, 215, 191,  23,  71, 151, 113, 200,  74, 195,  17,\n         15, 147, 245, 213, 232, 125,  67, 141, 160,  58, 229, 139,  19,  69,\n        183,  48,  55, 217,  19, 156,  75, 214], device='cuda:0')\nValidation Loss: 5.5285, Accuracy: 0.0039\nOn epoch number: 4 / 5\nTraining batch 0 at epoch 4 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.0441,  0.0193, -0.3652,  ...,  0.0309,  0.1553,  0.2219],\n        [-0.1384, -0.1628, -0.1299,  ..., -0.2457,  0.0757, -0.0475],\n        [ 0.3149,  0.0359, -0.0131,  ...,  0.0656,  0.2361, -0.1661],\n        ...,\n        [ 0.5059, -0.0978,  0.5391,  ...,  0.0127, -0.2505, -0.1263],\n        [-0.1985,  0.0204, -0.0272,  ..., -0.1169, -0.0979,  0.0540],\n        [ 0.1981,  0.1964,  0.0401,  ...,  0.0582, -0.2155, -0.0228]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([135,  94, 132,  36,  19, 153, 182, 194, 187, 145, 105,  63,  89, 223,\n         69, 176, 211, 209, 103,   6, 140,  18, 160,   1,  10, 198,  49, 188,\n        108, 245,   7,  22, 223, 218,  18, 193, 194,   8, 131, 130, 207,  90,\n        226, 245,  15,  75, 164,  74, 104,  72,  53, 199,  69,  28,  14, 233,\n        145, 249,  73, 145, 233, 205, 180, 125,  54,  93,  72,  66,  99,  48,\n         28, 137,  31, 245, 136, 113,  98, 153, 181, 169, 152,  27, 181, 191,\n        133,  47, 145, 124,  45,  59, 189, 208, 104,  96, 246,  48,  25, 103,\n          4,  40, 235, 172, 231, 210, 116,  65, 218,  56,  70, 151,  91,  43,\n         32, 221,  49, 101, 170,   2,  19, 118, 193,  89,  58, 161,  61,  67,\n         98, 103, 165,  82, 133, 242, 216, 191,  71,  87, 203, 102,  26,  45,\n         55,  77, 156,  37, 225, 174,  81,  11, 184,  19, 182, 110, 121,  93,\n         89,  37, 210,  14,  17,  95,  75, 126, 128, 199,  74, 111,  53, 210,\n         48, 148,  98, 101,  44, 125,  19, 234,  91, 126,  33, 108,  96, 130,\n        127, 129, 230,  44,  74, 158, 210, 244, 150, 169,  51,  16, 119, 104,\n        166, 157, 159, 139, 180,  17, 160,   8, 135, 218, 134,  95, 102, 152,\n        136,   8, 184,  24, 162, 177, 228,  25, 112, 220, 174, 242,  47, 118,\n         70,  16, 147,  99,  29, 159, 124,  15, 108,  27, 162,  25, 181,  10,\n         60,  93, 182,  18,  67, 149,  84, 197, 193, 218, 134, 104,  45,   8,\n         30,  66, 172, 188,  81,  20,  49, 229,  88, 206, 147, 123, 108, 185,\n        191, 240, 112,  49, 146, 120, 109,  64,  93, 197,  41, 208, 120, 157,\n        111,  15,   4,  43, 236, 162, 157, 146,  31,  98,  58, 171,  53,  91,\n         94,  46, 159, 196, 206,  21,  93, 197,  15,  83, 197,  55,  35,  95,\n        156, 235, 173,   1,  31,  33, 238, 107, 144, 160,  19, 234, 183,  77,\n         43, 150, 206, 121, 198, 173, 151,  22, 158, 164,   2,  43,  87,  87,\n         18, 248,  22, 210,  52, 181,  10,  63, 166,  43, 186, 226,  95, 182,\n        168, 142,  53,  71,  88,   8, 147,  64,  82,   9, 132,  98, 126,  62,\n        110, 100,  94,  22,  42, 146, 155, 124, 190,  49, 137,  25, 166,  90,\n        231,  30, 113, 106,  70,  99, 246, 213,  92, 173, 209, 162,  37, 232,\n        125, 106, 100, 112, 132, 189, 112,  47,  29, 246,  54,  41,  69, 133,\n        150, 126,   7,  38,  72,  12,  40, 153, 169, 241, 212, 138,  95, 116,\n         51, 216,  36, 184, 141, 176,   9, 126,   2,  84,  43,  93, 129,   9,\n         90, 105, 234,  18,  76,  89, 117, 215, 166, 136, 231,  47, 183,  54,\n        117,   1, 136, 235, 133, 141, 140, 163,  36, 173, 236,  77,  33, 145,\n        139,   0, 194, 184, 164,  38,  22, 203, 156,  24,  11,   8, 156, 181,\n        167,  17, 219,  16, 106, 160, 165, 150, 246,  14, 149, 116, 189,  45,\n        242,  10,  96, 140, 214,  28, 211, 243, 240,  54,  42, 235, 237, 215,\n         21,  60, 246,  88, 131, 136, 112,  73], device='cuda:0')\nTraining batch 1 at epoch 4 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.4968,  0.1038,  0.2261,  ...,  0.1224,  0.0711, -0.1641],\n        [-0.1661, -0.0034, -0.0824,  ...,  0.1210, -0.0056, -0.0699],\n        [-0.0689, -0.3225,  0.3320,  ...,  0.0298,  0.2954, -0.0784],\n        ...,\n        [ 0.0676,  0.1539, -0.0530,  ...,  0.1008,  0.0175,  0.0663],\n        [-0.0292, -0.1591,  0.0233,  ...,  0.0356,  0.0183,  0.0956],\n        [ 0.3303,  0.5229,  0.1327,  ..., -0.5962,  0.6069,  0.1582]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([ 45,  58, 208, 130, 105, 237, 105,   5,  94, 172,  73,  15,  62, 194,\n        112, 177,   5,  84, 126,  88, 186, 213,  39,  24,  34, 202, 145, 131,\n        139,   2, 114, 212, 104, 135,  77, 244,  76, 237,  25,  51, 158, 180,\n        121,  43, 204,  27,  49, 209,  64,  80, 220, 230, 114,  23,  91, 156,\n         39, 180,  44, 172, 171,  94, 154,  68,  11, 195, 166, 208, 149, 116,\n        186, 139,   4, 182,  50, 239, 198, 224,  18, 229,  67, 106, 104,  42,\n         12, 230, 237,  16, 173, 138,  87,  37,  96, 239, 191,  81, 159,  78,\n        158,  87, 227, 210, 108, 203,  35,  65, 131, 238, 220, 232,  23, 194,\n         27,  57,  18,  76,  22, 201, 157, 176, 175, 136,  64,  12,  84, 169,\n         75, 130,   6,  31, 235, 101,   5, 213, 212, 193, 171, 162, 161, 136,\n        175, 122, 203, 229,  25,   2, 201, 227,  65, 139, 164,  44, 102,  17,\n        187, 208, 125,  21, 210,  59, 157, 125,  77, 112,  61,  34, 109,  19,\n        218, 105, 163,   3,  55, 241,  35, 134, 180, 204,  22, 234,  40, 154,\n        101,  97, 135, 229,  87,   0, 238, 217, 188, 135, 189, 126, 140, 132,\n         96,   0,  75, 157,  10,  14, 179, 160, 115,  61, 212, 166,  45,  47,\n        153,  12,  56, 229, 118, 233, 229,  75,  85,  75, 223,  94,  53,  11,\n        182, 216,   7,  86,  53,  97, 175,  37,  22,  39, 161,  98, 214,  49,\n         33,  32,   1,  63, 181,  57, 158, 142,  34, 134, 167,  78, 177, 129,\n        100, 167, 130,   8,  32,   4, 156, 239,  45,  52, 172,  26, 143,  38,\n         94, 174, 123, 105, 205,  11,  39,  92,  64,  94,  86, 232, 167, 148,\n         19,  20,  76,  32, 198, 242,   0, 214, 199,  33,  33, 131,  90, 203,\n        144, 233,  35, 130, 148, 124, 235,  55, 175,  31, 186,  35,  42,   9,\n        215, 127, 153,  58, 195, 108,  72, 236, 138,   5,  81,  25, 192,  99,\n         88, 120,  36,  45,  71, 108,  60, 198,   6, 165, 150, 221, 215, 172,\n        216,  29, 162, 162, 177, 161,  28, 146, 117, 175, 121,  92,  89,  91,\n         18, 152, 131, 231, 177, 126, 239,  87,  46, 214,  57, 194, 232,  47,\n         78, 142,  15, 155, 213, 114,  72, 190,  99, 216, 136,  36,  92,  63,\n        127, 214,  38,   8, 230,  31, 118,  82, 119, 160, 238, 211,  99, 167,\n        194,  38,  66, 144, 136, 140,  11, 120,  78, 109, 228,  43, 194, 225,\n          0,  40, 202,  28, 192, 236,  33, 106, 147, 212, 132, 163,  41, 171,\n        110,  93, 116, 117,  24,   5, 191, 127, 165,  68, 165,  64, 161,  50,\n         57, 102, 141, 204,  52, 171, 194, 122,  30, 108,  92,  25, 246, 113,\n        185, 237,  11,  70, 109,  21, 153, 121, 196, 173, 210, 142, 125, 171,\n         33, 215, 126, 219,  78, 239, 197,   1, 191,  41,  65, 248, 211, 230,\n        135, 206, 238, 246, 183, 193,  13, 173, 212,  29, 239, 162, 160,  47,\n         55, 112, 121, 202,  13, 219, 127, 206,  75, 100, 156,  30,  28, 144,\n         60, 198, 147,  56, 199,  14, 220,  70], device='cuda:0')\nTraining batch 2 at epoch 4 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.0378,  0.3206, -0.1143,  ...,  0.3687,  0.0557, -0.0345],\n        [ 0.1549,  0.2157,  0.1946,  ...,  0.2500,  0.0781, -0.1294],\n        [ 0.3809,  0.2156, -0.1548,  ...,  0.0448, -0.0620, -0.2136],\n        ...,\n        [ 0.0806, -0.0692,  0.1886,  ...,  0.0523,  0.2686,  0.1346],\n        [ 0.1416,  0.1654, -0.6489,  ..., -0.4844,  0.1324, -0.0748],\n        [ 0.2620,  0.0145, -0.2534,  ...,  0.2313,  0.2399, -0.2109]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([ 74, 148,  66,  54,  45, 111,   1, 104, 119,  14,  65, 118,  93, 128,\n        245,  67, 221, 241,  62, 177,  98,  85, 232, 115, 188, 101,  78,  72,\n         28, 201, 129,  85, 163, 164, 176, 242, 153, 181, 124,  23, 242,  65,\n        211,  53, 192,  76,  38,  16, 162, 167, 191, 112, 159,  74,  28, 200,\n        143,  47,  54,  53,  50,  66, 147, 170, 206, 248,  79, 218, 223, 179,\n        125,  69,  98,  69,  70,  56, 125,  88, 192, 131, 116,  37, 188,   9,\n         40,  72, 180,  85,  78, 212,  45,  28, 213,   5,  21, 213,  81,  85,\n         32, 118, 218, 117, 199, 115, 212,  44, 198,  63,  53, 177, 177,  21,\n        113, 117, 219,  86, 149,  86,  50,  54,  50, 162,  53,  70, 145, 204,\n        169,  37,  31,  62, 231,  40, 245, 241, 138, 118,   5, 202,  40,  64,\n         85, 202, 182, 106, 176, 227, 237, 182, 232,  25,  43,  21,  44, 165,\n        186, 117,  56,  34, 109,  88, 188,  81,  40,  70,  85,  47, 200,   4,\n         45,  79, 115,  32, 111,   0, 197,   4, 117,  63,  76,  71, 179,  48,\n        114, 165, 211, 141, 105,  57,  84,  82,  74,  90, 145,  66,  56, 140,\n         57, 185,  82, 149, 183, 211, 114, 135,  39, 103,   6,  33, 182,  13,\n        128,  97, 153,  78, 125, 240, 153, 119,   2,  91, 131,   9, 151,  20,\n        224, 205, 237, 207,  56,  51, 191, 137,  58,  61, 225, 220, 243, 210,\n        121, 194,  17, 150,  13,  38,  92, 203,  65, 201, 169,  45, 139,  88,\n         25, 200, 204, 104, 176,   7,  67,   5, 244, 240,  48,  14, 163,  64,\n        164, 136, 151, 152, 148,  92,  63, 141,  52, 109,  67, 197,  12, 188,\n         22,  48,  41, 174,  90,  58,  58,  15, 207, 124, 113,  61, 194, 242,\n        101,  37,   1,  76,  51,  88, 196, 210, 152, 232, 119, 130, 145,  16,\n        238,  16, 134, 166,  61, 111, 177,   4,  65, 224,  85,  82,  79,  39,\n         89, 154, 197, 218, 106, 233, 139, 233,  39, 226, 249, 185,   8, 188,\n        180, 144,   1, 158, 245, 231, 190, 119, 176, 200,  74,  41, 215,  12,\n        159, 179,  66, 216, 240, 226,  64,   7, 183, 176, 126, 152, 246,   7,\n        108,  53, 193, 155,  49, 185,  50,  13, 172,  65,  34, 218, 169,  98,\n         93, 195, 149,  29, 199,  13, 245, 161, 129, 113, 107, 115, 103,  17,\n        140, 126,  27, 163, 203,  60,  41, 212, 171,  39,  14, 175,  99, 208,\n         75,  84,  13,  70, 177,  18,  12, 223,  10,  93,  55,  46,  95, 202,\n        176, 100, 153,  75, 210,  52,  98, 151,  21, 154,  93,  82, 171, 238,\n        180, 209,  89,  37, 244, 206,  41, 195, 232, 227,  49, 127, 154,  48,\n        146, 133,  40,  74, 131, 142,  62, 171, 108, 103, 218, 186,  42, 167,\n         82, 186,   2, 204,   8, 149,  83,   6, 179, 247,  76, 193, 101, 212,\n        220, 121,  71, 120, 115,  21,  45, 133, 162, 136, 176, 131, 161,  71,\n        199,  34,  52, 240, 170, 129, 149,  43,  96, 146,  85,  41, 234, 186,\n        225, 161,  87, 153, 182, 223,  21,   2], device='cuda:0')\nTraining batch 3 at epoch 4 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.1447,  0.4800,  0.4612,  ..., -0.5190,  0.1709,  0.7090],\n        [ 0.1860,  0.0302,  0.2507,  ...,  0.3020, -0.1306, -0.0674],\n        [ 0.8516, -0.4434,  0.3018,  ...,  0.5479,  0.7920,  0.2152],\n        ...,\n        [-0.3518,  0.6567, -0.3083,  ...,  0.4353,  0.0843,  0.4741],\n        [ 0.0811,  0.1278, -0.1193,  ...,  0.2771, -0.0128, -0.0017],\n        [-0.0056,  0.1187,  0.1208,  ...,  0.0178,  0.2216, -0.0319]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([208, 241,   1, 143,  50, 224,  66, 214, 191,  58, 196,  71,  58, 159,\n         89, 192, 117,  44, 201,  81,  98, 129, 118,  39, 230,   5, 210, 135,\n         60, 223, 236, 224,  85, 221, 140, 215, 213, 149, 146,  45, 211, 215,\n        181,  25, 111, 109,  96,  99, 203,  78,  20, 160, 131,  19, 171,  32,\n        181,  18, 199, 218, 159,  83, 239, 148, 166,  48, 133,  31,  53, 224,\n          6, 149, 217,   5,  41, 208,  94, 215,  70, 145, 162,   2,  76, 176,\n         27, 174,  12, 167,  53,  25, 195,  42, 128,  32,  53,   9, 236,   7,\n         92, 146, 113, 187, 202,  42, 235, 132,  39,   9, 226, 142, 144, 159,\n        134, 190,  11,  59,  17,  27,  17,  82,  24,   2,  24, 134, 172,  53,\n         22, 174,   4, 193, 227, 247, 179,  44, 185, 107,  78, 109,  12, 192,\n         81, 239, 123,  52, 130, 241, 222, 204, 144,  12,  87, 214, 170,  11,\n         22,  38, 156, 157, 189,  52, 156, 180, 243, 161, 162, 146,  93,   1,\n        139, 162, 198,  13, 168, 124,  57,  71, 143, 123,  87, 110, 144, 237,\n        168, 147, 226,  52, 135,  44, 232, 104, 223, 176,  70,  37, 206, 153,\n          5,  31, 168,  21, 181, 166,  47, 222,  18, 138,  65, 138,  25, 236,\n         71, 214,  67,   3, 130, 158, 164, 133, 245,   0, 119, 110, 125,  50,\n         51, 234,  61,  63,  99,  36, 173, 135, 225, 247, 109, 245, 248, 242,\n        119,  65,  45,  22, 107,  43, 125, 188,  88,  25,  41,  92,  26, 104,\n         34,  67, 180, 233,  90, 171, 146, 128,  44, 178, 167,  32,   7, 119,\n         69, 143, 230, 115, 100, 183, 165,  57,  58, 105,  62, 241, 235, 206,\n        181,  79, 112,  85,  12, 104,  45, 109,  82, 172,  15, 184,  87, 158,\n        188, 101,  50, 184, 156,   5, 173, 246,  13, 127,  98, 101, 139,  80,\n        230, 228,  38,  37, 116,  12, 143,  89,  19, 235, 194, 189, 244,  85,\n         39, 242, 115, 247, 130, 137, 165, 192,   5,  93, 172, 194, 227, 131,\n        115, 221,  99,  61,  85, 199,  64, 108, 151, 186, 143, 165, 107,  26,\n         93,  52, 184, 224,   1,  83,  36, 126,  43, 103,  19, 194, 206,   9,\n        196, 198, 158,  79,  39,  48,  39,  50,  51,  11,  73, 122,  46,  38,\n        115,  63,  70, 199, 199,  93, 130, 183, 133, 234, 194,  90,  38, 117,\n        160, 114, 225,  23, 148,  83, 175, 191, 184,   3,  87, 115,  51,  15,\n         68, 213, 217, 145,  80,  81, 216, 110, 223,  41,  56,  17,  22, 117,\n         32, 124,   3, 176, 142, 115, 156,  94,  91, 135,  10, 163, 124, 195,\n         57, 218, 185, 236, 206,  16,  89, 131, 148, 157, 186, 131, 196, 101,\n        241, 169,  77,  97,  47,  55, 213,  30, 173, 130,  83,  88,   7, 232,\n         15,  77,  55, 136, 232, 178,  75,  96, 153, 207,  28, 181, 207,  48,\n        103, 193,  21, 174, 231, 123,  49, 225,  85,  21, 221,  85,  93, 166,\n         91, 222, 169,  20, 119, 113, 236,  27, 124, 190, 112, 194, 133, 206,\n        154, 178, 177, 228, 175,  38, 126, 238], device='cuda:0')\nEpoch 4 took 38.668291330337524 seconds to train\nEpoch 4/5, Training loss: 5.5613, Accuracy: 0.0044\nValidation at epoch 4 input shape is: torch.Size([512, 384, 708])\nValidation predictions: tensor([[ 0.3776,  0.1639,  0.2439,  ...,  0.0956,  0.2219,  0.2645],\n        [-0.0316, -0.0418, -0.0519,  ..., -0.0096,  0.0310, -0.0674],\n        [ 0.3501,  0.4412, -0.1566,  ...,  0.2085,  0.2443,  0.1030],\n        ...,\n        [-0.0447, -0.1086, -0.0823,  ..., -0.0325,  0.0357, -0.1124],\n        [-0.0227, -0.0585, -0.0570,  ..., -0.0006, -0.0041, -0.0523],\n        [-0.3135,  0.2401,  0.1190,  ..., -0.0989,  0.1816, -0.2320]],\n       device='cuda:0')\nValidation targets: tensor([  7, 188,  55,  32, 120, 108, 114,  29,  71, 107,  74, 150, 172,  34,\n        172,  31, 200, 124, 106, 108, 191,  66,  88, 139, 129,  43, 128, 232,\n        114, 247,  29, 129,  85,  48, 117, 240,  45, 248,  48,  80,  43, 249,\n        210, 178, 192, 234,  40,  75,   8, 202,  33,  82, 132,  15, 243,  44,\n         81,  56, 101,  64, 129,  95, 160,  80, 183,  41,  38,  25,  31, 175,\n        220, 146, 163,  30, 215, 207,  75, 148, 200, 118, 170,  67,  57,  70,\n        132,  56,  78,  62,  49, 164, 217, 129,  30, 147, 153,   6, 136,  90,\n         98, 168, 223,  91,  41, 236,  64,  66,  16, 216,  17,  44,  35,  93,\n        196, 162,  77, 103, 145, 154,  80,  87, 131, 191,  83, 137, 116,  58,\n        166, 196,  10, 119,  48, 150,  48,  92, 117,  58, 115, 193, 136,  46,\n         58,  33,  45, 150, 199,  38, 216, 152, 238,  97,  75, 178,  94,  88,\n        112,  91, 211,  33, 171, 195, 100,  19,  22,  17, 108,  88,  62, 113,\n         66,  98, 210, 155,   9, 204, 194,  74,  32, 221,  98, 210, 206, 213,\n         75,  84, 193, 172,  85, 164,  14, 214,  83, 130, 176, 116, 154,  64,\n        190,  13, 118, 220, 136,  77,  55, 211, 208, 141, 225, 121,  68, 191,\n         55,  54, 219, 156, 169, 206,   6, 126,   6,   3, 214,  63, 129,  39,\n        151, 104, 144, 213,  84,   2, 137,  87,  23, 107, 219,  60, 176, 144,\n         76,  31,  47, 107, 113, 114, 151, 211,  38, 212, 115,  20, 114, 139,\n        134, 233, 145, 217, 175,  30, 114, 200, 206, 102, 201, 154, 209, 154,\n         63, 237, 104,  75, 217, 135,  29, 133,  57,  86,  68,  82,  98,  47,\n        178, 227,  72, 118,  59,  21,  89,  94,   5, 187, 122,   5,  66, 101,\n        206, 193, 211,  83, 157, 212,  96,  72,  75,  15, 157, 218, 197, 140,\n         45, 129, 165, 196, 172, 171, 245, 160, 245, 125,  36, 101, 104, 123,\n         69, 161, 229,   4, 123, 116,   5, 140,  73,   2,  35,  65,  23,  23,\n        108, 162,  27, 167, 204,  59, 213,  28,  56,  36, 165,  76, 205, 171,\n         34,  57,  61,  29, 235,  12,  49, 120, 200,   7, 118,  29,  35,  99,\n         69, 206, 196,   0, 202,  92,   3, 177, 185,  40, 194,  52, 125, 164,\n        135, 198,  21,  80, 216, 229, 136, 227, 219,  12, 185, 113, 203, 119,\n         22, 136, 155, 232, 140, 162,   1,  40,  75, 147, 178, 154, 137, 170,\n        145, 151,   5,  11, 124,  31,  73, 224,  69,  51,  58, 219,   2, 195,\n        116,  34, 141, 180,  90,  25, 222, 215, 113, 172,  48,  44, 143, 109,\n          3,  89,  80, 201, 190, 115, 137,  38, 202, 108,  96,  78, 155, 144,\n         15, 203, 159,  85,  92, 132,   8,   1, 154, 161, 201,   4,  49, 160,\n        182,  19, 114,  48, 210,  31,  54,  72,  98,   3,  96,  10, 241, 100,\n        136,  83, 191,  11, 231,  30,  51, 233, 133,  24,  91, 157,  34, 184,\n         99, 136, 225, 190, 236, 113,  75,  17, 147, 169, 208, 211,  45,  64,\n        208, 143, 186,  31,  63, 238, 101,  55], device='cuda:0')\nValidation Loss: 5.5473, Accuracy: 0.0059\nOn epoch number: 5 / 5\nTraining batch 0 at epoch 5 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.4553,  0.1241,  0.6621,  ..., -0.2930,  1.1328,  0.3572],\n        [-0.4839, -0.1973,  0.3208,  ...,  0.0055, -0.0019, -0.1094],\n        [ 0.1722, -0.0317,  0.2123,  ...,  0.2013,  0.4619, -0.0674],\n        ...,\n        [ 0.1986,  0.1665, -0.3379,  ..., -0.3357,  0.3894, -0.1119],\n        [ 0.6685,  0.6030, -0.1125,  ...,  0.6816,  0.0902,  0.1804],\n        [ 0.5439,  0.6309, -0.3914,  ...,  0.2676,  0.4929, -0.0666]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([109, 172, 199,  25,  63, 153,   2,   8,  90,   2, 119, 143, 237,  23,\n         99, 171,  63, 213, 173,   1, 188,  54, 197, 132, 231, 209, 143, 203,\n        215,  79,  43, 229,   5,   5, 181,  78,  58,  95,  32, 112,  36,  48,\n        111,   7, 167, 210, 175, 157,  16,  89, 116, 218, 182, 247,  44, 175,\n        241, 223, 105,  11, 140,  39,  14, 146, 212, 233, 147, 171, 203, 185,\n         78, 160, 214,  45,  27,   1, 186, 108,  18, 185, 201, 159, 206, 141,\n        109, 126, 160, 154, 245,  40, 180, 167,  89, 123, 132, 130,  15,  33,\n         71, 181,   5, 190,  65,  32,  33, 117, 187, 109, 191, 166, 144, 221,\n        242, 127, 181, 207, 177, 245,  12, 155,   9,  75, 242, 227, 131, 196,\n        195, 230,  70, 126,  83,  47, 214,  38, 165,  36,  46, 117,  88,   2,\n         93, 164, 180, 231, 154, 208, 139,  15, 133,  92,   4, 175, 215, 195,\n        107, 191,  15,  43, 176, 165,  94, 224, 236, 199,  21, 177, 132,  37,\n        177, 104,   1, 110, 134,  28, 148,  29, 124, 118, 130,  57,   0,  37,\n         23, 205, 149, 135, 112,  42, 103,  54, 207, 145, 107, 185, 105, 119,\n        113,  70, 160, 208, 241, 125,  62, 116,  65, 193, 106,  70, 114, 178,\n        237, 210,  28, 204, 117, 246, 167,  25, 216, 223,  20,  39,  88,  55,\n         49, 110,  32,  30,  51,  44, 246, 243, 224,  16,  98,  41,  48,  79,\n        120,  53, 235, 101,  40,  53, 129, 173,  41,  71, 104,   9,  60,  41,\n         37, 246, 176,  73,  93,  58, 130, 146,  82,  91,  93,  58,  48, 188,\n         47, 193, 161, 132, 232,   6, 133, 168, 189, 212,  41, 211, 174,  70,\n        185, 218, 143,  69, 181, 246, 116,  87, 150,  56, 136,  56, 220, 226,\n         20,  53, 179,  37, 114, 180,  96, 184,  22, 120,  47, 131, 161, 208,\n         60,  61, 243,  31,  77, 140, 152, 185,   4, 111,  70, 180,  87,  45,\n        115, 125, 188,  22,  45,  91,  50, 166,  11, 144, 242, 116,   5, 206,\n        217, 108,  21, 145,   8, 108,  49, 172,   8,  73, 130, 220, 145,  96,\n        136, 130, 103, 143, 197, 168, 184,  19, 112,  97, 226, 134,  89, 163,\n          5, 156,  98, 194, 110, 230, 186, 171, 188, 184, 117, 142, 212, 168,\n        159, 106, 225, 211, 151, 128,  52, 100,  91, 156,  66,   7,  20,  60,\n        144, 191, 212, 228,  92,  17,  64, 238, 147, 150, 105,  98, 121,  48,\n        241, 140, 154, 169,  52,  32, 212,  90,  34,  67, 172, 101, 121,  86,\n         39,  10, 165, 113, 220, 182,  25, 142,  96, 122, 126, 126,  57, 118,\n         53, 170,  82,  38,  69, 183, 199,  80, 123, 130, 159,  27, 218, 149,\n        224,   9,  65, 193, 232,  69,  67, 147,  12, 162,  77,  83, 138,  42,\n        121, 149,  85, 202, 235,  53, 109, 139, 145, 234,  74,  71,  90, 197,\n        182, 124,   2, 241,  15, 221,  14, 182, 197, 219,  50, 193,  27,  21,\n        113,  11, 247, 202, 135,  72,  34,  93, 148,  55, 135, 109, 195, 113,\n        108, 216, 215,  87, 130,  57, 227,  62], device='cuda:0')\nTraining batch 1 at epoch 5 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[-0.1443,  0.0408, -0.2913,  ...,  0.0439, -0.3027,  0.0746],\n        [ 0.3411,  0.0155, -0.1763,  ...,  0.2098,  0.4038, -0.1681],\n        [-0.0490,  0.1085,  0.0855,  ...,  0.0329,  0.1974,  0.1130],\n        ...,\n        [-0.0383,  0.0388,  0.0197,  ...,  0.0471,  0.1637, -0.0826],\n        [-0.0856,  0.0233, -0.2195,  ...,  0.0695,  0.0278,  0.0470],\n        [-0.0567,  0.2186,  0.1211,  ..., -0.1343, -0.1310,  0.2644]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([ 56, 233, 185, 240, 220,  74, 199, 203, 169, 245, 185,  76,  87, 157,\n        229, 202, 189,  15, 217, 158,  49,  39, 199,  37,  25,  75, 190, 119,\n        133,  50, 214, 119, 236,  71,  43, 129,  34, 146, 231,   5,  38, 172,\n         63,   4, 151,  51, 216,  53, 198, 141, 120,  89, 139,   9,  27,  89,\n        176, 225, 244,  55, 108, 163, 198,  51,  28,  87,  65,  88, 154, 194,\n         95,   6,  45, 150,  91, 147,  42, 120, 223, 152,  96, 214,   5, 206,\n        247,  94,  66, 152, 100,  65, 125,  63, 148,  67, 202, 112,  70, 202,\n          1,  98, 155, 218,  82, 199, 228,  76,  10,  24, 227,  16,  91,  14,\n        213,  89, 207, 239,   8,  64,  92, 221,  40,  18,  86,   7, 248, 214,\n        129,  58, 218, 173,  99, 106,  96, 191, 145, 197,  98,  93, 133, 153,\n        238,  33,  24, 137,  52, 159, 109,  33, 214,   9,  18, 162,  25, 206,\n         75, 234,  31, 204, 101,  90,  25,  55, 141,  35, 194, 235, 214,  28,\n         78,  17,  11,  51,  23,  13, 125, 181, 226, 186,  11, 110,  12, 196,\n        169, 115, 207, 166,  48, 210,  17, 131, 240,  96, 199,  70, 215, 204,\n         53, 124,  97, 241, 136,  99, 231, 232, 140, 181, 216, 177,  54,   3,\n         50, 242, 153, 215, 181,  47,  95, 218,  45,  28,  75, 126, 136, 234,\n        219, 212, 115,   9,  47, 190, 104, 170, 171, 135,  85,  72, 138,  10,\n        176, 119, 232, 229,  93,  18, 112, 192, 194, 179, 186, 116,  48, 245,\n         18,  81,  66, 213, 130, 119, 189,  63, 104, 221,  85,  12,  84, 238,\n        194,  87,  16,  45, 197, 198, 235, 166,  93,  56,  44,  39,  52,  27,\n        223, 182,  77, 195,  82,  37, 160,  60,  65, 157,  38,  78,  94,  29,\n        111, 112, 247, 104, 177,  62,  56, 206,  71,  89,  72,  42,  11,   4,\n        237, 140, 227,  43,  21, 232,  19,  53,  99,  39, 105,  71,  18,  75,\n        230,  85, 192,   7, 127,  11,  98,  44,  81,  45,  37, 135, 129, 183,\n         41,  85, 194, 157,  20, 240,  94,  17, 188,  12, 123, 191,   1, 121,\n        100, 242,  71,  47, 141, 176, 226, 194, 175, 222, 163,  29, 244,  32,\n        118, 215, 133, 211,   7,  26, 192, 133, 162, 173,  34, 151, 165, 181,\n        196,  64,  73,   8, 169,   7,  92, 237,  93, 200, 211, 119, 224, 203,\n         71, 131, 199,  64, 216, 195, 104, 174, 233, 202, 136, 136, 121, 186,\n        183, 103,   5, 197,  51, 121,  27, 237,   9,  72,  81,  99, 115, 206,\n         50, 149,  38, 127, 163, 226, 243,  49, 117, 155, 233, 164,   0, 221,\n         99,  13,  38, 202,  53, 156, 103,  90,  46,  54,  47,  88,  81,   5,\n         67, 148,  65, 191, 245, 159,  32,  82,  44,  85, 198,   1, 164,  59,\n        229, 146, 213, 144, 130, 149, 171, 241, 139, 111,  16, 245, 220, 156,\n         33,  94,  36,  11,  12,  92,  61, 167,  85, 203,  90,  65, 239, 177,\n         45,  25, 151,   3,  61,  60, 146,  22,  28, 177,  55,  68, 246, 210,\n         50, 174, 240, 101, 177,  70, 170, 113], device='cuda:0')\nTraining batch 2 at epoch 5 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.0903,  0.3330, -0.2236,  ...,  0.0105,  0.0304,  0.0917],\n        [ 0.2004, -0.0174,  0.1005,  ..., -0.2998,  0.1279, -0.0612],\n        [-0.0523, -0.0944,  0.1357,  ...,  0.3867,  0.1989, -0.1256],\n        ...,\n        [-0.1628,  1.0107, -0.0307,  ...,  0.6040, -0.3975,  0.0030],\n        [-0.1042, -0.0580, -0.0181,  ..., -0.0894,  0.1685, -0.0743],\n        [ 0.6758,  0.5630,  0.3196,  ...,  0.3538,  0.3638,  0.5786]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([210, 135,   5,  91, 174,  82,  39, 203,  13,  88, 161, 115, 192,  89,\n        191,  31,  78,  72, 182,  47,  67, 105,  82, 106, 123, 148, 198,  73,\n        140,  82,  49,  93,  21, 171,   0, 101, 104, 128, 135, 153,  81, 106,\n        153, 135,  34,  99, 199,  75,  60,  61, 146,  16, 240,  31,  22, 166,\n         22,  64, 139,  79,  72, 235, 229,  95,  28,  13, 166,   8, 200, 167,\n         88, 119,  57,  81, 133,  31, 160, 209, 164,  57,  83,  25,  45, 201,\n         21, 107, 131, 114, 194, 199,  43, 152,  78, 110,  68, 176,   6,  36,\n         21, 189,   8,  52,  93, 188, 113, 134, 128, 117,  75, 186,  69,  66,\n         65, 179, 194,  53,  97,  62, 153, 135, 156, 153, 191, 218, 139, 145,\n         56,   0,  54, 146, 225, 172,  91, 231, 124, 117,  78, 192, 158, 181,\n         29,  14,  25,  13,   4,   9, 157,  48,   8,  48,  19,  19, 113, 154,\n        109, 157, 242, 173,  31,  24, 117, 197, 165, 174, 119, 173, 203,  21,\n         25, 176, 210,  29,  64,  97, 196, 218,  38,  26, 225,   0,  57,  76,\n         98, 150, 176,  65,  79, 165, 223,  41, 208, 100,  41, 231, 112, 244,\n        175, 112,  33, 133, 238, 201,  93, 218, 143, 194, 104,  45,   2, 161,\n        131, 160, 208, 183,  80,  12,  38,  49, 102,  93,   1,  76, 163, 180,\n         25,  22, 206, 108,  40,  92,  88,  55,  18,  48,  76,  17, 127, 184,\n         94, 194, 106, 190,  31, 102, 132,  75, 213,  74, 101,  56, 232,  52,\n        115, 218, 131, 121, 223, 103, 230, 162, 232, 192, 164, 153,  50, 151,\n        126,  41, 140, 215, 162, 161,  22,  55,  53, 171,  82, 129,  50, 149,\n         16, 140, 212, 178, 149,   4, 175, 147, 178,  72,  30, 210, 103,   4,\n         14, 182, 245, 176, 249, 234,  85, 162,  33,  21,  38, 153, 148, 226,\n        171,  52,  32, 236, 162,  76,  81, 200, 172, 228, 186,  25,  93, 127,\n         56,  20,  88,  12, 218, 222, 179, 157, 157, 206,  66,  15, 134,  45,\n        236, 232, 193, 139, 128,  39,  36, 193, 145, 204,  67,  94,  52, 154,\n         54,  47, 131, 108, 162, 112, 248, 142,  19,  31, 211, 164, 147, 151,\n        109, 171, 205, 115, 244,  21, 239,  79,  39, 224,  63,  45, 211, 108,\n        158, 168,  52,  66, 236, 244,   9, 194, 130,   2,  85,  76, 162,  85,\n        165, 210,  63, 126,  83, 166, 179, 177, 213, 104, 131,  18,  99,  13,\n         76, 192,  87, 156,  15,  74,  98, 219,  46, 125,  78, 181, 210,  43,\n        127, 146, 167,   5, 167,  85,  15,  75, 105,  30, 239, 145, 177, 132,\n        106,  58, 118,  81, 133,  10, 126, 180, 107, 232, 204,  47, 125, 238,\n        162, 146,   3, 209,  85, 184,  32,  32,  67,  35,  10,  58,  39,  84,\n        179,  62, 105,  39,   6, 183, 101,  50,  49, 156,  22, 141,  45, 158,\n        133, 220,  62, 129, 213,  58, 158, 137,  81, 144,  74,  87,  13,  96,\n         64,  19, 114,  19,  12, 102,  33,  64,   6, 182, 124,  12,  66, 233,\n        103,  53, 145,  38,  25, 146, 159,  24], device='cuda:0')\nTraining batch 3 at epoch 5 input shape is: torch.Size([512, 384, 708])\nTrain predictions: tensor([[ 0.9893,  0.1135, -0.0941,  ...,  0.1660,  0.2411, -0.0853],\n        [-0.2004, -0.0528, -0.0904,  ..., -0.1010,  0.1781,  0.0524],\n        [ 0.7549, -0.1624, -0.3918,  ..., -0.4307, -0.0945,  0.1289],\n        ...,\n        [ 0.3691,  0.1190,  0.2261,  ...,  0.1709,  0.0944,  0.1024],\n        [ 0.5107,  0.3337, -0.0300,  ..., -0.1814,  0.0771, -0.0103],\n        [ 0.1992,  0.0834, -0.2188,  ..., -0.0040,  0.0601, -0.3088]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)\nTrain targets: tensor([ 21, 117, 204, 169, 209, 236, 152,  96,  43, 182, 136, 136, 174, 153,\n        126,  44, 171,  14,  22, 138,  57, 216, 142, 172,  61, 130,   8,  78,\n         70,  16, 191, 189,  65, 163,  10,  84,  64, 215,  53, 104,  99,  44,\n          7, 122, 231, 167, 207, 131,  99, 127, 165, 150, 149, 111, 230,  98,\n        184,  24, 208, 125, 223, 117,  28, 240,  47, 115, 166,  52, 126,  39,\n        208, 239,  28,  91, 205, 191,  30,   2, 124, 172, 180,  58,  42, 194,\n         37,  22, 165,  19, 144, 107,  69,  84,   2, 162, 136, 236, 164, 116,\n         34, 102, 210, 138, 161, 135, 232,  17, 237, 160, 170,  28, 232,   7,\n          5, 156, 230,  48,  40, 134, 181, 238, 228, 215, 189,  74,  68, 115,\n        193, 148, 194, 143, 195, 187, 203, 199,  18,  76, 201, 137, 166,  57,\n        175,  61, 153, 128, 246, 111,   3, 176, 144, 156, 174,  87,  14, 125,\n         22, 139, 222,  98, 211,   2,  32, 173, 160, 131,   5,  40,  36,  17,\n          1,  59,  61, 160,  36, 169, 235, 201,  18, 218,   1,  50,  53, 173,\n        108, 166, 136, 101, 177, 181, 187, 194,  90,  80, 221,  87,  94,  88,\n        242, 115,  92, 139,  70,  75,  31,  70, 245,  87, 124,  13, 126, 188,\n         43, 118,  51, 176,  84, 124, 158, 158,  19, 198, 143,  18, 223, 219,\n        124,  25, 122, 103,  41,  12,  10,  59, 126, 113,  88,  40, 248,  63,\n        117, 108,  93, 234, 104, 206, 109,  49, 134, 100, 109, 147,   7, 161,\n        235, 145, 167,  37,   8,  53, 108,  83,  15, 162,  35,  98,  49,  90,\n         45,  84, 242,  63,  77, 112, 121, 115, 225, 142, 176,  43, 180,  88,\n         89, 123,  26,  93,  86,  41,  17, 193,  51,   2, 159,  55, 114, 131,\n         40, 183,  33, 224, 180,  42, 227, 210, 206,  45, 212, 131, 173,  14,\n        183,  77, 180,  41,  54, 212, 212, 159,  12, 246, 208,  38, 188, 239,\n        234, 217, 172,  44,  43,  15,  95, 162, 234, 125, 115,  94, 188, 241,\n         44,  30,  66, 130,  89, 158, 186, 206,  70, 224, 184,  37, 196,   0,\n         74,  49, 138, 211, 193, 161, 135,  95, 248,  78, 238, 134,  51,  69,\n        210,  57, 156,  22,  94, 169,  13, 169,  21,  98, 229,  43, 161, 148,\n        126,  38,  67,  85, 236,  92, 101, 206,  26,  17, 227, 171, 186,  58,\n         93, 124,  11,  24, 105, 172, 188, 230,  41, 236,  77, 216,  86,  43,\n        137, 196, 136,  92,  39,  44,  46, 233, 118,  45, 120,  35,  22, 241,\n         19,  74,  77, 125, 249,  71, 162, 100,  27, 145, 239, 118,  83,  40,\n        153, 176, 194,  58, 184,   1, 152, 118, 150,  34, 159, 235, 114, 230,\n        233,   6, 112, 142, 169, 173, 101,   4, 235,  85, 149, 144, 246, 213,\n         37, 129, 149,  82, 175, 242, 213,  61, 200, 246, 163, 198, 214, 156,\n        204, 225,  42, 182,  35,  23, 165, 136, 239, 198, 190, 223,  21,   1,\n        186, 142, 153, 116,  27, 110,  87, 131, 125, 158,  11, 238,  85, 237,\n        198, 245, 119,   0, 237, 138, 225,  64], device='cuda:0')\nEpoch 5 took 37.853058099746704 seconds to train\nEpoch 5/5, Training loss: 5.5908, Accuracy: 0.0020\nValidation at epoch 5 input shape is: torch.Size([512, 384, 708])\nValidation predictions: tensor([[-0.0184, -0.0140, -0.0641,  ..., -0.0417,  0.0046, -0.0782],\n        [ 0.0520,  0.0455, -0.1585,  ..., -0.0708, -0.0763, -0.1167],\n        [-0.0617, -0.0397, -0.0573,  ..., -0.0466,  0.0520, -0.0559],\n        ...,\n        [ 0.2781,  0.3889, -0.0687,  ...,  0.1665,  0.2292,  0.1418],\n        [-0.0326, -0.0602, -0.0722,  ..., -0.0533,  0.0355, -0.0827],\n        [ 0.1968, -0.2836, -1.0050,  ..., -0.3108,  1.2839,  0.1663]],\n       device='cuda:0')\nValidation targets: tensor([ 45,  51, 212, 154,  64,  35,  28, 233, 140,  36, 114,  87, 139, 162,\n        212, 175,  24, 220,  95,  99,  70, 117, 178, 113, 164,  48, 193, 185,\n        113,  88,   3,  91,  40, 191,  44, 116,  55,  69, 151,  30, 172, 108,\n         17, 229, 185, 206, 190, 154, 220, 123,  41, 154, 136,  54,  38, 116,\n        134, 131,  91,  98,  82, 144,  68,  65,  44, 166,   1,  77, 133,  58,\n         45, 124, 199, 210,  46,  91,  35,  80, 136,  11, 150,  56, 118, 126,\n        206,  19, 151,  85, 113,  32,  69, 171, 191,   2,  16,  22, 214, 210,\n        169, 188,  51,   7, 108,  58,  98, 102, 129, 206,  19, 129, 143,  31,\n        203, 144, 178,  45, 118, 219, 211, 177,  29,  48,  64,  62, 132,  90,\n        202, 194,  39,  15,  36,  31,   2, 202, 136,  49,  34, 222,  49, 195,\n        160,  13, 113,   5,   7,  89,  66,  59,  68,  88, 155, 201, 204, 130,\n         34,  48,  71, 155, 119, 114, 216, 231, 234, 196,  96, 157, 144, 216,\n        159, 137,  57,   4, 119, 176, 155,  30, 154,  62, 200, 162, 211, 136,\n        198,  35,  78,  75, 114, 219,   1, 219,  75,  34, 227,  82, 235, 135,\n        249, 160, 186, 141,  64,  32,  63,  30, 116, 210, 206, 108, 148, 200,\n         85,   0,  72,   3, 240,  57, 170, 209, 124,  80, 145,  38, 196,  94,\n        151,   9, 113, 129, 241, 147,  31, 107,  31, 118, 207,  45, 107,  78,\n         40, 225,   8, 117, 218, 210,  98,  73,  20, 104,  76,  44, 160, 182,\n        165, 100, 211,  49, 214, 161, 243, 238,  80, 147,  72,  73,  75, 147,\n        236, 219,  58,  58,  27,  76,  90, 225, 211,  55, 227, 167,  74,  80,\n        201,  83, 195, 211, 232,   6,  63, 137, 141,  86,  33, 156, 132,  22,\n        133,  54,  59, 217, 247,  29,  43, 170, 145, 193,  63,  14, 150, 122,\n        224, 200, 245,  75, 136, 172,  47,  83,   6,  21,  12, 203, 187,  52,\n         87,  84, 178, 164, 213, 169,  29,  15,  81,  25, 190,  66,  67,  11,\n        202, 120, 153, 139, 140,  64, 137,   8, 137,  33,  97, 201, 125, 103,\n        248,  17,   2, 101,  75, 215, 213, 154,   5,  25,  29, 216, 152, 163,\n         40, 104, 118,  43, 112, 190, 157,   5, 120,  47,  10,  41,  98, 116,\n        104,  92, 178, 206, 208,  96,  31,  38,  48,  23, 162,  83,  55, 136,\n        236, 221, 191,  83, 129, 229, 196,  92, 193, 176,  96,  12,  80, 171,\n         48, 217, 238, 150, 114, 208,   5,   3, 146, 233, 172, 145, 128, 197,\n         21,  34, 114, 136,  33,  56, 213,  66,  98, 106, 183,  10,  57,  23,\n         75, 168,  69,  92,  75,  93, 115, 172,   3,  74, 180, 171, 109, 204,\n         55,  85, 200, 164, 100, 125,  88, 217, 191,   4,  48,  15, 115,  61,\n        129, 205, 135,  23,  38, 154, 101, 129,  30,  75,  99,  89,  77, 157,\n        237, 184, 161, 143, 172, 114, 192,  66, 115, 208, 101,  94, 132, 121,\n        107,   6, 108,  17, 123, 194,  29, 196,  56, 140, 175, 165, 223, 215,\n         72, 245,  60,  31, 108, 101,  84, 232], device='cuda:0')\nValidation Loss: 5.5572, Accuracy: 0.0020\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-23-1f560d137b67>:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"{CFG.output_dir}/{CFG.comment}-fold{fold}-best-loss.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Final Validation Loss: 5.5285, Accuracy: 0.0039\n\nTraining Results Summary:\nFold 0: Loss = 5.5285, Accuracy = 0.0039\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[(0, 5.528475284576416, 0.00390625)]"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}