{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "*Taken from dataset [Kaggle page](https://www.kaggle.com/competitions/asl-signs/data)*\n",
    "\n",
    "Deaf children are often born to hearing parents who do not know sign language. Your challenge in this competition is to help identify signs made in processed videos, which will support the development of mobile apps to help teach parents sign language so they can communicate with their Deaf children.\n",
    "\n",
    "### Files\n",
    "`train_landmark_files/[participant_id]/\\[sequence_id].parquet` \n",
    "\n",
    "The landmark data. The landmarks were extracted from raw videos with the MediaPipe holistic model. Not all of the frames necessarily had visible hands or hands that could be detected by the model.\n",
    "\n",
    "- frame - The frame number in the raw video.\n",
    "- row_id - A unique identifier for the row.\n",
    "- type - The type of landmark. One of ['face', 'left_hand', 'pose', 'right_hand'].\n",
    "- landmark_index - The landmark index number. Details of the hand landmark locations can be found here.\n",
    "- [x/y/z] - The normalized spatial coordinates of the landmark. These are the only columns that will be provided to your submitted model for inference. The MediaPipe model is not fully trained to predict depth so you may wish to ignore the z values.\n",
    "\n",
    "`train.csv`\n",
    "- path - The path to the landmark file.\n",
    "- participant_id - A unique identifier for the data contributor.\n",
    "- sequence_id - A unique identifier for the landmark sequence.\n",
    "- sign - The label for the landmark sequence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
