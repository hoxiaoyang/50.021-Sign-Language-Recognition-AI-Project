{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"},{"sourceId":5158870,"sourceType":"datasetVersion","datasetId":2997548},{"sourceId":5194802,"sourceType":"datasetVersion","datasetId":3020507},{"sourceId":5594542,"sourceType":"datasetVersion","datasetId":3218684},{"sourceId":340465,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":284722,"modelId":305563},{"sourceId":348216,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":290792,"modelId":311487}],"dockerImageVersionId":30475,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install -q /kaggle/input/tensorflow-2120/tensorflow-2.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q tensorflow-addons==0.20.0\n!pip install -q git+https://github.com/hoyso48/tf-utils@main","metadata":{"id":"IpEQKDrDqAFP","outputId":"2affcd98-7204-4d46-c030-ce8daefe9ad4","execution":{"iopub.status.busy":"2025-04-20T17:51:35.049325Z","iopub.execute_input":"2025-04-20T17:51:35.049675Z","iopub.status.idle":"2025-04-20T17:51:49.352161Z","shell.execute_reply.started":"2025-04-20T17:51:35.049647Z","shell.execute_reply":"2025-04-20T17:51:49.350791Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m^C\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip\", line 5, in <module>\n    from pip._internal.cli.main import main\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 9, in <module>\n    from pip._internal.cli.autocompletion import autocomplete\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/cli/autocompletion.py\", line 12, in <module>\n    from pip._internal.metadata import get_default_environment\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/metadata/__init__.py\", line 3, in <module>\n    from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/metadata/base.py\", line 21, in <module>\n    from pip._vendor.packaging.requirements import Requirement\n  File \"/usr/local/lib/python3.8/site-packages/pip/_vendor/packaging/requirements.py\", line 10, in <module>\n    from pip._vendor.pyparsing import (  # noqa\n  File \"/usr/local/lib/python3.8/site-packages/pip/_vendor/pyparsing/__init__.py\", line 141, in <module>\n    from .helpers import *\n  File \"/usr/local/lib/python3.8/site-packages/pip/_vendor/pyparsing/helpers.py\", line 688, in <module>\n    common_html_entity = Regex(\"&(?P<entity>\" + \"|\".join(_htmlEntityMap) + \");\").set_name(\n  File \"/usr/local/lib/python3.8/site-packages/pip/_vendor/pyparsing/core.py\", line 2933, in __init__\n    self.re = re.compile(self.pattern, self.flags)\n  File \"/usr/local/lib/python3.8/re.py\", line 252, in compile\n    return _compile(pattern, flags)\n  File \"/usr/local/lib/python3.8/re.py\", line 304, in _compile\n    p = sre_compile.compile(pattern, flags)\n  File \"/usr/local/lib/python3.8/sre_compile.py\", line 764, in compile\n    p = sre_parse.parse(p, flags)\n  File \"/usr/local/lib/python3.8/sre_parse.py\", line 948, in parse\n    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n  File \"/usr/local/lib/python3.8/sre_parse.py\", line 443, in _parse_sub\n    itemsappend(_parse(source, state, verbose, nested + 1,\n  File \"/usr/local/lib/python3.8/sre_parse.py\", line 834, in _parse\n    p = _parse_sub(source, state, sub_verbose, nested + 1)\n  File \"/usr/local/lib/python3.8/sre_parse.py\", line 443, in _parse_sub\n    itemsappend(_parse(source, state, verbose, nested + 1,\n  File \"/usr/local/lib/python3.8/sre_parse.py\", line 529, in _parse\n    subpatternappend((LITERAL, _ord(this)))\n  File \"/usr/local/lib/python3.8/sre_parse.py\", line 173, in append\n    self.data.append(code)\nKeyboardInterrupt\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport tensorflow.keras.mixed_precision as mixed_precision\n\nfrom tqdm.autonotebook import tqdm\nimport sklearn\n\nfrom tf_utils.schedules import OneCycleLR, ListedLR\nfrom tf_utils.callbacks import Snapshot, SWA\nfrom tf_utils.learners import FGM, AWP\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport glob\nimport datetime\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')","metadata":{"id":"wD7tqFC_qAFQ","outputId":"426ff58f-01d7-451e-ea99-7b00405d8781","execution":{"iopub.status.busy":"2025-04-20T15:53:51.929831Z","iopub.execute_input":"2025-04-20T15:53:51.930102Z","iopub.status.idle":"2025-04-20T15:54:33.727075Z","shell.execute_reply.started":"2025-04-20T15:53:51.930078Z","shell.execute_reply":"2025-04-20T15:54:33.726226Z"},"trusted":true},"outputs":[{"name":"stderr","text":"D0420 15:54:24.383456641      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0420 15:54:24.383480079      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0420 15:54:24.383483439      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0420 15:54:24.383486146      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0420 15:54:24.383488563      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0420 15:54:24.383506941      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0420 15:54:24.383513359      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0420 15:54:24.383523547      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0420 15:54:24.383526079      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0420 15:54:24.383528421      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0420 15:54:24.383530996      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0420 15:54:24.383533646      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0420 15:54:24.383536039      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0420 15:54:24.383538415      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0420 15:54:24.383714660      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\nD0420 15:54:24.392043250      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD0420 15:54:24.392073666      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0420 15:54:24.392518764      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0420 15:54:24.392527010      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0420 15:54:24.392530886      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0420 15:54:24.392534258      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0420 15:54:24.392537735      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0420 15:54:24.392541057      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0420 15:54:24.392549830      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0420 15:54:24.392574730      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0420 15:54:24.392613064      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0420 15:54:24.392647015      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0420 15:54:24.392651072      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0420 15:54:24.392654224      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0420 15:54:24.392657642      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0420 15:54:24.392660900      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0420 15:54:24.392663869      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0420 15:54:24.392667171      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0420 15:54:24.396500909      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0420 15:54:24.409707565     426 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0420 15:54:24.414904031     426 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2025-04-20T15:54:24.414888534+00:00\"}\n/usr/local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n/tmp/ipykernel_15/1000973416.py:9: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow Version: 2.12.0\nPython Version: 3.8.16 (default, Apr 12 2023, 14:58:47) \n[GCC 10.2.1 20210110]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Seed all random number generators, for consistency\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n# Using TPUs on Kaggle\ndef get_strategy(device='TPU-VM'):\n    if \"TPU\" in device:\n        tpu = 'local' if device=='TPU-VM' else None\n        print(\"connecting to TPU...\")\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        IS_TPU = True\n\n    if device == \"GPU\"  or device==\"CPU\":\n        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n        if ngpu>1:\n            print(\"Using multi GPU\")\n            strategy = tf.distribute.MirroredStrategy()\n        elif ngpu==1:\n            print(\"Using single GPU\")\n            strategy = tf.distribute.get_strategy()\n        else:\n            print(\"Using CPU\")\n            strategy = tf.distribute.get_strategy()\n            CFG.device = \"CPU\"\n\n    if device == \"GPU\":\n        print(\"Num GPUs Available: \", ngpu)\n\n    AUTO     = tf.data.experimental.AUTOTUNE\n    REPLICAS = strategy.num_replicas_in_sync\n    print(f'REPLICAS: {REPLICAS}')\n    \n    return strategy, REPLICAS, IS_TPU\n\nSTRATEGY, N_REPLICAS, IS_TPU = get_strategy()","metadata":{"id":"u74o98JxqAFQ","outputId":"af042ef9-76e7-40a2-9938-e5fd4bb6f495","execution":{"iopub.status.busy":"2025-04-20T15:54:33.728217Z","iopub.execute_input":"2025-04-20T15:54:33.728889Z","iopub.status.idle":"2025-04-20T15:54:42.132331Z","shell.execute_reply.started":"2025-04-20T15:54:33.728862Z","shell.execute_reply":"2025-04-20T15:54:42.131496Z"},"trusted":true},"outputs":[{"name":"stdout","text":"connecting to TPU...\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS: 8\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# For training, validation, and test we will use the randomly split 5-fold tfrecords data.\n# This data is meant to be used for 5-fold cross validation training, but we will only be training for one fold due to time constraints.\nTRAIN_FILENAMES = glob.glob('/kaggle/input/islr-5fold/*.tfrecords')\nprint(len(TRAIN_FILENAMES))","metadata":{"id":"QVDc3vkwqAFQ","outputId":"b84108e5-9e50-4a46-eca8-6a60ecdde99c","execution":{"iopub.status.busy":"2025-04-20T15:56:20.271165Z","iopub.execute_input":"2025-04-20T15:56:20.271527Z","iopub.status.idle":"2025-04-20T15:56:20.277300Z","shell.execute_reply.started":"2025-04-20T15:56:20.271499Z","shell.execute_reply":"2025-04-20T15:56:20.276475Z"},"trusted":true},"outputs":[{"name":"stdout","text":"187\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Train DataFrame, for visualisation\ntrain_df = pd.read_csv('/kaggle/input/asl-signs/train.csv')\ndisplay(train_df.head())\ndisplay(train_df.info())","metadata":{"id":"DtrBI-jwqAFQ","outputId":"3ebb3c2b-8fbe-4552-8a16-d50484823b57","execution":{"iopub.status.busy":"2025-04-20T15:56:36.365351Z","iopub.execute_input":"2025-04-20T15:56:36.365704Z","iopub.status.idle":"2025-04-20T15:56:36.487932Z","shell.execute_reply.started":"2025-04-20T15:56:36.365678Z","shell.execute_reply":"2025-04-20T15:56:36.487172Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                            path  participant_id  sequence_id   \n0  train_landmark_files/26734/1000035562.parquet           26734   1000035562  \\\n1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n\n    sign  \n0   blow  \n1   wait  \n2  cloud  \n3   bird  \n4   owie  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 94477 entries, 0 to 94476\nData columns (total 4 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   path            94477 non-null  object\n 1   participant_id  94477 non-null  int64 \n 2   sequence_id     94477 non-null  int64 \n 3   sign            94477 non-null  object\ndtypes: int64(2), object(2)\nmemory usage: 2.9+ MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import re\n# Allows us to count exact number of items in original csv file, to make sure it corresponds to the tf records in the split dataset used for 5-fold training\n# We have 94477 sequences/samples in total.\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename.split('/')[-1]).group(1)) for filename in filenames]\n    return np.sum(n)\nprint(count_data_items(TRAIN_FILENAMES), len(train_df))\nassert count_data_items(TRAIN_FILENAMES) == len(train_df)","metadata":{"id":"iAc9FKztqAFQ","outputId":"ade0413c-5a17-4a4f-f5a6-b0048ab76ffa","execution":{"iopub.status.busy":"2025-04-20T15:56:41.538536Z","iopub.execute_input":"2025-04-20T15:56:41.539487Z","iopub.status.idle":"2025-04-20T15:56:41.545834Z","shell.execute_reply.started":"2025-04-20T15:56:41.539450Z","shell.execute_reply":"2025-04-20T15:56:41.544918Z"},"trusted":true},"outputs":[{"name":"stdout","text":"94477 94477\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Preprocessing functions - explained in training-ryan.ipynb","metadata":{}},{"cell_type":"code","source":"ROWS_PER_FRAME = 543\nMAX_LEN = 384\nCROP_LEN = MAX_LEN\nNUM_CLASSES  = 250\nPAD = -100.\nNOSE=[\n    1,2,98,327\n]\nLNOSE = [98]\nRNOSE = [327]\nLIP = [ 0, \n    61, 185, 40, 39, 37, 267, 269, 270, 409,\n    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n]\nLLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\nRLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n\nPOSE = [500, 502, 504, 501, 503, 505, 512, 513]\nLPOSE = [513,505,503,501]\nRPOSE = [512,504,502,500]\n\nREYE = [\n    33, 7, 163, 144, 145, 153, 154, 155, 133,\n    246, 161, 160, 159, 158, 157, 173,\n]\nLEYE = [\n    263, 249, 390, 373, 374, 380, 381, 382, 362,\n    466, 388, 387, 386, 385, 384, 398,\n]\n\nLHAND = np.arange(468, 489).tolist()\nRHAND = np.arange(522, 543).tolist()\n\nPOINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n\nNUM_NODES = len(POINT_LANDMARKS)\nCHANNELS = 6*NUM_NODES\n\nprint(NUM_NODES)\nprint(CHANNELS)\n\ndef interp1d_(x, target_len, method='random'):\n    length = tf.shape(x)[1]\n    target_len = tf.maximum(1,target_len)\n    if method == 'random':\n        if tf.random.uniform(()) < 0.33:\n            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n        else:\n            if tf.random.uniform(()) < 0.5:\n                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n            else:\n                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n    else:\n        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n    return x\n\ndef tf_nan_mean(x, axis=0, keepdims=False):\n    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n\ndef tf_nan_std(x, center=None, axis=0, keepdims=False):\n    if center is None:\n        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n    d = x - center\n    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n\nclass Preprocess(tf.keras.layers.Layer):\n    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.point_landmarks = point_landmarks\n\n    def call(self, inputs):\n        if tf.rank(inputs) == 3:\n            x = inputs[None,...]\n        else:\n            x = inputs\n        \n        mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n        mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n        \n        x = (x - mean)/std\n\n        if self.max_len is not None:\n            x = x[:,:self.max_len]\n        length = tf.shape(x)[1]\n        x = x[...,:2]\n\n        dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n\n        dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n\n        x = tf.concat([\n            tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n            tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n            tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n        ], axis = -1)\n        \n        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n        \n        return x","metadata":{"id":"6xyloTyiqAFQ","outputId":"dab46b40-8d61-4eb7-a7a5-4d5e10ab8538","execution":{"iopub.status.busy":"2025-04-20T15:56:44.114223Z","iopub.execute_input":"2025-04-20T15:56:44.114548Z","iopub.status.idle":"2025-04-20T15:56:44.138057Z","shell.execute_reply.started":"2025-04-20T15:56:44.114521Z","shell.execute_reply":"2025-04-20T15:56:44.137263Z"},"trusted":true},"outputs":[{"name":"stdout","text":"118\n708\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def decode_tfrec(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'coordinates': tf.io.FixedLenFeature([], tf.string),\n        'sign': tf.io.FixedLenFeature([], tf.int64),\n    })\n    out = {}\n    out['coordinates']  = tf.reshape(tf.io.decode_raw(features['coordinates'], tf.float32), (-1,ROWS_PER_FRAME,3))\n    out['sign'] = features['sign']\n    return out\n\ndef filter_nans_tf(x, ref_point=POINT_LANDMARKS):\n    mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x,ref_point,axis=1)), axis=[-2,-1]))\n    x = tf.boolean_mask(x, mask, axis=0)\n    return x\n\ndef preprocess(x, augment=False, max_len=MAX_LEN):\n    coord = x['coordinates']\n    coord = filter_nans_tf(coord)\n    if augment:\n        coord = augment_fn(coord, max_len=max_len)\n    coord = tf.ensure_shape(coord, (None,ROWS_PER_FRAME,3))\n    \n    return tf.cast(Preprocess(max_len=max_len)(coord)[0],tf.float32), tf.one_hot(x['sign'], NUM_CLASSES)\n\ndef flip_lr(x):\n    x,y,z = tf.unstack(x, axis=-1)\n    x = 1-x\n    new_x = tf.stack([x,y,z], -1)\n    new_x = tf.transpose(new_x, [1,0,2])\n    lhand = tf.gather(new_x, LHAND, axis=0)\n    rhand = tf.gather(new_x, RHAND, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[...,None], rhand)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[...,None], lhand)\n    llip = tf.gather(new_x, LLIP, axis=0)\n    rlip = tf.gather(new_x, RLIP, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[...,None], rlip)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[...,None], llip)\n    lpose = tf.gather(new_x, LPOSE, axis=0)\n    rpose = tf.gather(new_x, RPOSE, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[...,None], rpose)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[...,None], lpose)\n    leye = tf.gather(new_x, LEYE, axis=0)\n    reye = tf.gather(new_x, REYE, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[...,None], reye)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[...,None], leye)\n    lnose = tf.gather(new_x, LNOSE, axis=0)\n    rnose = tf.gather(new_x, RNOSE, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[...,None], rnose)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[...,None], lnose)\n    new_x = tf.transpose(new_x, [1,0,2])\n    return new_x\n\ndef resample(x, rate=(0.8,1.2)):\n    rate = tf.random.uniform((), rate[0], rate[1])\n    length = tf.shape(x)[0]\n    new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32)\n    new_x = interp1d_(x, new_size)\n    return new_x\n\ndef spatial_random_affine(xyz,\n    scale  = (0.8,1.2),\n    shear = (-0.15,0.15),\n    shift  = (-0.1,0.1),\n    degree = (-30,30),\n):\n    center = tf.constant([0.5,0.5])\n    if scale is not None:\n        scale = tf.random.uniform((),*scale)\n        xyz = scale*xyz\n\n    if shear is not None:\n        xy = xyz[...,:2]\n        z = xyz[...,2:]\n        shear_x = shear_y = tf.random.uniform((),*shear)\n        if tf.random.uniform(()) < 0.5:\n            shear_x = 0.\n        else:\n            shear_y = 0.\n        shear_mat = tf.identity([\n            [1.,shear_x],\n            [shear_y,1.]\n        ])\n        xy = xy @ shear_mat\n        center = center + [shear_y, shear_x]\n        xyz = tf.concat([xy,z], axis=-1)\n\n    if degree is not None:\n        xy = xyz[...,:2]\n        z = xyz[...,2:]\n        xy -= center\n        degree = tf.random.uniform((),*degree)\n        radian = degree/180*np.pi\n        c = tf.math.cos(radian)\n        s = tf.math.sin(radian)\n        rotate_mat = tf.identity([\n            [c,s],\n            [-s, c],\n        ])\n        xy = xy @ rotate_mat\n        xy = xy + center\n        xyz = tf.concat([xy,z], axis=-1)\n\n    if shift is not None:\n        shift = tf.random.uniform((),*shift)\n        xyz = xyz + shift\n\n    return xyz\n\ndef temporal_crop(x, length=MAX_LEN):\n    l = tf.shape(x)[0]\n    offset = tf.random.uniform((), 0, tf.clip_by_value(l-length,1,length), dtype=tf.int32)\n    x = x[offset:offset+length]\n    return x\n\ndef temporal_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n    l = tf.shape(x)[0]\n    mask_size = tf.random.uniform((), *size)\n    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_size, tf.int32)\n    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l-mask_size,1,l), dtype=tf.int32)\n    x = tf.tensor_scatter_nd_update(x,tf.range(mask_offset, mask_offset+mask_size)[...,None],tf.fill([mask_size,543,3],mask_value))\n    return x\n\ndef spatial_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n    mask_offset_y = tf.random.uniform(())\n    mask_offset_x = tf.random.uniform(())\n    mask_size = tf.random.uniform((), *size)\n    mask_x = (mask_offset_x<x[...,0]) & (x[...,0] < mask_offset_x + mask_size)\n    mask_y = (mask_offset_y<x[...,1]) & (x[...,1] < mask_offset_y + mask_size)\n    mask = mask_x & mask_y\n    x = tf.where(mask[...,None], mask_value, x)\n    return x\n\ndef augment_fn(x, always=False, max_len=None):\n    print(\"Preprocessing...\")\n    if tf.random.uniform(())<0.8 or always:\n        print(\"Resample applied\")\n        x = resample(x, (0.5,1.5))\n    if tf.random.uniform(())<0.5 or always:\n        print(\"Flip applied\")\n        x = flip_lr(x)\n    if max_len is not None:\n        print(\"Temporal crop applied\")\n        x = temporal_crop(x, max_len)\n    if tf.random.uniform(())<0.75 or always:\n        print(\"Spatial random affine applied\")\n        x = spatial_random_affine(x)\n    if tf.random.uniform(())<0.5 or always:\n        print(\"Temporal mask applied\")\n        x = temporal_mask(x)\n    if tf.random.uniform(())<0.5 or always:\n        print(\"Spatial mask applied\")\n        x = spatial_mask(x)\n    return x\n\ndef get_tfrec_dataset(tfrecords, batch_size=64, max_len=64, drop_remainder=False, augment=False, shuffle=False, repeat=False):\n    # Initialize dataset with TFRecords\n    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n    # Decodes serialized TFRecord entries (e.g., converting binary to usable features).\n    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n    # Applies above preprocessing logic\n    ds = ds.map(lambda x: preprocess(x, augment=augment, max_len=max_len), tf.data.AUTOTUNE)\n\n    # Enables epoch-wise repetition\n    if repeat: \n        ds = ds.repeat()\n\n    # Enables epoch-wise shuffling\n    if shuffle:\n        ds = ds.shuffle(shuffle)\n        options = tf.data.Options()\n        options.experimental_deterministic = (False)\n        ds = ds.with_options(options)\n\n    # Batches and pads variable-length data to a uniform shape: [batch_size, max_len, CHANNELS]\n    if batch_size:\n        ds = ds.padded_batch(batch_size, padding_values=PAD, padded_shapes=([max_len,CHANNELS],[NUM_CLASSES]), drop_remainder=drop_remainder)\n\n    # Enables background data loading to optimize training speed\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n        \n    return ds\n\n# Checking to see if dataloader is able to iterate over dataset without error\nds = get_tfrec_dataset(TRAIN_FILENAMES, augment=False, batch_size=1024)\nfor x in ds:\n    temp_train = x\n    break","metadata":{"id":"r17ZnZaGqAFQ","execution":{"iopub.status.busy":"2025-04-20T15:56:49.838770Z","iopub.execute_input":"2025-04-20T15:56:49.839582Z","iopub.status.idle":"2025-04-20T15:56:51.095523Z","shell.execute_reply.started":"2025-04-20T15:56:49.839551Z","shell.execute_reply":"2025-04-20T15:56:51.094403Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Model\n\nOur best-performing model includes the following layers:\n\n**ECA**: introduces lightweight attention across channels to help the model focus on more informative features. It first compresses temporal information using global average pooling, then applies a 1D convolution to model interactions between channels. After a sigmoid activation, the resulting weights are used to scale the original input features channel-wise, enhancing the most relevant parts of the signal.\n\n**LateDropout** delays the application of dropout until a specified number of training steps have passed. This helps the model stabilize in early training before introducing regularization. Once the step threshold is reached, it applies standard dropout during training, reducing overfitting by randomly zeroing out portions of the input.\n\n**CausalDWConv1D**: performs causal, depthwise 1D convolution with dilation, meaning each output at time t only depends on inputs at time t and earlier (important for autoregressive models). It applies zero-padding to maintain sequence length and uses a depthwise convolution to filter each input channel separately, preserving temporal structure while being computationally efficient.\n\n**Conv1DBlock**: a modular block designed for efficient 1D sequence modeling. It first expands the feature dimension, applies a depthwise convolution via CausalDWConv1D, followed by batch normalization and channel attention using ECA. It then projects the features back to the target size and includes residual connections if input and output dimensions match. Dropout is applied optionally for regularization. This block combines efficiency with representational power, making it ideal for deep temporal models.\n\n**MultiHeadSelfAttention**: implements multi-head self-attention, a key component of transformers. It learns to focus on different parts of the input sequence by projecting the input into query (Q), key (K), and value (V) vectors using a single dense layer. These are split into multiple attention heads to capture diverse relationships. Attention scores are computed using scaled dot-product, optionally masked, normalized with softmax, and applied to the value vectors. Finally, outputs from all heads are combined and projected back to the original dimension. Dropout is used to regularize the attention weights.\n\n**TransformerBlock**: wraps multi-head self-attention and a feed-forward network with normalization, residual connections, and dropout for stability and regularization. It first normalizes inputs and applies attention, adds a residual connection, then feeds the output into a two-layer feed-forward network that expands and reduces the feature dimension. Another residual connection ensures better gradient flow. This structure allows the model to learn both local and global temporal patterns effectively while staying robust during training.\n\nOur other models (1DCNN only, Transformers only, 1DCNN + LSTM, 1DCNN + GRU, 1DCNN + LinTransformers) are also available to be chosen for training, just change the model instantiated to the one that you want. Note that our LSTM and GRU layers are not supported with ","metadata":{}},{"cell_type":"markdown","source":"Below are all the definitions for model layers that we will use in our different types of models. This includes the ECA layer, LateDropout layer, CausalDWConv1D layer, Conv1DBlock layer,MultiHeadSelfAttention layer,TransformerBlock layer, LSTM layer, GRU layer, LinSelfAttention, LinTransformerBlock.","metadata":{}},{"cell_type":"code","source":"class ECA(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=5, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.kernel_size = kernel_size\n        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n\n    def call(self, inputs, mask=None):\n        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n        nn = tf.expand_dims(nn, -1)\n        nn = self.conv(nn)\n        nn = tf.squeeze(nn, -1)\n        nn = tf.nn.sigmoid(nn)\n        nn = nn[:,None,:]\n        return inputs * nn\n\nclass LateDropout(tf.keras.layers.Layer):\n    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.rate = rate\n        self.start_step = start_step\n        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n      \n    def build(self, input_shape):\n        super().build(input_shape)\n        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n\n    def call(self, inputs, training=False):\n        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n        if training:\n            self._train_counter.assign_add(1)\n        return x\n\nclass CausalDWConv1D(tf.keras.layers.Layer):\n    def __init__(self, \n        kernel_size=17,\n        dilation_rate=1,\n        use_bias=False,\n        depthwise_initializer='glorot_uniform',\n        name='', **kwargs):\n        super().__init__(name=name,**kwargs)\n        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n                            kernel_size,\n                            strides=1,\n                            dilation_rate=dilation_rate,\n                            padding='valid',\n                            use_bias=use_bias,\n                            depthwise_initializer=depthwise_initializer,\n                            name=name + '_dwconv')\n        self.supports_masking = True\n        \n    def call(self, inputs):\n        x = self.causal_pad(inputs)\n        x = self.dw_conv(x)\n        return x\n\ndef Conv1DBlock(channel_size,\n          kernel_size,\n          dilation_rate=1,\n          drop_rate=0.0,\n          expand_ratio=2,\n          se_ratio=0.25,\n          activation='swish',\n          name=None):\n    '''\n    efficient conv1d block, @hoyso48\n    '''\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n    # Expansion phase\n    def apply(inputs):\n        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n        channels_expand = channels_in * expand_ratio\n\n        skip = inputs\n\n        x = tf.keras.layers.Dense(\n            channels_expand,\n            use_bias=True,\n            activation=activation,\n            name=name + '_expand_conv')(inputs)\n\n        # Depthwise Convolution\n        x = CausalDWConv1D(kernel_size,\n            dilation_rate=dilation_rate,\n            use_bias=False,\n            name=name + '_dwconv')(x)\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n\n        x  = ECA()(x)\n\n        x = tf.keras.layers.Dense(\n            channel_size,\n            use_bias=True,\n            name=name + '_project_conv')(x)\n\n        if drop_rate > 0:\n            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n\n        if (channels_in == channel_size):\n            x = tf.keras.layers.add([x, skip], name=name + '_add')\n        return x\n\n    return apply","metadata":{"id":"zaIHc89AqAFQ","execution":{"iopub.status.busy":"2025-04-20T15:56:54.975687Z","iopub.execute_input":"2025-04-20T15:56:54.976620Z","iopub.status.idle":"2025-04-20T15:56:54.994149Z","shell.execute_reply.started":"2025-04-20T15:56:54.976581Z","shell.execute_reply":"2025-04-20T15:56:54.993303Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class MultiHeadSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.num_heads = num_heads\n        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        qkv = self.qkv(inputs)\n        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n\n        if mask is not None:\n            mask = mask[:, None, None, :]\n\n        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n        attn = self.drop1(attn)\n\n        x = attn @ v\n        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n        x = self.proj(x)\n        return x\n\n\ndef TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n    def apply(inputs):\n        x = inputs\n        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n        x = tf.keras.layers.Add()([inputs, x])\n        attn_out = x\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n        x = tf.keras.layers.Add()([attn_out, x])\n        return x\n    return apply","metadata":{"id":"8hPmJX0YqAFR","execution":{"iopub.status.busy":"2025-04-20T15:56:58.777893Z","iopub.execute_input":"2025-04-20T15:56:58.778303Z","iopub.status.idle":"2025-04-20T15:56:58.791346Z","shell.execute_reply.started":"2025-04-20T15:56:58.778272Z","shell.execute_reply":"2025-04-20T15:56:58.790500Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class LSTM(tf.keras.layers.Layer):\n    def __init__(self, units, bidirectional=True, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.bidirectional = bidirectional\n        self.supports_masking = True\n\n        if bidirectional:\n            self.lstm = tf.keras.layers.Bidirectional(\n                tf.keras.layers.LSTM(units, return_sequences=True, dtype='float32')\n            )\n        else:\n            self.lstm = tf.keras.layers.LSTM(units, return_sequences=True, dtype='float32')\n\n    def call(self, inputs, mask=None):\n        inputs = tf.cast(inputs, tf.float32)  # Cast to float32 for compatibility\n        return self.lstm(inputs, mask=mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:57:00.920802Z","iopub.execute_input":"2025-04-20T15:57:00.921151Z","iopub.status.idle":"2025-04-20T15:57:00.927350Z","shell.execute_reply.started":"2025-04-20T15:57:00.921124Z","shell.execute_reply":"2025-04-20T15:57:00.926500Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class GRU(tf.keras.layers.Layer):\n    def __init__(self, units, bidirectional=True, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.bidirectional = bidirectional\n        self.supports_masking = True\n\n        if bidirectional:\n            self.gru = tf.keras.layers.Bidirectional(\n                tf.keras.layers.GRU(units, return_sequences=True, dtype='float32')\n            )\n        else:\n            self.gru = tf.keras.layers.GRU(units, return_sequences=True, dtype='float32')\n\n    def call(self, inputs, mask=None):\n        inputs = tf.cast(inputs, tf.float32)   # Cast to float32 for compatibility\n        return self.gru(inputs, mask=mask) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:57:01.265246Z","iopub.execute_input":"2025-04-20T15:57:01.265543Z","iopub.status.idle":"2025-04-20T15:57:01.271774Z","shell.execute_reply.started":"2025-04-20T15:57:01.265521Z","shell.execute_reply":"2025-04-20T15:57:01.271044Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class LinformerSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, num_heads=4, dropout=0, k=128, **kwargs):\n        \"\"\"\n        Linformer Self-Attention with linear complexity O(n)\n        \n        Args:\n            dim: dimension of the input\n            num_heads: number of attention heads\n            dropout: dropout rate\n            k: the lower dimension to project keys and values (smaller k means more efficiency)\n        \"\"\"\n        super().__init__(**kwargs)\n        \n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.k = k  # projection dimension for linear attention\n        \n        # Regular projections for queries, keys, values\n        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n        self.supports_masking = True\n    \n    def build(self, input_shape):\n        seq_len = input_shape[1]\n        # Set the projection matrices dimensions\n        self.E_k = self.add_weight(\n            shape=(self.num_heads, self.k, seq_len),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n            name=\"projection_k\"\n        )\n        self.E_v = self.add_weight(\n            shape=(self.num_heads, self.k, seq_len),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n            name=\"projection_v\"\n        )\n        super().build(input_shape)\n    \n    def call(self, inputs, mask=None):\n        # Get the input data type for consistency\n        input_dtype = inputs.dtype\n        \n        batch_size = tf.shape(inputs)[0]\n        seq_len = tf.shape(inputs)[1]\n        \n        # Get query, key, value projections [batch, seq_len, dim]\n        qkv = self.qkv(inputs)\n        # Reshape to [batch, seq_len, num_heads, 3 * head_dim]\n        qkv = tf.reshape(qkv, (batch_size, seq_len, self.num_heads, 3 * self.head_dim))\n        # Transpose to [batch, num_heads, seq_len, 3 * head_dim]\n        qkv = tf.transpose(qkv, [0, 2, 1, 3])\n        \n        # Split into q, k, v\n        q, k, v = tf.split(qkv, [self.head_dim, self.head_dim, self.head_dim], axis=-1)\n        \n        # Linear projection of keys: [batch, num_heads, k, head_dim]\n        k_projected = tf.einsum('bhnd,hkn->bhkd', k, tf.cast(self.E_k, input_dtype))\n        \n        # Linear projection of values: [batch, num_heads, k, head_dim]\n        v_projected = tf.einsum('bhnd,hkn->bhkd', v, tf.cast(self.E_v, input_dtype))\n        \n        # Compute attention scores with the projected keys [batch, num_heads, seq_len, k]\n        attn = tf.matmul(q, k_projected, transpose_b=True) * self.scale\n        \n        # Apply mask if provided\n        if mask is not None:\n            # Expand mask for broadcasting\n            mask = mask[:, None, :, None]  # [batch, 1, seq_len, 1]\n            # Cast mask to the same data type as the input\n            mask = tf.cast(mask, input_dtype)\n            adder = (1.0 - mask) * tf.cast(-1e9, input_dtype)\n            attn = attn + adder\n        \n        # Apply softmax and dropout\n        attn = tf.nn.softmax(attn, axis=-1)\n        attn = self.drop1(attn)\n        \n        # Apply attention weights to the projected values [batch, num_heads, seq_len, head_dim]\n        x = tf.matmul(attn, v_projected)\n        \n        # Reshape back to original dimensions [batch, seq_len, dim]\n        x = tf.transpose(x, [0, 2, 1, 3])\n        x = tf.reshape(x, (batch_size, seq_len, self.dim))\n        \n        # Final projection\n        x = self.proj(x)\n        return x\n\n\ndef LinTransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish', k=128):\n    def apply(inputs):\n        x = inputs\n        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n        x = LinformerSelfAttention(dim=dim, num_heads=num_heads, k=k, dropout=attn_dropout)(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n        x = tf.keras.layers.Add()([inputs, x])\n        attn_out = x\n        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n        x = tf.keras.layers.Add()([attn_out, x])\n        return x\n    return apply","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:17:22.291579Z","iopub.execute_input":"2025-04-20T18:17:22.292324Z","iopub.status.idle":"2025-04-20T18:17:22.311613Z","shell.execute_reply.started":"2025-04-20T18:17:22.292290Z","shell.execute_reply":"2025-04-20T18:17:22.310720Z"}},"outputs":[],"execution_count":124},{"cell_type":"markdown","source":"Below are all the model definitions for different types of models.","metadata":{}},{"cell_type":"code","source":"def cnn_transformers_model(max_len=64, dropout_step=0, dim=192):\n    inp = tf.keras.Input((max_len,CHANNELS))\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp)\n    ksize = 17\n    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = TransformerBlock(dim,expand=2)(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = TransformerBlock(dim,expand=2)(x)\n\n    if dim == 384: #for the 4x sized model\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = TransformerBlock(dim,expand=2)(x)\n\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = TransformerBlock(dim,expand=2)(x)\n\n    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = LateDropout(0.8, start_step=dropout_step)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n    return tf.keras.Model(inp, x)\n\ndef cnn_lstm_model(max_len=64, dropout_step=0, dim=192):\n    inp = tf.keras.Input((max_len,CHANNELS))\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp)\n    ksize = 17\n    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = LSTM(dim)(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = LSTM(dim)(x)\n\n    if dim == 384: #for the 4x sized model\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = LSTM(dim)(x)\n\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = LSTM(dim)(x)\n\n    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = LateDropout(0.8, start_step=dropout_step)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n    return tf.keras.Model(inp, x)\n\ndef cnn_gru_model(max_len=64, dropout_step=0, dim=192):\n    inp = tf.keras.Input((max_len,CHANNELS))\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp)\n    ksize = 17\n    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = GRU(dim)(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = GRU(dim)(x)\n\n    if dim == 384: #for the 4x sized model\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = GRU(dim)(x)\n\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = GRU(dim)(x)\n\n    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = LateDropout(0.8, start_step=dropout_step)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n    return tf.keras.Model(inp, x)\n\ndef cnn_model(max_len=64, dropout_step=0, dim=192):\n    inp = tf.keras.Input((max_len,CHANNELS))\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp)\n    ksize = 17\n    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n\n    if dim == 384: #for the 4x sized model\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n\n    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = LateDropout(0.8, start_step=dropout_step)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n    return tf.keras.Model(inp, x)\n\ndef transformers_model(max_len=64, dropout_step=0, dim=192):\n    inp = tf.keras.Input((max_len,CHANNELS))\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp)\n    ksize = 17\n    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n\n    x = TransformerBlock(dim,expand=2)(x)\n    x = TransformerBlock(dim,expand=2)(x)\n\n    if dim == 384: #for the 4x sized model\n        x = TransformerBlock(dim,expand=2)(x)\n        x = TransformerBlock(dim,expand=2)(x)\n\n    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = LateDropout(0.8, start_step=dropout_step)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n    return tf.keras.Model(inp, x)\n\ndef cnn_lintransformers_model(max_len=64, dropout_step=0, dim=192):\n    inp = tf.keras.Input((max_len,CHANNELS))\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp)\n    ksize = 17\n    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = LinTransformerBlock(dim,expand=2)(x)\n\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n    x = LinTransformerBlock(dim,expand=2)(x)\n\n    if dim == 384: #for the 4x sized model\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = LinTransformerBlock(dim,expand=2)(x)\n\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n        x = LinTransformerBlock(dim,expand=2)(x)\n\n    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = LateDropout(0.8, start_step=dropout_step)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n    return tf.keras.Model(inp, x)","metadata":{"id":"KIooIcnSqAFR","outputId":"4d6a1f52-1630-45d6-a33c-441aab02d1b2","execution":{"iopub.status.busy":"2025-04-20T18:10:05.147383Z","iopub.execute_input":"2025-04-20T18:10:05.147695Z","iopub.status.idle":"2025-04-20T18:10:05.187536Z","shell.execute_reply.started":"2025-04-20T18:10:05.147670Z","shell.execute_reply":"2025-04-20T18:10:05.186689Z"},"trusted":true},"outputs":[],"execution_count":119},{"cell_type":"code","source":"def get_model(model_int=1, max_len=64, dropout_step=0, dim=192):\n    if model_int == 1:\n        return cnn_transformers_model(max_len=max_len, dropout_step=dropout_step, dim=dim)\n    elif model_int == 2:\n        return cnn_lstm_model(max_len=max_len, dropout_step=dropout_step, dim=dim)\n    elif model_int == 3:\n        return cnn_gru_model(max_len=max_len, dropout_step=dropout_step, dim=dim)\n    elif model_int == 4:\n        return cnn_model(max_len=max_len, dropout_step=dropout_step, dim=dim)\n    elif model_int == 5:\n        return transformers_model(max_len=max_len, dropout_step=dropout_step, dim=dim)\n    elif model_int == 6:\n        return cnn_lintransformers_model(max_len=max_len, dropout_step=dropout_step, dim=dim)\n    else:\n        return cnn_transformers_model(max_len=max_len, dropout_step=dropout_step, dim=dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:10:07.730099Z","iopub.execute_input":"2025-04-20T18:10:07.730420Z","iopub.status.idle":"2025-04-20T18:10:07.737431Z","shell.execute_reply.started":"2025-04-20T18:10:07.730396Z","shell.execute_reply":"2025-04-20T18:10:07.736522Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"# You can change the arguments to get_model() to change the type of model\n# 1: cnn_transformers_model\n# 2: cnn_lstm_model\n# 3: cnn_gru_model\n# 4: cnn_model\n# 5: transformers_model\n# 6: cnn_lintransformers_model\n# Default is cnn_transformers_model\n\nmodel = get_model(model_int=6)\n\n# Check supports_masking. This tells model to ignore the masked timesteps during computations like attention or state updates.\nfor x in model.layers:\n    if not x.supports_masking:\n        print(x.supports_masking, x.name)","metadata":{"id":"Nui7lZmUqAFR","execution":{"iopub.status.busy":"2025-04-20T18:17:26.230609Z","iopub.execute_input":"2025-04-20T18:17:26.231459Z","iopub.status.idle":"2025-04-20T18:17:27.483736Z","shell.execute_reply.started":"2025-04-20T18:17:26.231427Z","shell.execute_reply":"2025-04-20T18:17:27.482585Z"},"trusted":true},"outputs":[],"execution_count":125},{"cell_type":"markdown","source":"# Training\n\nOur model is able to be trained in folds. Due to compute limitations on Kaggle, we will only be doing one fold over 120 epochs.\n\n**Data split**: We used a downloaded pre-split 5-fold data split, but split the validation set equally into a validation, and a test set. There are 140 files in the train set, 23 files in the validation set, and 24 files in the test set. There are altogether 94477 instances of sequences grouped in all the pre-split 5-fold data files.\n\n**Evaluation metrics used**: Categorical accuracy, Top-5 accuracy, Micro F1 and Macro F1 scores.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef train_fold(CFG, fold, train_files, valid_files=None, test_files=None, model_int=1, strategy=STRATEGY, summary=True):\n    seed_everything(CFG.seed)\n    tf.keras.backend.clear_session()\n    gc.collect()\n    tf.config.optimizer.set_jit(True)\n        \n    if CFG.fp16:\n        try:\n            policy = mixed_precision.Policy('mixed_bfloat16')\n            mixed_precision.set_global_policy(policy)\n        except:\n            policy = mixed_precision.Policy('mixed_float16')\n            mixed_precision.set_global_policy(policy)\n    else:\n        policy = mixed_precision.Policy('float32')\n        mixed_precision.set_global_policy(policy)\n\n    if fold != 'all':\n        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.batch_size, max_len=CFG.max_len, drop_remainder=True, augment=True, repeat=True, shuffle=32768)\n        valid_ds = get_tfrec_dataset(valid_files, batch_size=CFG.batch_size, max_len=CFG.max_len, drop_remainder=False, repeat=False, shuffle=False)\n        test_ds = get_tfrec_dataset(test_files, batch_size=CFG.batch_size, max_len=CFG.max_len, drop_remainder=False, repeat=False, shuffle=False)\n    else:\n        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.batch_size, max_len=CFG.max_len, drop_remainder=False, augment=True, repeat=True, shuffle=32768)\n        valid_ds = None\n        test_ds = None\n        valid_files = []\n        test_files = []\n    \n    num_train = count_data_items(train_files)\n    num_valid = count_data_items(valid_files) if valid_files else 0\n    num_test = count_data_items(test_files) if test_files else 0\n    steps_per_epoch = num_train//CFG.batch_size\n    \n    with strategy.scope():\n        dropout_step = CFG.dropout_start_epoch * steps_per_epoch\n        model = get_model(model_int, max_len=CFG.max_len, dropout_step=dropout_step, dim=CFG.dim)\n\n        schedule = OneCycleLR(CFG.lr, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=CFG.resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min, decay_type=CFG.decay_type, warmup_type='linear')\n        decay_schedule = OneCycleLR(CFG.lr*CFG.weight_decay, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=CFG.resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min*CFG.weight_decay, decay_type=CFG.decay_type, warmup_type='linear')\n                \n        awp_step = CFG.awp_start_epoch * steps_per_epoch\n        if CFG.fgm:\n            model = FGM(model.input, model.output, delta=CFG.awp_lambda, eps=0., start_step=awp_step)\n        elif CFG.awp:\n            model = AWP(model.input, model.output, delta=CFG.awp_lambda, eps=0., start_step=awp_step)\n\n        opt = tfa.optimizers.RectifiedAdam(learning_rate=schedule, weight_decay=decay_schedule, sma_threshold=4)#, clipvalue=1.)\n        opt = tfa.optimizers.Lookahead(opt,sync_period=5)\n\n        model.compile(\n            optimizer=opt,\n            loss=[tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)], #[tf.keras.losses.CategoricalCrossentropy(from_logits=True)],\n            metrics=[\n                [\n                tf.keras.metrics.CategoricalAccuracy(),\n                tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')\n                ],\n            ],\n            steps_per_execution=steps_per_epoch,\n        )\n    \n    if summary:\n        print()\n        model.summary()\n        print()\n        print(f\"Train dataset: {train_ds}\")\n        print(f\"Validation dataset: {valid_ds}\")\n        print(f\"Test dataset: {test_ds}\")\n        print()\n        schedule.plot()\n        print()\n        init=False\n    \n    print(f'---------fold{fold}---------')\n    print(f'train:{num_train} valid:{num_valid} test:{num_test}')\n    print()\n    \n    if CFG.resume:\n        print(f'resume from epoch{CFG.resume}')\n        model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n        if train_ds is not None:\n            model.evaluate(train_ds.take(steps_per_epoch))\n        if valid_ds is not None:\n            model.evaluate(valid_ds)\n\n    logger = tf.keras.callbacks.CSVLogger(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')\n    sv_loss = tf.keras.callbacks.ModelCheckpoint(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-best.h5', monitor='val_loss', verbose=0, save_best_only=True,\n                save_weights_only=True, mode='min', save_freq='epoch')\n    snap = Snapshot(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', CFG.snapshot_epochs)\n    swa = SWA(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', CFG.swa_epochs, strategy=strategy, train_ds=train_ds, valid_ds=valid_ds, valid_steps=-(num_valid//-CFG.batch_size))\n    callbacks = []\n\n    if CFG.save_output:\n        callbacks.append(logger)\n        callbacks.append(snap)\n        callbacks.append(swa)\n        if fold != 'all':\n            callbacks.append(sv_loss)\n        \n    history = model.fit(\n        train_ds,\n        epochs=CFG.epoch-CFG.resume,\n        steps_per_epoch=steps_per_epoch,\n        callbacks=callbacks,\n        validation_data=valid_ds,\n        verbose=CFG.verbose,\n        validation_steps=-(num_valid//-CFG.batch_size) if num_valid > 0 else None\n    )\n\n    if CFG.save_output:\n        try:\n            model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-best.h5')\n        except:\n            pass\n    \n    # Evaluate on test set if available\n    if fold != 'all' and test_ds is not None:\n        print(\"\\n----- Evaluating on test set -----\")\n        # Standard evaluation with built-in metrics\n        test_metrics = model.evaluate(test_ds, verbose=CFG.verbose, steps=-(num_test//-CFG.batch_size))\n        test_results = dict(zip(model.metrics_names, test_metrics))\n        \n        # Collect predictions for F1 scores\n        y_pred = []\n        y_true = []\n        test_steps = 0\n        \n        for x, y in test_ds:\n            pred = model.predict(x, verbose=0)\n            if isinstance(pred, list):\n                pred = pred[0]\n            \n            batch_pred = np.argmax(pred, axis=1)\n            batch_true = np.argmax(y.numpy(), axis=1)\n            \n            y_pred.append(batch_pred)\n            y_true.append(batch_true)\n            \n            test_steps += 1\n            if test_steps >= -(num_test//-CFG.batch_size):\n                break\n                \n        y_pred = np.concatenate(y_pred)\n        y_true = np.concatenate(y_true)\n        \n        # Calculate F1 scores\n        macro_f1 = f1_score(y_true, y_pred, average='macro')\n        micro_f1 = f1_score(y_true, y_pred, average='micro')\n        \n        # Add F1 scores to results dictionary\n        test_results['macro_f1_score'] = macro_f1\n        test_results['micro_f1_score'] = micro_f1\n        \n        # Print all test metrics\n        print(\"\\n----- Test Results -----\")\n        print(f\"Loss:                 {test_results['loss']:.4f}\")\n        print(f\"Categorical Accuracy:  {test_results['categorical_accuracy']:.4f}\")\n        print(f\"Top-5 Accuracy:        {test_results['top5_acc']:.4f}\")\n        print(f\"Macro F1 Score:        {macro_f1:.4f}\")\n        print(f\"Micro F1 Score:        {micro_f1:.4f}\")\n        \n        cv = test_results\n    else:\n        cv = None\n\n    return model, cv, history\n\ndef train_folds(CFG, folds, model_int=1, strategy=STRATEGY, summary=True):\n    all_fold_results = []\n    \n    for fold in folds:\n        if fold != 'all':\n            all_files = TRAIN_FILENAMES\n            # Split validation files into validation and test\n            fold_files = [x for x in all_files if f'fold{fold}' in x]\n            \n            # Determine split ratio (default 50/50 split between validation and test)\n            val_test_split = getattr(CFG, 'val_test_split', 0.5)\n            \n            # Split fold files into validation and test\n            split_idx = int(len(fold_files) * val_test_split)\n            valid_files = fold_files[:split_idx]\n            test_files = fold_files[split_idx:]\n            \n            # Get training files (excluding both validation and test)\n            train_files = [x for x in all_files if f'fold{fold}' not in x]\n            \n            print(f\"Fold {fold} - Train: {len(train_files)} files, Validation: {len(valid_files)} files, Test: {len(test_files)} files\")\n        else:\n            train_files = TRAIN_FILENAMES\n            valid_files = None\n            test_files = None\n        \n        model, cv_results, history = train_fold(CFG, fold, train_files, valid_files, test_files, model_int=model_int, strategy=strategy, summary=summary)\n        \n        if cv_results is not None:\n            all_fold_results.append({\n                'fold': fold,\n                'results': cv_results\n            })\n    \n    # Print aggregated results across all folds\n    if all_fold_results:\n        print(\"\\n\\n========== FINAL TEST RESULTS ACROSS ALL FOLDS ==========\")\n        metrics = ['loss', 'categorical_accuracy', 'top5_acc', 'macro_f1_score', 'micro_f1_score']\n        \n        # Initialize dictionaries to store metrics\n        sum_metrics = {metric: 0.0 for metric in metrics}\n        \n        # Sum all metrics across folds\n        for fold_result in all_fold_results:\n            results = fold_result['results']\n            for metric in metrics:\n                if metric in results:\n                    sum_metrics[metric] += results[metric]\n        \n        # Calculate averages\n        num_folds = len(all_fold_results)\n        avg_metrics = {metric: sum_metrics[metric] / num_folds for metric in metrics}\n        \n        # Print final aggregated results\n        print(f\"Averaged across {num_folds} folds:\")\n        print(f\"Loss:                 {avg_metrics['loss']:.4f}\")\n        print(f\"Categorical Accuracy:  {avg_metrics['categorical_accuracy']:.4f}\")\n        print(f\"Top-5 Accuracy:        {avg_metrics['top5_acc']:.4f}\")\n        print(f\"Macro F1 Score:        {avg_metrics['macro_f1_score']:.4f}\")\n        print(f\"Micro F1 Score:        {avg_metrics['micro_f1_score']:.4f}\")\n        \n    return all_fold_results","metadata":{"id":"BoGUEL6-oEWO","execution":{"iopub.status.busy":"2025-04-20T16:00:30.064297Z","iopub.execute_input":"2025-04-20T16:00:30.064618Z","iopub.status.idle":"2025-04-20T16:00:30.099346Z","shell.execute_reply.started":"2025-04-20T16:00:30.064594Z","shell.execute_reply":"2025-04-20T16:00:30.098430Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Evaluation\nModels can be evaluated with this function separately from the training step (though our training functions also displays final test results)\n\nThese functions will split the data the same way as the training step does, since we are using the same random seed 42. This method can be used to cross-check evaluation results amongst all models trained. This evaluation function assumes that only one fold of data is trained.","metadata":{}},{"cell_type":"code","source":"def evaluate_all(CFG, model_path, train_files, valid_files=None, test_files=None, model_int=1, strategy=STRATEGY, summary=True):\n    seed_everything(CFG.seed)\n    tf.keras.backend.clear_session()\n    gc.collect()\n    tf.config.optimizer.set_jit(True)\n        \n    if CFG.fp16:\n        try:\n            policy = mixed_precision.Policy('mixed_bfloat16')\n            mixed_precision.set_global_policy(policy)\n        except:\n            policy = mixed_precision.Policy('mixed_float16')\n            mixed_precision.set_global_policy(policy)\n    else:\n        policy = mixed_precision.Policy('float32')\n        mixed_precision.set_global_policy(policy)\n\n    train_ds = get_tfrec_dataset(train_files, batch_size=CFG.batch_size, max_len=CFG.max_len, drop_remainder=True, augment=True, repeat=True, shuffle=32768)\n    valid_ds = get_tfrec_dataset(valid_files, batch_size=CFG.batch_size, max_len=CFG.max_len, drop_remainder=False, repeat=False, shuffle=False)\n    test_ds = get_tfrec_dataset(test_files, batch_size=CFG.batch_size, max_len=CFG.max_len, drop_remainder=False, repeat=False, shuffle=False)\n    \n    num_train = count_data_items(train_files)\n    num_valid = count_data_items(valid_files) if valid_files else 0\n    num_test = count_data_items(test_files) if test_files else 0\n    steps_per_epoch = num_train//CFG.batch_size\n    \n    with strategy.scope():\n        dropout_step = CFG.dropout_start_epoch * steps_per_epoch\n        model = get_model(model_int, max_len=CFG.max_len, dropout_step=dropout_step, dim=CFG.dim)\n        model.load_weights(model_path)\n\n        schedule = OneCycleLR(CFG.lr, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=CFG.resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min, decay_type=CFG.decay_type, warmup_type='linear')\n        decay_schedule = OneCycleLR(CFG.lr*CFG.weight_decay, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=CFG.resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min*CFG.weight_decay, decay_type=CFG.decay_type, warmup_type='linear')\n                \n        awp_step = CFG.awp_start_epoch * steps_per_epoch\n        if CFG.fgm:\n            model = FGM(model.input, model.output, delta=CFG.awp_lambda, eps=0., start_step=awp_step)\n        elif CFG.awp:\n            model = AWP(model.input, model.output, delta=CFG.awp_lambda, eps=0., start_step=awp_step)\n\n        opt = tfa.optimizers.RectifiedAdam(learning_rate=schedule, weight_decay=decay_schedule, sma_threshold=4)#, clipvalue=1.)\n        opt = tfa.optimizers.Lookahead(opt,sync_period=5)\n\n        model.compile(\n            optimizer=opt,\n            loss=[tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)], #[tf.keras.losses.CategoricalCrossentropy(from_logits=True)],\n            metrics=[\n                [\n                tf.keras.metrics.CategoricalAccuracy(),\n                tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')\n                ],\n            ],\n            steps_per_execution=steps_per_epoch,\n        )\n    \n    if summary:\n        print()\n        model.summary()\n        print()\n        print(f\"Test dataset: {test_ds}\")\n        print()\n        schedule.plot()\n        print()\n        init=False\n\n    print(f'Number of test samples:{num_test}')\n    print()\n \n    # Evaluate on test set\n    print(\"\\n----- Evaluating on test set -----\")\n    # Standard evaluation with built-in metrics\n    test_metrics = model.evaluate(test_ds, verbose=CFG.verbose, steps=-(num_test//-CFG.batch_size))\n    test_results = dict(zip(model.metrics_names, test_metrics))\n    \n    # Collect predictions for F1 scores\n    y_pred = []\n    y_true = []\n    test_steps = 0\n        \n    for x, y in test_ds:\n        pred = model.predict(x, verbose=0)\n        if isinstance(pred, list):\n            pred = pred[0]\n        \n        batch_pred = np.argmax(pred, axis=1)\n        batch_true = np.argmax(y.numpy(), axis=1)\n        \n        y_pred.append(batch_pred)\n        y_true.append(batch_true)\n        \n        test_steps += 1\n        if test_steps >= -(num_test//-CFG.batch_size):\n            break\n            \n    y_pred = np.concatenate(y_pred)\n    y_true = np.concatenate(y_true)\n    \n    # Calculate F1 scores\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    \n    # Add F1 scores to results dictionary\n    test_results['macro_f1_score'] = macro_f1\n    test_results['micro_f1_score'] = micro_f1\n    \n    # Print all test metrics\n    print(\"\\n----- Test Results -----\")\n    print(f\"Loss:                 {test_results['loss']:.4f}\")\n    print(f\"Categorical Accuracy:  {test_results['categorical_accuracy']:.4f}\")\n    print(f\"Top-5 Accuracy:        {test_results['top5_acc']:.4f}\")\n    print(f\"Macro F1 Score:        {macro_f1:.4f}\")\n    print(f\"Micro F1 Score:        {micro_f1:.4f}\")\n    \n    cv = test_results\n\n    return model, cv\n\ndef evaluate(CFG, model_path, model_int=1, strategy=STRATEGY, summary=True):\n    all_results = []\n    \n    all_files = TRAIN_FILENAMES\n    # Split validation files into validation and test\n    files = [x for x in all_files if f'fold{0}' in x]\n    \n    # Determine split ratio (default 50/50 split between validation and test)\n    val_test_split = getattr(CFG, 'val_test_split', 0.5)\n    \n    # Split fold files into validation and test\n    split_idx = int(len(files) * val_test_split)\n    valid_files = files[:split_idx]\n    test_files = files[split_idx:]\n    \n    # Get training files (excluding both validation and test)\n    train_files = [x for x in all_files if f'fold{0}' not in x]\n    \n    print(f\"Test: {len(test_files)} files\")\n    \n    model, cv_results = evaluate_all(CFG, model_path, train_files, valid_files, test_files, model_int=model_int, strategy=strategy, summary=summary)   \n        \n    return cv_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:09:34.331538Z","iopub.execute_input":"2025-04-20T17:09:34.332060Z","iopub.status.idle":"2025-04-20T17:09:34.358782Z","shell.execute_reply.started":"2025-04-20T17:09:34.331986Z","shell.execute_reply":"2025-04-20T17:09:34.357723Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"# Best configurations for training, try not to modify this (except for output directory folder)\n\nclass CFG:\n    n_splits = 5\n    save_output = True\n    output_dir = '/kaggle/working'  # Change this to the directory that you want to save your model and results in.\n    \n    seed = 42\n    verbose = 2 #0) silent 1) progress bar 2) one line per epoch\n    \n    max_len = 384\n    replicas = 8\n    lr = 5e-4 * replicas\n    weight_decay = 0.1\n    lr_min = 1e-6\n    epoch = 120 #300\n    warmup = 0\n    batch_size = 64 * replicas\n    snapshot_epochs = []\n    swa_epochs = [] #list(range(epoch//2,epoch+1))\n    val_test_split = 0.5\n    \n    fp16 = True\n    fgm = False\n    awp = True\n    awp_lambda = 0.2\n    awp_start_epoch = 15\n    dropout_start_epoch = 15\n    resume = 0\n    decay_type = 'cosine'\n    dim = 192\n    comment = f'islr-fp16-192-8-seed{seed}'","metadata":{"id":"WYUI6mAlqAFR","execution":{"iopub.status.busy":"2025-04-20T17:39:54.311353Z","iopub.execute_input":"2025-04-20T17:39:54.311681Z","iopub.status.idle":"2025-04-20T17:39:54.318686Z","shell.execute_reply.started":"2025-04-20T17:39:54.311656Z","shell.execute_reply":"2025-04-20T17:39:54.317634Z"},"trusted":true},"outputs":[],"execution_count":102},{"cell_type":"code","source":"# Start training and print results for 1 fold cross-validation. Model weights and results will be saved to Kaggle's working folder.\n# Change model_int according to the model you want :\n# 1: cnn_transformers_model\n# 2: cnn_lstm_model\n# 3: cnn_gru_model\n# 4: cnn_model\n# 5: transformers_model\n# 6: cnn_lintransformers_model\n# Default is cnn_transformers_model\n\ntrain_folds(CFG, folds=[0], model_int=6)","metadata":{"id":"V8T5GhYPqAFR","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:17:50.080562Z","iopub.execute_input":"2025-04-20T18:17:50.080910Z","iopub.status.idle":"2025-04-20T18:25:39.230585Z","shell.execute_reply.started":"2025-04-20T18:17:50.080887Z","shell.execute_reply":"2025-04-20T18:25:39.229547Z"}},"outputs":[{"name":"stdout","text":"Fold 0 - Train: 140 files, Validation: 23 files, Test: 24 files\nPreprocessing...\nResample applied\nFlip applied\nTemporal crop applied\nSpatial random affine applied\nTemporal mask applied\nSpatial mask applied\n\nModel: \"awp\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 384, 708)]   0           []                               \n                                                                                                  \n masking (Masking)              (None, 384, 708)     0           ['input_1[0][0]']                \n                                                                                                  \n stem_conv (Dense)              (None, 384, 192)     135936      ['masking[0][0]']                \n                                                                                                  \n stem_bn (BatchNormalization)   (None, 384, 192)     768         ['stem_conv[0][0]']              \n                                                                                                  \n 1_expand_conv (Dense)          (None, 384, 384)     74112       ['stem_bn[0][0]']                \n                                                                                                  \n 1_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['1_expand_conv[0][0]']          \n                                                                                                  \n 1_bn (BatchNormalization)      (None, 384, 384)     1536        ['1_dwconv[0][0]']               \n                                                                                                  \n eca (ECA)                      (None, 384, 384)     5           ['1_bn[0][0]']                   \n                                                                                                  \n 1_project_conv (Dense)         (None, 384, 192)     73920       ['eca[0][0]']                    \n                                                                                                  \n 1_drop (Dropout)               (None, 384, 192)     0           ['1_project_conv[0][0]']         \n                                                                                                  \n 1_add (Add)                    (None, 384, 192)     0           ['1_drop[0][0]',                 \n                                                                  'stem_bn[0][0]']                \n                                                                                                  \n 2_expand_conv (Dense)          (None, 384, 384)     74112       ['1_add[0][0]']                  \n                                                                                                  \n 2_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['2_expand_conv[0][0]']          \n                                                                                                  \n 2_bn (BatchNormalization)      (None, 384, 384)     1536        ['2_dwconv[0][0]']               \n                                                                                                  \n eca_1 (ECA)                    (None, 384, 384)     5           ['2_bn[0][0]']                   \n                                                                                                  \n 2_project_conv (Dense)         (None, 384, 192)     73920       ['eca_1[0][0]']                  \n                                                                                                  \n 2_drop (Dropout)               (None, 384, 192)     0           ['2_project_conv[0][0]']         \n                                                                                                  \n 2_add (Add)                    (None, 384, 192)     0           ['2_drop[0][0]',                 \n                                                                  '1_add[0][0]']                  \n                                                                                                  \n 3_expand_conv (Dense)          (None, 384, 384)     74112       ['2_add[0][0]']                  \n                                                                                                  \n 3_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['3_expand_conv[0][0]']          \n                                                                                                  \n 3_bn (BatchNormalization)      (None, 384, 384)     1536        ['3_dwconv[0][0]']               \n                                                                                                  \n eca_2 (ECA)                    (None, 384, 384)     5           ['3_bn[0][0]']                   \n                                                                                                  \n 3_project_conv (Dense)         (None, 384, 192)     73920       ['eca_2[0][0]']                  \n                                                                                                  \n 3_drop (Dropout)               (None, 384, 192)     0           ['3_project_conv[0][0]']         \n                                                                                                  \n 3_add (Add)                    (None, 384, 192)     0           ['3_drop[0][0]',                 \n                                                                  '2_add[0][0]']                  \n                                                                                                  \n batch_normalization (BatchNorm  (None, 384, 192)    768         ['3_add[0][0]']                  \n alization)                                                                                       \n                                                                                                  \n linformer_self_attention (Linf  (None, 384, 192)    540672      ['batch_normalization[0][0]']    \n ormerSelfAttention)                                                                              \n                                                                                                  \n dropout_1 (Dropout)            (None, 384, 192)     0           ['linformer_self_attention[0][0]'\n                                                                 ]                                \n                                                                                                  \n add (Add)                      (None, 384, 192)     0           ['3_add[0][0]',                  \n                                                                  'dropout_1[0][0]']              \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 384, 192)    768         ['add[0][0]']                    \n rmalization)                                                                                     \n                                                                                                  \n dense_2 (Dense)                (None, 384, 384)     73728       ['batch_normalization_1[0][0]']  \n                                                                                                  \n dense_3 (Dense)                (None, 384, 192)     73728       ['dense_2[0][0]']                \n                                                                                                  \n dropout_2 (Dropout)            (None, 384, 192)     0           ['dense_3[0][0]']                \n                                                                                                  \n add_1 (Add)                    (None, 384, 192)     0           ['add[0][0]',                    \n                                                                  'dropout_2[0][0]']              \n                                                                                                  \n 4_expand_conv (Dense)          (None, 384, 384)     74112       ['add_1[0][0]']                  \n                                                                                                  \n 4_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['4_expand_conv[0][0]']          \n                                                                                                  \n 4_bn (BatchNormalization)      (None, 384, 384)     1536        ['4_dwconv[0][0]']               \n                                                                                                  \n eca_3 (ECA)                    (None, 384, 384)     5           ['4_bn[0][0]']                   \n                                                                                                  \n 4_project_conv (Dense)         (None, 384, 192)     73920       ['eca_3[0][0]']                  \n                                                                                                  \n 4_drop (Dropout)               (None, 384, 192)     0           ['4_project_conv[0][0]']         \n                                                                                                  \n 4_add (Add)                    (None, 384, 192)     0           ['4_drop[0][0]',                 \n                                                                  'add_1[0][0]']                  \n                                                                                                  \n 5_expand_conv (Dense)          (None, 384, 384)     74112       ['4_add[0][0]']                  \n                                                                                                  \n 5_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['5_expand_conv[0][0]']          \n                                                                                                  \n 5_bn (BatchNormalization)      (None, 384, 384)     1536        ['5_dwconv[0][0]']               \n                                                                                                  \n eca_4 (ECA)                    (None, 384, 384)     5           ['5_bn[0][0]']                   \n                                                                                                  \n 5_project_conv (Dense)         (None, 384, 192)     73920       ['eca_4[0][0]']                  \n                                                                                                  \n 5_drop (Dropout)               (None, 384, 192)     0           ['5_project_conv[0][0]']         \n                                                                                                  \n 5_add (Add)                    (None, 384, 192)     0           ['5_drop[0][0]',                 \n                                                                  '4_add[0][0]']                  \n                                                                                                  \n 6_expand_conv (Dense)          (None, 384, 384)     74112       ['5_add[0][0]']                  \n                                                                                                  \n 6_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['6_expand_conv[0][0]']          \n                                                                                                  \n 6_bn (BatchNormalization)      (None, 384, 384)     1536        ['6_dwconv[0][0]']               \n                                                                                                  \n eca_5 (ECA)                    (None, 384, 384)     5           ['6_bn[0][0]']                   \n                                                                                                  \n 6_project_conv (Dense)         (None, 384, 192)     73920       ['eca_5[0][0]']                  \n                                                                                                  \n 6_drop (Dropout)               (None, 384, 192)     0           ['6_project_conv[0][0]']         \n                                                                                                  \n 6_add (Add)                    (None, 384, 192)     0           ['6_drop[0][0]',                 \n                                                                  '5_add[0][0]']                  \n                                                                                                  \n batch_normalization_2 (BatchNo  (None, 384, 192)    768         ['6_add[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n linformer_self_attention_1 (Li  (None, 384, 192)    540672      ['batch_normalization_2[0][0]']  \n nformerSelfAttention)                                                                            \n                                                                                                  \n dropout_4 (Dropout)            (None, 384, 192)     0           ['linformer_self_attention_1[0][0\n                                                                 ]']                              \n                                                                                                  \n add_2 (Add)                    (None, 384, 192)     0           ['6_add[0][0]',                  \n                                                                  'dropout_4[0][0]']              \n                                                                                                  \n batch_normalization_3 (BatchNo  (None, 384, 192)    768         ['add_2[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n dense_6 (Dense)                (None, 384, 384)     73728       ['batch_normalization_3[0][0]']  \n                                                                                                  \n dense_7 (Dense)                (None, 384, 192)     73728       ['dense_6[0][0]']                \n                                                                                                  \n dropout_5 (Dropout)            (None, 384, 192)     0           ['dense_7[0][0]']                \n                                                                                                  \n add_3 (Add)                    (None, 384, 192)     0           ['add_2[0][0]',                  \n                                                                  'dropout_5[0][0]']              \n                                                                                                  \n top_conv (Dense)               (None, 384, 384)     74112       ['add_3[0][0]']                  \n                                                                                                  \n global_average_pooling1d (Glob  (None, 384)         0           ['top_conv[0][0]']               \n alAveragePooling1D)                                                                              \n                                                                                                  \n late_dropout (LateDropout)     (None, 384)          1           ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n classifier (Dense)             (None, 250)          96250       ['late_dropout[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 2,623,001\nTrainable params: 2,616,472\nNon-trainable params: 6,529\n__________________________________________________________________________________________________\n\nTrain dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(512, 384, 708), dtype=tf.float32, name=None), TensorSpec(shape=(512, 250), dtype=tf.float32, name=None))>\nValidation dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 384, 708), dtype=tf.float32, name=None), TensorSpec(shape=(None, 250), dtype=tf.float32, name=None))>\nTest dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 384, 708), dtype=tf.float32, name=None), TensorSpec(shape=(None, 250), dtype=tf.float32, name=None))>\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\n---------fold0---------\ntrain:70611 valid:11776 test:12090\n\nWARNING:tensorflow:5 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7e7123c2f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7e7123c2f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","output_type":"stream"},{"name":"stdout","text":"137/137 - 387s - loss: 5.1537 - categorical_accuracy: 0.0476 - top5_acc: 0.1492 - val_loss: 4.5669 - val_categorical_accuracy: 0.1357 - val_top5_acc: 0.3385 - 387s/epoch - 3s/step\n\n----- Evaluating on test set -----\n24/24 - 19s - loss: 4.5710 - categorical_accuracy: 0.1355 - top5_acc: 0.3307 - 19s/epoch - 795ms/step\n","output_type":"stream"},{"name":"stderr","text":"2025-04-20 18:24:52.220904: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2025-04-20 18:24:52.412085: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2025-04-20 18:25:33.094490: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2025-04-20 18:25:33.398321: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"\n----- Test Results -----\nLoss:                 4.5710\nCategorical Accuracy:  0.1355\nTop-5 Accuracy:        0.3307\nMacro F1 Score:        0.1032\nMicro F1 Score:        0.1355\n\n\n========== FINAL TEST RESULTS ACROSS ALL FOLDS ==========\nAveraged across 1 folds:\nLoss:                 4.5710\nCategorical Accuracy:  0.1355\nTop-5 Accuracy:        0.3307\nMacro F1 Score:        0.1032\nMicro F1 Score:        0.1355\n","output_type":"stream"},{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"[{'fold': 0,\n  'results': {'loss': 4.570952415466309,\n   'categorical_accuracy': 0.13548387587070465,\n   'top5_acc': 0.3306865096092224,\n   'macro_f1_score': 0.10321193416226436,\n   'micro_f1_score': 0.13548387096774195}}]"},"metadata":{}}],"execution_count":126},{"cell_type":"code","source":"# Make sure that the arguments for models_path and model_int correspond to model weights for the model type that you have trained.\n# You can unzip the files in our results folder on our github, and load the best model weights (in .h5 files) for the model you want to test.\n# Replace the path below with your own path to the best model weights.\nmodel_path = '/kaggle/input/b/tensorflow2/default/1/islr-fp16-192-8-seed42-fold0-best.h5'\n\n# evaluate(CFG, model_path, model_int=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:09:41.284105Z","iopub.execute_input":"2025-04-20T17:09:41.284501Z","iopub.status.idle":"2025-04-20T17:11:06.270413Z","shell.execute_reply.started":"2025-04-20T17:09:41.284453Z","shell.execute_reply":"2025-04-20T17:11:06.269225Z"}},"outputs":[{"name":"stdout","text":"Test: 24 files\nPreprocessing...\nResample applied\nFlip applied\nTemporal crop applied\nSpatial random affine applied\nTemporal mask applied\nSpatial mask applied\n\nModel: \"awp\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 384, 708)]   0           []                               \n                                                                                                  \n masking (Masking)              (None, 384, 708)     0           ['input_1[0][0]']                \n                                                                                                  \n stem_conv (Dense)              (None, 384, 192)     135936      ['masking[0][0]']                \n                                                                                                  \n stem_bn (BatchNormalization)   (None, 384, 192)     768         ['stem_conv[0][0]']              \n                                                                                                  \n 1_expand_conv (Dense)          (None, 384, 384)     74112       ['stem_bn[0][0]']                \n                                                                                                  \n 1_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['1_expand_conv[0][0]']          \n                                                                                                  \n 1_bn (BatchNormalization)      (None, 384, 384)     1536        ['1_dwconv[0][0]']               \n                                                                                                  \n eca (ECA)                      (None, 384, 384)     5           ['1_bn[0][0]']                   \n                                                                                                  \n 1_project_conv (Dense)         (None, 384, 192)     73920       ['eca[0][0]']                    \n                                                                                                  \n 1_drop (Dropout)               (None, 384, 192)     0           ['1_project_conv[0][0]']         \n                                                                                                  \n 1_add (Add)                    (None, 384, 192)     0           ['1_drop[0][0]',                 \n                                                                  'stem_bn[0][0]']                \n                                                                                                  \n 2_expand_conv (Dense)          (None, 384, 384)     74112       ['1_add[0][0]']                  \n                                                                                                  \n 2_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['2_expand_conv[0][0]']          \n                                                                                                  \n 2_bn (BatchNormalization)      (None, 384, 384)     1536        ['2_dwconv[0][0]']               \n                                                                                                  \n eca_1 (ECA)                    (None, 384, 384)     5           ['2_bn[0][0]']                   \n                                                                                                  \n 2_project_conv (Dense)         (None, 384, 192)     73920       ['eca_1[0][0]']                  \n                                                                                                  \n 2_drop (Dropout)               (None, 384, 192)     0           ['2_project_conv[0][0]']         \n                                                                                                  \n 2_add (Add)                    (None, 384, 192)     0           ['2_drop[0][0]',                 \n                                                                  '1_add[0][0]']                  \n                                                                                                  \n 3_expand_conv (Dense)          (None, 384, 384)     74112       ['2_add[0][0]']                  \n                                                                                                  \n 3_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['3_expand_conv[0][0]']          \n                                                                                                  \n 3_bn (BatchNormalization)      (None, 384, 384)     1536        ['3_dwconv[0][0]']               \n                                                                                                  \n eca_2 (ECA)                    (None, 384, 384)     5           ['3_bn[0][0]']                   \n                                                                                                  \n 3_project_conv (Dense)         (None, 384, 192)     73920       ['eca_2[0][0]']                  \n                                                                                                  \n 3_drop (Dropout)               (None, 384, 192)     0           ['3_project_conv[0][0]']         \n                                                                                                  \n 3_add (Add)                    (None, 384, 192)     0           ['3_drop[0][0]',                 \n                                                                  '2_add[0][0]']                  \n                                                                                                  \n batch_normalization (BatchNorm  (None, 384, 192)    768         ['3_add[0][0]']                  \n alization)                                                                                       \n                                                                                                  \n multi_head_self_attention (Mul  (None, 384, 192)    147456      ['batch_normalization[0][0]']    \n tiHeadSelfAttention)                                                                             \n                                                                                                  \n dropout_1 (Dropout)            (None, 384, 192)     0           ['multi_head_self_attention[0][0]\n                                                                 ']                               \n                                                                                                  \n add (Add)                      (None, 384, 192)     0           ['3_add[0][0]',                  \n                                                                  'dropout_1[0][0]']              \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 384, 192)    768         ['add[0][0]']                    \n rmalization)                                                                                     \n                                                                                                  \n dense_2 (Dense)                (None, 384, 384)     73728       ['batch_normalization_1[0][0]']  \n                                                                                                  \n dense_3 (Dense)                (None, 384, 192)     73728       ['dense_2[0][0]']                \n                                                                                                  \n dropout_2 (Dropout)            (None, 384, 192)     0           ['dense_3[0][0]']                \n                                                                                                  \n add_1 (Add)                    (None, 384, 192)     0           ['add[0][0]',                    \n                                                                  'dropout_2[0][0]']              \n                                                                                                  \n 4_expand_conv (Dense)          (None, 384, 384)     74112       ['add_1[0][0]']                  \n                                                                                                  \n 4_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['4_expand_conv[0][0]']          \n                                                                                                  \n 4_bn (BatchNormalization)      (None, 384, 384)     1536        ['4_dwconv[0][0]']               \n                                                                                                  \n eca_3 (ECA)                    (None, 384, 384)     5           ['4_bn[0][0]']                   \n                                                                                                  \n 4_project_conv (Dense)         (None, 384, 192)     73920       ['eca_3[0][0]']                  \n                                                                                                  \n 4_drop (Dropout)               (None, 384, 192)     0           ['4_project_conv[0][0]']         \n                                                                                                  \n 4_add (Add)                    (None, 384, 192)     0           ['4_drop[0][0]',                 \n                                                                  'add_1[0][0]']                  \n                                                                                                  \n 5_expand_conv (Dense)          (None, 384, 384)     74112       ['4_add[0][0]']                  \n                                                                                                  \n 5_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['5_expand_conv[0][0]']          \n                                                                                                  \n 5_bn (BatchNormalization)      (None, 384, 384)     1536        ['5_dwconv[0][0]']               \n                                                                                                  \n eca_4 (ECA)                    (None, 384, 384)     5           ['5_bn[0][0]']                   \n                                                                                                  \n 5_project_conv (Dense)         (None, 384, 192)     73920       ['eca_4[0][0]']                  \n                                                                                                  \n 5_drop (Dropout)               (None, 384, 192)     0           ['5_project_conv[0][0]']         \n                                                                                                  \n 5_add (Add)                    (None, 384, 192)     0           ['5_drop[0][0]',                 \n                                                                  '4_add[0][0]']                  \n                                                                                                  \n 6_expand_conv (Dense)          (None, 384, 384)     74112       ['5_add[0][0]']                  \n                                                                                                  \n 6_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['6_expand_conv[0][0]']          \n                                                                                                  \n 6_bn (BatchNormalization)      (None, 384, 384)     1536        ['6_dwconv[0][0]']               \n                                                                                                  \n eca_5 (ECA)                    (None, 384, 384)     5           ['6_bn[0][0]']                   \n                                                                                                  \n 6_project_conv (Dense)         (None, 384, 192)     73920       ['eca_5[0][0]']                  \n                                                                                                  \n 6_drop (Dropout)               (None, 384, 192)     0           ['6_project_conv[0][0]']         \n                                                                                                  \n 6_add (Add)                    (None, 384, 192)     0           ['6_drop[0][0]',                 \n                                                                  '5_add[0][0]']                  \n                                                                                                  \n batch_normalization_2 (BatchNo  (None, 384, 192)    768         ['6_add[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n multi_head_self_attention_1 (M  (None, 384, 192)    147456      ['batch_normalization_2[0][0]']  \n ultiHeadSelfAttention)                                                                           \n                                                                                                  \n dropout_4 (Dropout)            (None, 384, 192)     0           ['multi_head_self_attention_1[0][\n                                                                 0]']                             \n                                                                                                  \n add_2 (Add)                    (None, 384, 192)     0           ['6_add[0][0]',                  \n                                                                  'dropout_4[0][0]']              \n                                                                                                  \n batch_normalization_3 (BatchNo  (None, 384, 192)    768         ['add_2[0][0]']                  \n rmalization)                                                                                     \n                                                                                                  \n dense_6 (Dense)                (None, 384, 384)     73728       ['batch_normalization_3[0][0]']  \n                                                                                                  \n dense_7 (Dense)                (None, 384, 192)     73728       ['dense_6[0][0]']                \n                                                                                                  \n dropout_5 (Dropout)            (None, 384, 192)     0           ['dense_7[0][0]']                \n                                                                                                  \n add_3 (Add)                    (None, 384, 192)     0           ['add_2[0][0]',                  \n                                                                  'dropout_5[0][0]']              \n                                                                                                  \n top_conv (Dense)               (None, 384, 384)     74112       ['add_3[0][0]']                  \n                                                                                                  \n global_average_pooling1d (Glob  (None, 384)         0           ['top_conv[0][0]']               \n alAveragePooling1D)                                                                              \n                                                                                                  \n late_dropout (LateDropout)     (None, 384)          1           ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n classifier (Dense)             (None, 250)          96250       ['late_dropout[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 1,836,569\nTrainable params: 1,830,040\nNon-trainable params: 6,529\n__________________________________________________________________________________________________\n\nTest dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 384, 708), dtype=tf.float32, name=None), TensorSpec(shape=(None, 250), dtype=tf.float32, name=None))>\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\n\n----- Evaluating on test set -----\n24/24 - 27s - loss: 1.7630 - categorical_accuracy: 0.7843 - top5_acc: 0.9233 - 27s/epoch - 1s/step\n","output_type":"stream"},{"name":"stderr","text":"2025-04-20 17:10:19.348362: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2025-04-20 17:10:19.531105: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2025-04-20 17:11:00.377733: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2025-04-20 17:11:00.702264: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"\n----- Test Results -----\nLoss:                 1.7630\nCategorical Accuracy:  0.7843\nTop-5 Accuracy:        0.9233\nMacro F1 Score:        0.7806\nMicro F1 Score:        0.7845\n","output_type":"stream"},{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"{'loss': 1.7629574537277222,\n 'categorical_accuracy': 0.7842845320701599,\n 'top5_acc': 0.9233250617980957,\n 'macro_f1_score': 0.7805755120709428,\n 'micro_f1_score': 0.7845326716294458}"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}