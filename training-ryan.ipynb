{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222eecf9",
   "metadata": {
    "papermill": {
     "duration": 0.003988,
     "end_time": "2025-04-14T15:24:15.668422",
     "exception": false,
     "start_time": "2025-04-14T15:24:15.664434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c56856",
   "metadata": {
    "papermill": {
     "duration": 0.002747,
     "end_time": "2025-04-14T15:24:15.674733",
     "exception": false,
     "start_time": "2025-04-14T15:24:15.671986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3a3abe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:15.682415Z",
     "iopub.status.busy": "2025-04-14T15:24:15.682018Z",
     "iopub.status.idle": "2025-04-14T15:24:31.713590Z",
     "shell.execute_reply": "2025-04-14T15:24:31.712574Z"
    },
    "papermill": {
     "duration": 16.037614,
     "end_time": "2025-04-14T15:24:31.715434",
     "exception": false,
     "start_time": "2025-04-14T15:24:15.677820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchvision.transforms import RandomAffine\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5bdf56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:31.726710Z",
     "iopub.status.busy": "2025-04-14T15:24:31.725693Z",
     "iopub.status.idle": "2025-04-14T15:24:31.735547Z",
     "shell.execute_reply": "2025-04-14T15:24:31.734208Z"
    },
    "papermill": {
     "duration": 0.016708,
     "end_time": "2025-04-14T15:24:31.737527",
     "exception": false,
     "start_time": "2025-04-14T15:24:31.720819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88585c97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:31.745423Z",
     "iopub.status.busy": "2025-04-14T15:24:31.745092Z",
     "iopub.status.idle": "2025-04-14T15:24:31.983430Z",
     "shell.execute_reply": "2025-04-14T15:24:31.982435Z"
    },
    "papermill": {
     "duration": 0.244473,
     "end_time": "2025-04-14T15:24:31.985360",
     "exception": false,
     "start_time": "2025-04-14T15:24:31.740887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"../input/asl-signs/\"\n",
    "\n",
    "# Read in train dataset\n",
    "df = pd.read_csv(f\"{BASE_DIR}/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd0c063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:31.995389Z",
     "iopub.status.busy": "2025-04-14T15:24:31.995014Z",
     "iopub.status.idle": "2025-04-14T15:24:32.021577Z",
     "shell.execute_reply": "2025-04-14T15:24:32.020559Z"
    },
    "papermill": {
     "duration": 0.034328,
     "end_time": "2025-04-14T15:24:32.023456",
     "exception": false,
     "start_time": "2025-04-14T15:24:31.989128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a label-to-index mapping\n",
    "SIGN_TO_INDEX = {sign: idx for idx, sign in enumerate(df['sign'].unique())}\n",
    "\n",
    "# Frame Constants\n",
    "LANDMARKS_PER_FRAME = 543 # Number of landmarks per frame from MediaPipe\n",
    "MAX_LEN = 384 # Maximum number of frames used per video (for padding/cropping)\n",
    "CROP_LEN = MAX_LEN # Same as `MAX_LEN` defined separately for flexibility e.g. use a shorter crop for training\n",
    "NUM_CLASSES = 250 # Number of sign classes in the dataset\n",
    "PAD = -100. # Fill value for padding frames to distinguish from real values\n",
    "\n",
    "# Landmark Groups\n",
    "NOSE=[1, 2, 98, 327]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "LIP = [\n",
    "    0, 13, 14, 17, 37, 39, 40, 61, 78, 80, 81, 82, 84, 87, 88, 91, \n",
    "    95, 146, 178, 181, 185, 191, 267, 269, 270, 291, 308, 310, 311, \n",
    "    312, 314, 317, 318, 321, 324, 375, 402, 405, 409, 415\n",
    "] # Full mouth contour\n",
    "LLIP = [37, 39, 40, 61, 78, 80, 81, 82, 84, 87, 88, 91, 95, 146, 178, 181, 185, 191] # Left part of the lips\n",
    "RLIP = [267, 269, 270, 291, 308, 310, 311, 312, 314, 317, 318, 321, 324, 375, 402, 405, 409, 415] # Right part of the lips\n",
    "\n",
    "POSE = [500, 501, 502, 503, 504, 505, 512, 513] # Pose landmarks\n",
    "LPOSE = [501, 503, 505, 513]\n",
    "RPOSE = [500, 502, 504, 512]\n",
    "\n",
    "REYE = [7, 33, 133, 144, 145, 153, 154, 155, 157, 158, 159, 160, 161, 163, 173, 246] # Right eye\n",
    "LEYE = [249, 263, 362, 373, 374, 380, 381, 382, 384, 385, 386, 387, 388, 390, 398, 466] # Left eye\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist() # Left hand\n",
    "RHAND = np.arange(522, 543).tolist() # Right hand\n",
    "\n",
    "# Final landmark selection: list of landmarks to be used as input to model\n",
    "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n",
    "NUM_NODES = len(POINT_LANDMARKS) # Number of chosen landmark points\n",
    "CHANNELS = 6*NUM_NODES # 6 input channels because preprocessing creates x,dx,dx2,y,dy,dy2 per landmark point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c8d5e",
   "metadata": {
    "papermill": {
     "duration": 0.002977,
     "end_time": "2025-04-14T15:24:32.030097",
     "exception": false,
     "start_time": "2025-04-14T15:24:32.027120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Preprocessing\n",
    "- Landmark Selection & Dimensionality Reduction:\n",
    "    - Only a subset of informative landmark indices (`point_landmarks`) are used to reduce dimensionality.\n",
    "    - We discard the z-axis (depth), retaining only x and y coordinates.\n",
    "- Normalization:\n",
    "    - Landmark coordinates are normalized by subtracting the mean position of the nose (landmark 17).\n",
    "    - Normalization is done per sample to center the hand/body around the face.\n",
    "    - The full sample is divided by the global standard deviation for scale invariance.\n",
    "- Padding / Truncation:\n",
    "    - Sequences are padded with the last frame (if too short) or truncated (if too long) to a fixed `max_len`.\n",
    "    - Padding is only for keeping sequence size constant for the data loader. We generate a binary mask (1 for real values) to be passed to the model for it to ignore padded positions during training and inferrence.\n",
    "- Motion Features:\n",
    "    - We compute velocity (`dx`) and acceleration (`dx2`) features by differencing adjacent and second-adjacent frames.\n",
    "    - Final input = concatenation of `[positions, velocity, acceleration]` (for both x and y) per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a713d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:32.038465Z",
     "iopub.status.busy": "2025-04-14T15:24:32.038133Z",
     "iopub.status.idle": "2025-04-14T15:24:32.048374Z",
     "shell.execute_reply": "2025-04-14T15:24:32.047590Z"
    },
    "papermill": {
     "duration": 0.016172,
     "end_time": "2025-04-14T15:24:32.049761",
     "exception": false,
     "start_time": "2025-04-14T15:24:32.033589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, point_landmarks: list[int], max_len: int):\n",
    "        self.point_landmarks = point_landmarks # List of selected landmark indices\n",
    "        self.max_len = max_len # Maximum sequence length for padding and truncation\n",
    "\n",
    "    def __call__(self, x): \n",
    "        # x: (T, 543, 3), where T is number of timesteps (frames)\n",
    "        T, P, C = x.shape # (timesteps, points, coordinates)\n",
    "\n",
    "        # Only use specified landmarks\n",
    "        x = x[:, self.point_landmarks, :2] # Discard z-coordinate (estimated depth)\n",
    "        # x now contains 2D coordinates only: (T, P, 2)\n",
    "\n",
    "        # Reference mean: landmark 17 (at the nose)\n",
    "        # It is usually located close to center ([0.5,0.5])\n",
    "        nose_reference = x[:, [17], :]\n",
    "\n",
    "        # Compute the mean position of the nose across all frames\n",
    "        nose_mean = torch.nanmean(nose_reference, dim=(0, 1), keepdim=True) # Mean across time and points\n",
    "        nose_mean[torch.isnan(nose_mean)] = 0.5 # Set any NaN values to 0.5 \n",
    "        x_no_nan = torch.nan_to_num(x, nan=0.0)\n",
    "        std = torch.std(x_no_nan, dim=(0, 1), keepdim=True) # Standard deviation across time and points (we let NaN values be 0)\n",
    "        # TODO: use torch.nanstd\n",
    "\n",
    "        # Normalize inputs\n",
    "        x = (x - nose_mean) / std\n",
    "\n",
    "        # Pad or truncate sequence to fixed length\n",
    "        original_seq_length = x.shape[0]\n",
    "        print(\"d:\", original_seq_length, self.max_len)\n",
    "        if original_seq_length > self.max_len:\n",
    "            # Truncate\n",
    "            x = x[:self.max_len] \n",
    "        elif original_seq_length < self.max_len:\n",
    "            # Pad with the last frame\n",
    "            pad = x[-1:].repeat(self.max_len - x.shape[0], 1, 1)\n",
    "            x = torch.cat([x, pad], dim=0)\n",
    "\n",
    "        # Create mask: 1 for real data, 0 for padded values (to be ignored by model)\n",
    "        mask = torch.ones(x.shape[0], dtype=torch.float32)\n",
    "        if original_seq_length < self.max_len: # i.e. If we padded the sequence\n",
    "            mask[original_seq_length:] = 0.0 # Set padded values (appended to original sequence) to 0\n",
    "\n",
    "        # Compute motion features: dx (first differential), dx2 (second differential)\n",
    "        # variable x contains 2D position data (both x and y coordinates)\n",
    "        dx = torch.zeros_like(x) # velocity\n",
    "        dx2 = torch.zeros_like(x) # acceleration\n",
    "\n",
    "        # Calculate the first difference (lag-1)\n",
    "        if x.shape[0] > 1:\n",
    "            dx[:-1] = x[1:] - x[:-1] # dx[t] = x[t+1] - x[t]\n",
    "\n",
    "        # Calculate the second difference (lag-2)\n",
    "        if x.shape[0] > 2:\n",
    "            dx2[:-2] = x[2:] - x[:-2] # dx2[t] = dx[t+1] - dx[t]\n",
    "            \n",
    "        # Reshape the tensor for concatenation: flatten each sequence into single vector\n",
    "        # x = x.view(x.shape[0], -1) # Shape: (T,P*2), flatten the 2D points (x,y) into single vector\n",
    "        # dx = dx.view(dx.shape[0], -1) # Shape: (T,P*2), flatten the velocity (dx)\n",
    "        # dx2 = dx2.view(dx2.shape[0], -1) # Shape: (T, P*2), flatten the acceleration (dx2)\n",
    "\n",
    "        # Concatenate x (position), dx (velocity), and dx2 (acceleration) along the feature axis\n",
    "        x = torch.cat([x, dx, dx2], dim=-1) # Shape: (T, P*6), combining all features\n",
    "        x[torch.isnan(x)] = 0.0 # Replace any NaN values with 0\n",
    "\n",
    "        return x, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b8827",
   "metadata": {
    "papermill": {
     "duration": 0.002842,
     "end_time": "2025-04-14T15:24:32.056021",
     "exception": false,
     "start_time": "2025-04-14T15:24:32.053179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Augmentation\n",
    "- Temporal Rescaling:\n",
    "    - Each sequence is randomly stretched or compressed using linear interpolation (by a random scale ∈ [0.5, 1.5]).\n",
    "- Random Temporal Masking:\n",
    "    - With some probability, a subset of frames is replaced with NaNs to simulate missing data.\n",
    "- Horizontal Flipping:\n",
    "    - With a certain probability, the x-coordinates are flipped to simulate mirrored gestures.\n",
    "- Affine Transformations:\n",
    "    - Each frame is independently perturbed using random affine transforms (rotation, translation, scaling, shearing).\n",
    "- Cutout (Frame Dropout):\n",
    "    - A contiguous block of frames is set to zero to simulate occlusion or dropped frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b302f980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:32.064239Z",
     "iopub.status.busy": "2025-04-14T15:24:32.063533Z",
     "iopub.status.idle": "2025-04-14T15:24:32.073307Z",
     "shell.execute_reply": "2025-04-14T15:24:32.072308Z"
    },
    "papermill": {
     "duration": 0.016009,
     "end_time": "2025-04-14T15:24:32.075091",
     "exception": false,
     "start_time": "2025-04-14T15:24:32.059082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Augment:\n",
    "    def __init__(\n",
    "        self, \n",
    "        temporal_scale_range=(0.5,1.5), \n",
    "        masking_prob=0.1, \n",
    "        hflip_prob=0.5,\n",
    "        cutout_frames=10, \n",
    "        affine_params=dict(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1), shear=10)\n",
    "    ):\n",
    "        self.temporal_scale_range = temporal_scale_range\n",
    "        self.masking_prob = masking_prob\n",
    "        self.hflip_prob = hflip_prob\n",
    "        self.cutout_frames = cutout_frames\n",
    "        self.affine = RandomAffine(**affine_params)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        T, P, C = x.shape # (timesteps, points, coordinates)\n",
    "\n",
    "        # (1) Temporal Augmentations\n",
    "        # Random Resample: Randomly stretch/compress the time sequence using linear interpolation\n",
    "        scale = random.uniform(*self.temporal_scale_range)\n",
    "        new_len = max(4, int(T * scale)) # Avoid overly small length < 4 \n",
    "        x_tc = x.permute(1, 2, 0).reshape(1, P * C, T) # Reshape so that each (p,c) is its own channel: (1, P*C, T)\n",
    "        x_tc = F.interpolate(x_tc, size=new_len, mode='linear', align_corners=True) # Interpolate along temporal dimension\n",
    "        x = x_tc.reshape(1, P, C, new_len).squeeze(0).permute(2, 0, 1) # Reshape back to (T', P, C)\n",
    "\n",
    "        # Random Masking: Dropout-like temporal occlusion\n",
    "        if random.random() < self.masking_prob:\n",
    "            mask_idx = torch.randperm(new_len)[: new_len // 10]\n",
    "            x[mask_idx] = float('nan')\n",
    "\n",
    "        # (2) Spatial Augmentations\n",
    "        # Horizontal Flip (Mirroring)\n",
    "        if random.random() < self.hflip_prob:\n",
    "            x[..., 0] = 1.0 - x[..., 0] # flip x-coordinates\n",
    "\n",
    "        # Affine Transform: Local geometric transformation\n",
    "        for i in range(x.shape[0]):\n",
    "            frame = x[i] # (P, C)\n",
    "            x[i] = self.affine(frame.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        # (3) Temporal Cutout (Zero out frames): Simulates lost/blank frames\n",
    "        if new_len > self.cutout_frames:\n",
    "            start = random.randint(0, new_len - self.cutout_frames)\n",
    "            x[start:start + self.cutout_frames] = 0\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58138dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:32.083101Z",
     "iopub.status.busy": "2025-04-14T15:24:32.082729Z",
     "iopub.status.idle": "2025-04-14T15:24:32.091350Z",
     "shell.execute_reply": "2025-04-14T15:24:32.090468Z"
    },
    "papermill": {
     "duration": 0.014329,
     "end_time": "2025-04-14T15:24:32.092700",
     "exception": false,
     "start_time": "2025-04-14T15:24:32.078371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, df=None, augment=None, preprocess=None):\n",
    "        self.data = df or pd.read_csv(f\"{BASE_DIR}/train.csv\")\n",
    "        self.augment = augment or Augment()\n",
    "        self.preprocess = preprocess or Preprocess(\n",
    "            point_landmarks=POINT_LANDMARKS, \n",
    "            max_len=MAX_LEN\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sample from the DataFrame\n",
    "        sample = self.data.iloc[idx]\n",
    "\n",
    "        # Load landmark data from the Parquet file specified in the 'path' column\n",
    "        path = f\"{BASE_DIR}/{sample['path']}\"\n",
    "        landmark_df = pd.read_parquet(path)\n",
    "\n",
    "        # Extract coordinates (x, y, z) from the DataFrame\n",
    "        coordinates = landmark_df[['x', 'y', 'z']].values.reshape(-1, LANDMARKS_PER_FRAME, 3) # (num_frames, num_landmarks, xyz)\n",
    "        \n",
    "        # Extract label (sign)\n",
    "        sign = sample['sign']\n",
    "        sign_index = SIGN_TO_INDEX[sign]\n",
    "\n",
    "        # Convert to tensors\n",
    "        inputs = torch.tensor(coordinates, dtype=torch.float32)\n",
    "        target = torch.tensor(sign_index, dtype=torch.long)\n",
    "\n",
    "        print(\"[DEBUG] Pre-augment input shape:\", inputs.shape)\n",
    "\n",
    "        # Augmentation\n",
    "        if self.augment:\n",
    "            inputs = self.augment(inputs)\n",
    "            print(\"[DEBUG] Post-augment input shape:\", inputs.shape)\n",
    "\n",
    "        # Preprocessing\n",
    "        if self.preprocess:\n",
    "            inputs, mask = self.preprocess(inputs)\n",
    "            print(\"[DEBUG] Post-processing input shape:\", inputs.shape)\n",
    "\n",
    "        return inputs, target, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb9b30c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:32.100478Z",
     "iopub.status.busy": "2025-04-14T15:24:32.100156Z",
     "iopub.status.idle": "2025-04-14T15:24:32.664639Z",
     "shell.execute_reply": "2025-04-14T15:24:32.663440Z"
    },
    "papermill": {
     "duration": 0.57013,
     "end_time": "2025-04-14T15:24:32.666167",
     "exception": false,
     "start_time": "2025-04-14T15:24:32.096037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Pre-augment input shape: torch.Size([8, 543, 3])\n",
      "[DEBUG] Post-augment input shape: torch.Size([9, 543, 3])\n",
      "d: 9 384\n",
      "[DEBUG] Post-processing input shape: torch.Size([384, 118, 6])\n",
      "Input shape: torch.Size([384, 118, 6])\n",
      "Label (sign index): tensor(104)\n",
      "Mask: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "dataset = SignLanguageDataset()\n",
    "\n",
    "# Sample\n",
    "idx = random.randint(0, len(dataset)-1)\n",
    "sample = dataset[idx]\n",
    "print(\"Input shape:\", sample[0].shape) # (MAX_LEN, NUM_NODES, 6)\n",
    "print(\"Label (sign index):\", sample[1])\n",
    "print(\"Mask:\", sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8180587c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T15:24:32.674007Z",
     "iopub.status.busy": "2025-04-14T15:24:32.673661Z",
     "iopub.status.idle": "2025-04-14T15:24:32.733597Z",
     "shell.execute_reply": "2025-04-14T15:24:32.732577Z"
    },
    "papermill": {
     "duration": 0.065643,
     "end_time": "2025-04-14T15:24:32.735153",
     "exception": false,
     "start_time": "2025-04-14T15:24:32.669510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Pre-augment input shape: torch.Size([23, 543, 3])\n",
      "[DEBUG] Post-augment input shape: torch.Size([34, 543, 3])\n",
      "d: 34 384\n",
      "[DEBUG] Post-processing input shape: torch.Size([384, 118, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][2]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5087314,
     "sourceId": 46105,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.319084,
   "end_time": "2025-04-14T15:24:35.624237",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-14T15:24:10.305153",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
