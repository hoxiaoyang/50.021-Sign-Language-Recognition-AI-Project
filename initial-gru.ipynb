{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 0. Dataset Description\n*Taken from dataset [Kaggle page](https://www.kaggle.com/competitions/asl-signs/data)*\n\nDeaf children are often born to hearing parents who do not know sign language. Your challenge in this competition is to help identify signs made in processed videos, which will support the development of mobile apps to help teach parents sign language so they can communicate with their Deaf children.\n\n### 0.1. Files\n`train_landmark_files/[participant_id]/[sequence_id].parquet` \n\nThe landmark data. The landmarks were extracted from raw videos with the MediaPipe holistic model. Not all of the frames necessarily had visible hands or hands that could be detected by the model.\n\n- frame - The frame number in the raw video.\n- row_id - A unique identifier for the row.\n- type - The type of landmark. One of ['face', 'left_hand', 'pose', 'right_hand'].\n- landmark_index - The landmark index number. Details of the hand landmark locations can be found here.\n- [x/y/z] - The normalized spatial coordinates of the landmark. These are the only columns that will be provided to your submitted model for inference. The MediaPipe model is not fully trained to predict depth so you may wish to ignore the z values.\n\n`train.csv`\n- path - The path to the landmark file.\n- participant_id - A unique identifier for the data contributor.\n- sequence_id - A unique identifier for the landmark sequence.\n- sign - The label for the landmark sequence.","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_landmarks(landmark_df, num_frames=48, feature_size=84):\n    \"\"\"\n    Preprocess landmark DataFrame into (num_frames, features) shape.\n    We'll only use left and right hands: 21 points each, (x,y), so 84 features.\n    \"\"\"\n    # Only keep hand landmarks\n    landmark_df = landmark_df[landmark_df['type'].isin(['left_hand', 'right_hand'])]\n\n    frames = []\n    for frame_id, frame_data in landmark_df.groupby('frame'):\n        frame_vec = []\n\n        for landmark_type in ['left_hand', 'right_hand']:\n            type_data = frame_data[frame_data['type'] == landmark_type]\n            type_data = type_data.sort_values('landmark_index')\n            xy = type_data[['x', 'y']].to_numpy().flatten()\n\n            # pad missing landmarks\n            expected_landmarks = 21 * 2  \n            if xy.shape[0] < expected_landmarks:\n                xy = np.pad(xy, (0, expected_landmarks - xy.shape[0]))\n\n            frame_vec.append(xy)\n\n        frame_vec = np.concatenate(frame_vec)\n\n        if frame_vec.shape[0] != feature_size:\n            print(f\"Warning: Frame has incorrect size {frame_vec.shape[0]} instead of {feature_size}\")\n\n        frames.append(frame_vec)\n\n    frames = np.stack(frames)\n\n    if frames.shape[0] < num_frames:\n        pad_len = num_frames - frames.shape[0]\n        frames = np.pad(frames, ((0, pad_len), (0, 0)), mode='constant')\n    elif frames.shape[0] > num_frames:\n        frames = frames[:num_frames]\n\n    return frames.astype(np.float32) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_frame(frame_vec, title=\"Hand Landmarks\"):\n    left_hand = frame_vec[:42].reshape(21, 2)\n    right_hand = frame_vec[42:].reshape(21, 2)\n\n    plt.figure(figsize=(5, 5))\n    plt.scatter(left_hand[:, 0], -left_hand[:, 1], c='blue', label='Left Hand')\n    plt.scatter(right_hand[:, 0], -right_hand[:, 1], c='red', label='Right Hand')\n    plt.legend()\n    plt.title(title)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ASLDataset(Dataset):\n    def __init__(self, df, landmarks_path, label_to_index, num_frames=48):\n        self.df = df\n        self.landmarks_path = landmarks_path\n        self.label_to_index = label_to_index\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        participant_id = row['participant_id']\n        sequence_id = row['sequence_id']\n        sign = row['sign']\n        \n        file_path = os.path.join(self.landmarks_path, str(participant_id), f\"{sequence_id}.parquet\")\n        \n        if os.path.exists(file_path):\n            landmark_df = pd.read_parquet(file_path)\n            video_array = preprocess_landmarks(landmark_df, num_frames=self.num_frames)\n            label = self.label_to_index[sign]\n            \n            return torch.tensor(video_array, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n        else:\n            raise FileNotFoundError(f\"Landmark file {file_path} not found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class SignLanguageGRU(nn.Module):\n    \"\"\"\n    A Bidirectional GRU-based neural network with Attention for sequence classification.\n\n    Args:\n        input_size (int): Number of input features per time step.\n        hidden_size (int): Number of features in the hidden state of the GRU.\n        num_classes (int): Number of output classes for classification.\n\n    Architecture:\n        - A bidirectional GRU that captures both past and future context from the input sequence.\n        - An attention mechanism that learns to weigh important time steps.\n        - A fully connected (linear) classifier that maps the attention-weighted context vector to class scores.\n    \"\"\"\n    def __init__(self, input_size=84, hidden_size=256, num_classes=250):  # adjust num_classes\n        super(SignLanguageGRU, self).__init__()\n        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n        self.attn = nn.Linear(hidden_size * 2, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size * 2, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)  # (batch, seq, 2*hidden)\n        attn_weights = torch.softmax(self.attn(gru_out).squeeze(-1), dim=1)  # (batch, seq)\n        context = torch.sum(gru_out * attn_weights.unsqueeze(-1), dim=1)  # (batch, 2*hidden)\n        return self.classifier(context)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Set seed for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Create label mappings\nlabel_to_index = {label: idx for idx, label in enumerate(df['sign'].unique())}\nindex_to_label = {idx: label for label, idx in label_to_index.items()}\n\n# Split dataframe into training and validation\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df['sign'], random_state=SEED)\n\n# Initialize datasets\ntrain_dataset = ASLDataset(train_df, landmarks_path, label_to_index)\nval_dataset = ASLDataset(val_df, landmarks_path, label_to_index)\n\n# Initialize DataLoaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct = 0\n\n    for inputs, labels in loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        loss.backward()\n        optimizer.step()\n\n    return total_loss / len(loader), correct / len(loader.dataset)\n\ndef evaluate_model(model, loader, device):\n    model.eval()\n    correct = 0\n    predictions, actuals = [], []\n\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            predictions.extend(preds.cpu().numpy())\n            actuals.extend(labels.cpu().numpy())\n\n    accuracy = correct / len(loader.dataset)\n    return accuracy, predictions, actuals","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SignLanguageGRU(input_size=84, hidden_size=256, num_classes=len(label_to_index)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nfor epoch in range(10):\n    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n    val_acc, preds, actuals = evaluate_model(model, val_loader, device)\n    print(f\"[Epoch {epoch+1}] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n\n\nidx_to_label = {v: k for k, v in label_to_index.items()}\nfor i in range(5):\n    print(f\"Predicted: {idx_to_label[preds[i]]}, Actual: {idx_to_label[actuals[i]]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}